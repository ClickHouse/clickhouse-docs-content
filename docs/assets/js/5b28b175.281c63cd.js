"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[99103],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=o.createContext({}),l=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=l(e.components);return o.createElement(c.Provider,{value:t},e.children)},p="mdxType",k={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},f=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(n),f=a,h=p["".concat(c,".").concat(f)]||p[f]||k[f]||i;return n?o.createElement(h,r(r({ref:t},u),{},{components:n})):o.createElement(h,r({ref:t},u))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=f;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:a,r[1]=s;for(var l=2;l<i;l++)r[l]=n[l];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}f.displayName="MDXCreateElement"},9587:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>k,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var o=n(87462),a=(n(67294),n(3905));const i={sidebar_label:"Kafka Connect Options",sidebar_position:5,slug:"/en/integrations/kafka/kafka-connect-options",description:"Options with Kafka Connect"},r="Connection Options",s={unversionedId:"en/integrations/data-ingestion/kafka/kafka-connect-options",id:"en/integrations/data-ingestion/kafka/kafka-connect-options",title:"Connection Options",description:"Options with Kafka Connect",source:"@site/docs/en/integrations/data-ingestion/kafka/kafka-connect-options.md",sourceDirName:"en/integrations/data-ingestion/kafka",slug:"/en/integrations/kafka/kafka-connect-options",permalink:"/docs/en/integrations/kafka/kafka-connect-options",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/integrations/data-ingestion/kafka/kafka-connect-options.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_label:"Kafka Connect Options",sidebar_position:5,slug:"/en/integrations/kafka/kafka-connect-options",description:"Options with Kafka Connect"},sidebar:"english",previous:{title:"Kafka Connect",permalink:"/docs/en/integrations/kafka/kafka-connect-intro"},next:{title:"ClickHouse Kafka Connect Sink",permalink:"/docs/en/integrations/kafka/clickhouse-kafka-connect-sink"}},c={},l=[{value:"ClickHouse Kafka Connect Sink",id:"clickhouse-kafka-connect-sink",level:2},{value:"Open Source connectors",id:"open-source-connectors",level:2}],u={toc:l},p="wrapper";function k(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,o.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"connection-options"},"Connection Options"),(0,a.kt)("p",null,"Kafka Connect uses Sink Connectors to deliver data from Kafka topics into other data stores such as ClickHouse."),(0,a.kt)("h2",{id:"clickhouse-kafka-connect-sink"},"ClickHouse Kafka Connect Sink"),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"  The connector is available in beta stage for early adopters. If you notice a problem, please ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/ClickHouse/clickhouse-kafka-connect/issues/new"},"file an issue."))),(0,a.kt)("p",null,"The official ",(0,a.kt)("a",{parentName:"p",href:"/docs/en/integrations/kafka/clickhouse-kafka-connect-sink"},"Kafka Connect Sink connector")," for ClickHouse.\nThe connector delivers data from a Kafka topic to a ClickHouse table."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Main Features")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Shipped with out-of-box exactly-once semantics. It's powered by a new ClickHouse core feature named KeeperMap (used as a state store by the connector) and allows for minimalistic architecture."),(0,a.kt)("li",{parentName:"ul"},"Support for 3rd party state stores: Currently defaults to In-memory but can use KeeperMap (Redis to be added soon)."),(0,a.kt)("li",{parentName:"ul"},"Core integration: Built, maintained, and supported by ClickHouse"),(0,a.kt)("li",{parentName:"ul"},"Tested continuously against ",(0,a.kt)("a",{parentName:"li",href:"https://clickhouse.com/cloud"},"ClickHouse Cloud")),(0,a.kt)("li",{parentName:"ul"},"Data inserts with a declared schema and schemaless"),(0,a.kt)("li",{parentName:"ul"},"Support for most major data types of ClickHouse (more to be added soon)"),(0,a.kt)("li",{parentName:"ul"},"Distributed under ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/LICENSE"},"Apache 2.0 license"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Limitations")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Deletes aren't supported."),(0,a.kt)("li",{parentName:"ul"},"Batch size is inherited from the Kafka Consumer properties."),(0,a.kt)("li",{parentName:"ul"},"When using KeeperMap for exactly-once and the offset is changed or rewinded, you need to delete the content from KeeperMap for that specific topic.")),(0,a.kt)("h2",{id:"open-source-connectors"},"Open Source connectors"),(0,a.kt)("p",null,"Two Sink connectors provided by Confluent are compatible with ClickHouse:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://docs.confluent.io/kafka-connect-jdbc/current/"},"JDBC Connector")," - This Connector is both a Sink and Source Connector (for pushing data to Kafka) via the JDBC interface."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://docs.confluent.io/kafka-connect-http/current/overview.html"},"HTTP Sink Connector")," - A connector for pulling data from Kafka and inserting it via its HTTP interface.")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Limitations")),(0,a.kt)("p",null,"Each of these has benefits and limitations:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("a",{parentName:"li",href:"./kafka-connect-jdbc"},"JDBC connector")," relies on the user providing a ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/ClickHouse/clickhouse-java"},"JDBC driver"),". This driver has several versions, including the official ClickHouse distribution. This version uses the HTTP interface, although native support is planned. Until the native interface is not supported, it provides no performance benefit over the HTTP Sink other than ease of configuration. ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/housepower/ClickHouse-Native-JDBC"},"Other drivers")," support the native protocol, but these have not been tested."),(0,a.kt)("li",{parentName:"ul"},"The JDBC connector requires a Kafka schema defining the types of the fields. It uses this schema, defined in JSON schema, to formulate insert statements. Whilst this is effective on primitive types, the connector does not support ClickHouse specific types, e.g., Arrays and Maps. Furthermore, this connector will not support several configuration options which rely on DDL queries - highlighted in the section ",(0,a.kt)("a",{parentName:"li",href:"./kafka-connect-jdbc"},"JDBC Connector")," below."),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("a",{parentName:"li",href:"./kafka-connect-http"},"HTTP Sink Connector")," does not require a data schema. Our example assumes the data is in JSON format - although this approach should be compatible with any ",(0,a.kt)("a",{parentName:"li",href:"https://clickhouse.com/docs/en/interfaces/formats/#data-formatting"},"formats")," that the ClickHouse HTTP interface can consume. "),(0,a.kt)("li",{parentName:"ul"},"The HTTP Sink Connector is also deployed natively in Confluent Cloud and has been tested with ClickHouse Cloud, unlike the JDBC, which must be self-managed. We provide instructions for both scenarios below."),(0,a.kt)("li",{parentName:"ul"},"The JDBC connector is not currently hosted in Confluent Cloud. This must be self-managed."),(0,a.kt)("li",{parentName:"ul"},"Both connectors have at-least-once delivery semantics. Duplicates may therefore occur in ClickHouse. ")),(0,a.kt)("p",null,"The JDBC Connector is distributed under the ",(0,a.kt)("a",{parentName:"p",href:"https://www.confluent.io/confluent-community-license"},"Confluent Community License"),". The HTTP Connector conversely requires a ",(0,a.kt)("a",{parentName:"p",href:"https://docs.confluent.io/kafka-connect-http/current/overview.html#license"},"Confluent Enterprise License"),"."))}k.isMDXComponent=!0}}]);