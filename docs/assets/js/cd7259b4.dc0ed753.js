"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[89544],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>f});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=c(a),m=r,f=u["".concat(l,".").concat(m)]||u[m]||p[m]||s;return a?n.createElement(f,i(i({ref:t},d),{},{components:a})):n.createElement(f,i({ref:t},d))}));function f(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=a.length,i=new Array(s);i[0]=m;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[u]="string"==typeof e?e:r,i[1]=o;for(var c=2;c<s;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},27609:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const s={sidebar_label:"Redshift",slug:"/en/integrations/redshift",description:"Migrating Data from Redshift to ClickHouse"},i="Migrating Data from Redshift to ClickHouse",o={unversionedId:"en/integrations/data-ingestion/redshift/index",id:"en/integrations/data-ingestion/redshift/index",title:"Migrating Data from Redshift to ClickHouse",description:"Migrating Data from Redshift to ClickHouse",source:"@site/docs/en/integrations/data-ingestion/redshift/index.md",sourceDirName:"en/integrations/data-ingestion/redshift",slug:"/en/integrations/redshift",permalink:"/docs/en/integrations/redshift",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/integrations/data-ingestion/redshift/index.md",tags:[],version:"current",frontMatter:{sidebar_label:"Redshift",slug:"/en/integrations/redshift",description:"Migrating Data from Redshift to ClickHouse"},sidebar:"docs",previous:{title:"dbt",permalink:"/docs/en/integrations/dbt"},next:{title:"Airbyte",permalink:"/docs/en/integrations/airbyte"}},l={},c=[{value:"Push Data from Redshift to ClickHouse",id:"push-data-from-redshift-to-clickhouse",level:2},{value:"Pros",id:"pros",level:3},{value:"Cons",id:"cons",level:3},{value:"Pull Data from Redshift to ClickHouse",id:"pull-data-from-redshift-to-clickhouse",level:2},{value:"Pros",id:"pros-1",level:3},{value:"Cons",id:"cons-1",level:3},{value:"Tutorial",id:"tutorial",level:3},{value:"Pivot Data from Redshift to ClickHouse using S3",id:"pivot-data-from-redshift-to-clickhouse-using-s3",level:2},{value:"Pros",id:"pros-2",level:3},{value:"Cons",id:"cons-2",level:3},{value:"Tutorial",id:"tutorial-1",level:3}],d={toc:c},u="wrapper";function p(e){let{components:t,...s}=e;return(0,r.kt)(u,(0,n.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"migrating-data-from-redshift-to-clickhouse"},"Migrating Data from Redshift to ClickHouse"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/redshift/"},"Amazon Redshift")," is a popular cloud data warehousing solution that is part of the Amazon Web Services offerings. This guide presents different approaches to migrating data from a Redshift instance to ClickHouse. We will cover three options:"),(0,r.kt)("img",{src:a(57326).Z,class:"image",alt:"Redshit to ClickHouse Migration Options"}),(0,r.kt)("p",null,"From the ClickHouse instance standpoint, you can either:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"#push-data-from-redshift-to-clickhouse"},"PUSH"))," data to ClickHouse using a third party ETL/ELT tool or service")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"#pull-data-from-redshift-to-clickhouse"},"PULL"))," data from Redshift leveraging the ClickHouse JDBC Bridge")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"#pivot-data-from-redshift-to-clickhouse-using-s3"},"PIVOT"))," using S3 object storage using an \u201cUnload then load\u201d logic"))),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"We used Redshift as a data source in this tutorial. However, the migration approaches presented here are not exclusive to Redshift, and similar steps can be derived for any compatible data source.")),(0,r.kt)("h2",{id:"push-data-from-redshift-to-clickhouse"},"Push Data from Redshift to ClickHouse"),(0,r.kt)("p",null,"In the push scenario, the idea is to leverage a third-party tool or service (either custom code or an ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Extract,_transform,_load#ETL_vs._ELT"},"ETL/ELT"),") to send your data to your ClickHouse instance. For example, you can use a software like ",(0,r.kt)("a",{parentName:"p",href:"https://www.airbyte.com/"},"Airbyte")," to move data between your Redshift instance (as a source) and ClickHouse as a destination (",(0,r.kt)("a",{parentName:"p",href:"/docs/en/integrations/airbyte"},"see our integration guide for Airbyte"),")"),(0,r.kt)("img",{src:a(89077).Z,class:"image",alt:"PUSH Redshit to ClickHouse"}),(0,r.kt)("h3",{id:"pros"},"Pros"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"It can leverage the existing catalog of connectors from the ETL/ELT software."),(0,r.kt)("li",{parentName:"ul"},"Built-in capabilities to keep data in sync (append/overwrite/increment logic)."),(0,r.kt)("li",{parentName:"ul"},"Enable data transformation scenarios (for example, see our ",(0,r.kt)("a",{parentName:"li",href:"/docs/en/integrations/dbt"},"integration guide for dbt"),").")),(0,r.kt)("h3",{id:"cons"},"Cons"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Users need to set up and maintain an ETL/ELT infrastructure."),(0,r.kt)("li",{parentName:"ul"},"Introduces a third-party element in the architecture which can turn into a potential scalability bottleneck.")),(0,r.kt)("h2",{id:"pull-data-from-redshift-to-clickhouse"},"Pull Data from Redshift to ClickHouse"),(0,r.kt)("p",null,"In the pull scenario, the idea is to leverage the ClickHouse JDBC Bridge to connect to a Redshift cluster directly from a ClickHouse instance and perform ",(0,r.kt)("inlineCode",{parentName:"p"},"INSERT INTO ... SELECT")," queries:"),(0,r.kt)("img",{src:a(49602).Z,class:"image",alt:"PULL from Redshit to ClickHouse"}),(0,r.kt)("h3",{id:"pros-1"},"Pros"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Generic to all JDBC compatible tools"),(0,r.kt)("li",{parentName:"ul"},"Elegant solution to allow querying multiple external datasources from within ClickHouse")),(0,r.kt)("h3",{id:"cons-1"},"Cons"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Requires a ClickHouse JDBC Bridge instance which can turn into a potential scalability bottleneck")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Even though Redshift is based on PostgreSQL, using the ClickHouse PostgreSQL table function or table engine is not possible since ClickHouse requires PostgreSQL version 9 or above and the Redshift API is based on an earlier version (8.x).")),(0,r.kt)("h3",{id:"tutorial"},"Tutorial"),(0,r.kt)("p",null,"To use this option, you need to set up a ClickHouse JDBC Bridge. ClickHouse JDBC Bridge is a standalone Java application that handles JDBC connectivity and acts as a proxy between the ClickHouse instance and the datasources. For this tutorial, we used a pre-populated Redshift instance with a ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html"},"sample database"),"."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Deploy the ClickHouse JDBC Bridge. For more details, see our user guide on ",(0,r.kt)("a",{parentName:"li",href:"/docs/en/integrations/jdbc/jdbc-with-clickhouse"},"JDBC for External Datasources"))),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"If you are using ClickHouse Cloud, you will need to run your ClickHouse JDBC Bridge on a separate environnment and connect to ClickHouse Cloud using the ",(0,r.kt)("a",{parentName:"p",href:"https://clickhouse.com/docs/en/sql-reference/table-functions/remote/"},"remoteSecure")," function")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Configure your Redshift datasource for ClickHouse JDBC Bridge. For example, ",(0,r.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-jdbc-bridge/config/datasources/redshift.json ")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "redshift-server": {\n    "aliases": [\n      "redshift"\n    ],\n    "driverUrls": [\n    "https://s3.amazonaws.com/redshift-downloads/drivers/jdbc/2.1.0.4/redshift-jdbc42-2.1.0.4.jar"\n    ],\n    "driverClassName": "com.amazon.redshift.jdbc.Driver",\n    "jdbcUrl": "jdbc:redshift://redshift-cluster-1.ckubnplpz1uv.us-east-1.redshift.amazonaws.com:5439/dev",\n    "username": "awsuser",\n    "password": "<password>",\n    "maximumPoolSize": 5\n  }\n}\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Once ClickHouse JDBC Bridge deployed and running, you can start querying your Redshift instance from ClickHouse"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM jdbc('redshift', 'select username, firstname, lastname from users limit 5')\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 1b7de211-c0f6-4117-86a2-276484f9f4c0\n\n\u250c\u2500username\u2500\u252c\u2500firstname\u2500\u252c\u2500lastname\u2500\u2510\n\u2502 PGL08LJI \u2502 Vladimir  \u2502 Humphrey \u2502\n\u2502 XDZ38RDD \u2502 Barry     \u2502 Roy      \u2502\n\u2502 AEB55QTM \u2502 Reagan    \u2502 Hodge    \u2502\n\u2502 OWY35QYB \u2502 Tamekah   \u2502 Juarez   \u2502\n\u2502 MSD36KVR \u2502 Mufutau   \u2502 Watkins  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n5 rows in set. Elapsed: 0.438 sec.\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM jdbc('redshift', 'select count(*) from sales')\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 2d0f957c-8f4e-43b2-a66a-cc48cc96237b\n\n\u250c\u2500\u2500count\u2500\u2510\n\u2502 172456 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 rows in set. Elapsed: 0.304 sec.\n")))),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"In the following, we display importing data using an ",(0,r.kt)("inlineCode",{parentName:"p"},"INSERT INTO ... SELECT")," statement"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"# TABLE CREATION with 3 columns\nCREATE TABLE users_imported\n(\n    `username` String,\n    `firstname` String,\n    `lastname` String\n)\nENGINE = MergeTree\nORDER BY firstname\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: c7c4c44b-cdb2-49cf-b319-4e569976ab05\n\nOk.\n\n0 rows in set. Elapsed: 0.233 sec.\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"# IMPORTING DATA\nINSERT INTO users_imported (*) SELECT *\nFROM jdbc('redshift', 'select username, firstname, lastname from users')\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 9d3a688d-b45a-40f4-a7c7-97d93d7149f1\n\nOk.\n\n0 rows in set. Elapsed: 4.498 sec. Processed 49.99 thousand rows, 2.49 MB (11.11 thousand rows/s., 554.27 KB/s.)\n")))),(0,r.kt)("h2",{id:"pivot-data-from-redshift-to-clickhouse-using-s3"},"Pivot Data from Redshift to ClickHouse using S3"),(0,r.kt)("p",null,"In this scenario, we export data to S3 in an intermediary pivot format and, in a second step, load the data from S3 into ClickHouse."),(0,r.kt)("img",{src:a(66620).Z,class:"image",alt:"PIVOT from Redshit using S3"}),(0,r.kt)("h3",{id:"pros-2"},"Pros"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Both Redshift and ClickHouse have powerful S3 integration features."),(0,r.kt)("li",{parentName:"ul"},"Leverages the existing features such as the Redshift ",(0,r.kt)("inlineCode",{parentName:"li"},"UNLOAD")," command and ClickHouse S3 table function / table engine."),(0,r.kt)("li",{parentName:"ul"},"Scales seamlessly thanks to parallel reads and high throughput capabilities from/to S3 in ClickHouse."),(0,r.kt)("li",{parentName:"ul"},"Can leverage sophisticated and compressed formats like Apache Parquet.")),(0,r.kt)("h3",{id:"cons-2"},"Cons"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Two steps in the process (unload from Redshift then load into ClickHouse).")),(0,r.kt)("h3",{id:"tutorial-1"},"Tutorial"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Using Redshift's ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html"},"UNLOAD")," feature, export the data into a an existing private S3 bucket:"),(0,r.kt)("img",{src:a(69515).Z,class:"image",alt:"UNLOAD from Redshit to S3"}),(0,r.kt)("p",{parentName:"li"},"It will generate part files containing the raw data in S3"),(0,r.kt)("img",{src:a(79458).Z,class:"image",alt:"Data in S3"})),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create the table in ClickHouse:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE users\n(\n    username String,\n    firstname String,\n    lastname String\n)\nENGINE = MergeTree\nORDER BY username\n")),(0,r.kt)("p",{parentName:"li"},"Alternatively, ClickHouse can try to infer the table structure using ",(0,r.kt)("inlineCode",{parentName:"p"},"CREATE TABLE ... EMPTY AS SELECT"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE users\nENGINE = MergeTree ORDER BY username\nEMPTY AS\nSELECT * FROM s3('https://your-bucket.s3.amazonaws.com/unload/users/*', '<aws_access_key>', '<aws_secret_access_key>', 'CSV')\n")),(0,r.kt)("p",{parentName:"li"},"This works especially well when the data is in a format that contains information about data types, like Parquet.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Load the S3 files into ClickHouse using an ",(0,r.kt)("inlineCode",{parentName:"p"},"INSERT INTO ... SELECT")," statement:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO users SELECT *\nFROM s3('https://your-bucket.s3.amazonaws.com/unload/users/*', '<aws_access_key>', '<aws_secret_access_key>', 'CSV')\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 2e7e219a-6124-461c-8d75-e4f5002c8557\n\nOk.\n\n0 rows in set. Elapsed: 0.545 sec. Processed 49.99 thousand rows, 2.34 MB (91.72 thousand rows/s., 4.30 MB/s.)\n")))),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This example used CSV as the pivot format. However, for production workloads we recommend Apache Parquet as the best option for large migrations since it comes with compression and can save some storage costs while reducing transfer times. (By default, each row group is compressed using SNAPPY). ClickHouse also leverages Parquet's column orientation to speed up data ingestion.")))}p.isMDXComponent=!0},66620:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/pivot-3fe3713d3a34ba2c8740019113d000dc.png"},49602:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/pull-b3ab6dde4c89d2a59b961b5430a68167.png"},89077:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/push-7f0afefde34793a9510b8528823aca78.png"},57326:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/redshift-to-clickhouse-763a7570093d65fe0d2f8f523f60dc07.png"},69515:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/s3-1-be9a86f6c8ae20e367d00b100700d396.png"},79458:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/s3-2-dbabae19318318b3d931ff907823cf7a.png"}}]);