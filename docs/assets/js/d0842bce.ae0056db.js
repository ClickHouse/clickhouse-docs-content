"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[96156],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>k});var o=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,i=function(e,t){if(null==e)return{};var n,o,i={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=o.createContext({}),c=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=c(e.components);return o.createElement(s.Provider,{value:t},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=c(n),d=i,k=p["".concat(s,".").concat(d)]||p[d]||h[d]||r;return n?o.createElement(k,a(a({ref:t},u),{},{components:n})):o.createElement(k,a({ref:t},u))}));function k(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,a=new Array(r);a[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:i,a[1]=l;for(var c=2;c<r;c++)a[c]=n[c];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},76450:(e,t,n)=>{n.d(t,{ZP:()=>l});var o=n(87462),i=(n(67294),n(3905));const r={toc:[]},a="wrapper";function l(e){let{components:t,...n}=e;return(0,i.kt)(a,(0,o.Z)({},r,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("admonition",{title:"best practices",type:"important"},(0,i.kt)("p",{parentName:"admonition"},"When configuring ClickHouse Server by adding or editing configuration files you should:"),(0,i.kt)("ul",{parentName:"admonition"},(0,i.kt)("li",{parentName:"ul"},"Add files to ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/config.d/")," directory"),(0,i.kt)("li",{parentName:"ul"},"Add files to ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/users.d/")," directory"),(0,i.kt)("li",{parentName:"ul"},"Leave the ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/config.xml")," file as it is"),(0,i.kt)("li",{parentName:"ul"},"Leave the ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/users.xml")," file as it is "))))}l.isMDXComponent=!0},29214:(e,t,n)=>{n.d(t,{ZP:()=>l});var o=n(87462),i=(n(67294),n(3905));const r={toc:[{value:"Terminology",id:"terminology",level:2},{value:"Replica",id:"replica",level:3},{value:"Shard",id:"shard",level:3},{value:"Distributed coordination",id:"distributed-coordination",level:3}]},a="wrapper";function l(e){let{components:t,...n}=e;return(0,i.kt)(a,(0,o.Z)({},r,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"terminology"},"Terminology"),(0,i.kt)("h3",{id:"replica"},"Replica"),(0,i.kt)("p",null,"A copy of data.  ClickHouse always has at least one copy of your data, and so the minimum number of ",(0,i.kt)("strong",{parentName:"p"},"replicas")," is one.  This is an important detail, you may not be used to counting the original copy of your data as a replica, but that is the term used in ClickHouse code and documentation.  Adding a second replica of your data provides fault tolerance. "),(0,i.kt)("h3",{id:"shard"},"Shard"),(0,i.kt)("p",null,"A subset of data.  ClickHouse always has at least one shard for your data, so if you do not split the data across multiple servers, your data will be stored in one shard.  Sharding data across multiple servers can be used to divide the load if you exceed the capacity of a single server. The destination server is determined by the ",(0,i.kt)("strong",{parentName:"p"},"sharding key"),", and is defined when you create the distributed table. The sharding key can be random or as an output of a ",(0,i.kt)("a",{parentName:"p",href:"https://clickhouse.com/docs/en/sql-reference/functions/hash-functions"},"hash function"),".  The deployment examples involving sharding will use ",(0,i.kt)("inlineCode",{parentName:"p"},"rand()")," as the sharding key, and will provide further information on when and how to choose a different sharding key."),(0,i.kt)("h3",{id:"distributed-coordination"},"Distributed coordination"),(0,i.kt)("p",null,"ClickHouse Keeper provides the coordination system for data replication and distributed DDL queries execution. ClickHouse Keeper is compatible with Apache ZooKeeper."))}l.isMDXComponent=!0},39841:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>p,default:()=>f,frontMatter:()=>u,metadata:()=>h,toc:()=>k});var o=n(87462),i=(n(67294),n(3905)),r=n(29214),a=n(76450);const l={toc:[]},s="wrapper";function c(e){let{components:t,...n}=e;return(0,i.kt)(s,(0,o.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("admonition",{title:"best practices",type:"important"},(0,i.kt)("p",{parentName:"admonition"},"When configuring ClickHouse Keeper by adding or editing configuration files you should:"),(0,i.kt)("ul",{parentName:"admonition"},(0,i.kt)("li",{parentName:"ul"},"Add files to ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-keeper/config.d/")," directory"),(0,i.kt)("li",{parentName:"ul"},"Leave the ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-keeper/config.xml")," file as it is"))))}c.isMDXComponent=!0;const u={slug:"/en/architecture/replication",sidebar_label:"Replication for fault tolerance",sidebar_position:10,title:"Replication for fault tolerance"},p=void 0,h={unversionedId:"en/deployment-guides/replicated",id:"en/deployment-guides/replicated",title:"Replication for fault tolerance",description:"Description",source:"@site/docs/en/deployment-guides/replicated.md",sourceDirName:"en/deployment-guides",slug:"/en/architecture/replication",permalink:"/docs/en/architecture/replication",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/deployment-guides/replicated.md",tags:[],version:"current",sidebarPosition:10,frontMatter:{slug:"/en/architecture/replication",sidebar_label:"Replication for fault tolerance",sidebar_position:10,title:"Replication for fault tolerance"},sidebar:"docs",previous:{title:"Scaling out",permalink:"/docs/en/architecture/horizontal-scaling"},next:{title:"Configuring LDAP",permalink:"/docs/en/guides/sre/configuring-ldap"}},d={},k=[{value:"Description",id:"description",level:2},{value:"Level: Basic",id:"level-basic",level:2},{value:"Environment",id:"environment",level:2},{value:"Architecture Diagram",id:"architecture-diagram",level:3},{value:"Install",id:"install",level:2},{value:"Editing configuration files",id:"editing-configuration-files",level:2},{value:"clickhouse-01 configuration",id:"clickhouse-01-configuration",level:2},{value:"Network and logging configuration",id:"network-and-logging-configuration",level:3},{value:"Macros configuration",id:"macros-configuration",level:3},{value:"Replication and sharding configuration",id:"replication-and-sharding-configuration",level:3},{value:"Configuring the use of Keeper",id:"configuring-the-use-of-keeper",level:3},{value:"clickhouse-02 configuration",id:"clickhouse-02-configuration",level:2},{value:"Network and logging configuration",id:"network-and-logging-configuration-1",level:3},{value:"Macros configuration",id:"macros-configuration-1",level:3},{value:"Replication and sharding configuration",id:"replication-and-sharding-configuration-1",level:3},{value:"Configuring the use of Keeper",id:"configuring-the-use-of-keeper-1",level:3},{value:"clickhouse-keeper-01 configuration",id:"clickhouse-keeper-01-configuration",level:2},{value:"clickhouse-keeper-02 configuration",id:"clickhouse-keeper-02-configuration",level:2},{value:"clickhouse-keeper-02 configuration",id:"clickhouse-keeper-02-configuration-1",level:2},{value:"Testing",id:"testing",level:2},{value:"Verify that ClickHouse Keeper is running",id:"verify-that-clickhouse-keeper-is-running",level:3},{value:"Verify ClickHouse cluster functionality",id:"verify-clickhouse-cluster-functionality",level:3}],m={toc:k},g="wrapper";function f(e){let{components:t,...l}=e;return(0,i.kt)(g,(0,o.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"description"},"Description"),(0,i.kt)("p",null,"In this architecture, there are five servers configured. Two are used to host copies of the data. The other three servers are used to coordinate the replication of data. With this example, we'll create a database and table that will be replicated across both data nodes using the ReplicatedMergeTree table engine."),(0,i.kt)("h2",{id:"level-basic"},"Level: Basic"),(0,i.kt)(r.ZP,{mdxType:"ReplicationShardingTerminology"}),(0,i.kt)("h2",{id:"environment"},"Environment"),(0,i.kt)("h3",{id:"architecture-diagram"},"Architecture Diagram"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Architecture diagram for 1 shard and 2 replicas with ReplicatedMergeTree",src:n(11246).Z,width:"947",height:"837"})),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Node"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"clickhouse-01"),(0,i.kt)("td",{parentName:"tr",align:null},"Data")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"clickhouse-02"),(0,i.kt)("td",{parentName:"tr",align:null},"Data")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"clickhouse-keeper-01"),(0,i.kt)("td",{parentName:"tr",align:null},"Distributed coordination")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"clickhouse-keeper-02"),(0,i.kt)("td",{parentName:"tr",align:null},"Distributed coordination")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"clickhouse-keeper-03"),(0,i.kt)("td",{parentName:"tr",align:null},"Distributed coordination")))),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"It is possible to run ClickHouse Server and Keeper combined on the same server.  The other basic example, ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/architecture/horizontal-scaling"},"Scaling out"),", uses this method.  In this example we present the recommended method of separating Keeper from ClickHouse Server.  The Keeper servers can be smaller, 4GB RAM is generally enough for each Keeper server until your ClickHouse Servers grow very large.")),(0,i.kt)("h2",{id:"install"},"Install"),(0,i.kt)("p",null,"Install ClickHouse server and client on the two servers ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-01")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-02")," following the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#available-installation-options"},"instructions for your archive type")," (.deb, .rpm, .tar.gz, etc.). "),(0,i.kt)("p",null,"Install ClickHouse Keeper on the three servers ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-01"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-02")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-03")," following the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#install-standalone-clickhouse-keeper"},"instructions for your archive type")," (.deb, .rpm, .tar.gz, etc.)."),(0,i.kt)("h2",{id:"editing-configuration-files"},"Editing configuration files"),(0,i.kt)(a.ZP,{mdxType:"ConfigFileNote"}),(0,i.kt)("h2",{id:"clickhouse-01-configuration"},"clickhouse-01 configuration"),(0,i.kt)("p",null,"For clickhouse-01 there are five configuration files.  You may choose to combine these files into a single file, but for clarity in the documentation it may be simpler to look at them separately.  As you read through the configuration files you will see that most of the configuration is the same between clickhouse-01 and clickhouse-02; the differences will be highlighted."),(0,i.kt)("h3",{id:"network-and-logging-configuration"},"Network and logging configuration"),(0,i.kt)("p",null,"These values can be customized as you wish.  This example configuration gives you:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"a debug log that will roll over at 1000M three times"),(0,i.kt)("li",{parentName:"ul"},"the name displayed when you connect with ",(0,i.kt)("inlineCode",{parentName:"li"},"clickhouse-client")," is ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_1S_2R node 1")),(0,i.kt)("li",{parentName:"ul"},"ClickHouse will listen on the IPV4 network on ports 8123 and 9000.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/network-and-logging.xml on clickhouse-01"',title:'"/etc/clickhouse-server/config.d/network-and-logging.xml',on:!0,'clickhouse-01"':!0},"<clickhouse>\n    <logger>\n        <level>debug</level>\n        <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n        <size>1000M</size>\n        <count>3</count>\n    </logger>\n    <display_name>cluster_1S_2R node 1</display_name>\n    <listen_host>0.0.0.0</listen_host>\n    <http_port>8123</http_port>\n    <tcp_port>9000</tcp_port>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"macros-configuration"},"Macros configuration"),(0,i.kt)("p",null,"The macros ",(0,i.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," reduce the complexity of distributed DDL.  The values configured are automatically substituted in your DDL queries, which simplifies your DDL.  The macros for this configuration specify the shard and replica number for each node.",(0,i.kt)("br",{parentName:"p"}),"\n","In this 1 shard 2 replica example, the replica macro is ",(0,i.kt)("inlineCode",{parentName:"p"},"replica_1")," on clickhouse-01 and ",(0,i.kt)("inlineCode",{parentName:"p"},"replica_2")," on clickhouse-02.  The shard macro is ",(0,i.kt)("inlineCode",{parentName:"p"},"1")," on both clickhouse-01 and clickhouse-02 as there is only one shard. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/macros.xml on clickhouse-01"',title:'"/etc/clickhouse-server/config.d/macros.xml',on:!0,'clickhouse-01"':!0},"<clickhouse>\n    <macros>\n        <shard>01</shard>\n        \x3c!-- highlight-next-line --\x3e\n        <replica>01</replica>\n        <cluster>cluster_1S_2R</cluster>\n    </macros>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"replication-and-sharding-configuration"},"Replication and sharding configuration"),(0,i.kt)("p",null,"Starting from the top:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The remote_servers section of the XML specifies each of the clusters in the environment. The attribute ",(0,i.kt)("inlineCode",{parentName:"li"},"replace=true")," replaces the sample remote_servers in the default ClickHouse configuration with the remote_server configuration specified in this file.  Without this attribute the remote servers in this file would be appended to the list of samples in the default.  "),(0,i.kt)("li",{parentName:"ul"},"In this example, there is one cluster named ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_1S_2R"),"."),(0,i.kt)("li",{parentName:"ul"},"A secret is created for the cluster named ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_1S_2R")," with the value ",(0,i.kt)("inlineCode",{parentName:"li"},"mysecretphrase"),".  The secret is shared across all of the remote servers in the environment to ensure that the correct servers are joined together."),(0,i.kt)("li",{parentName:"ul"},"The cluster ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_1S_2R")," has one shard, and two replicas.  Take a look at the architecture diagram toward the beginning of this document, and compare it with the ",(0,i.kt)("inlineCode",{parentName:"li"},"shard")," definition in the XML below.  The shard definition contains two replicas.  The host and port for each replica is specified.  One replica is stored on ",(0,i.kt)("inlineCode",{parentName:"li"},"clickhouse-01"),", and the other replica is stored on ",(0,i.kt)("inlineCode",{parentName:"li"},"clickhouse-02"),"."),(0,i.kt)("li",{parentName:"ul"},"Internal replication for the shard is set to true.  Each shard can have the internal_replication parameter defined in the config file. If this parameter is set to true, the write operation selects the first healthy replica and writes data to it.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/remote-servers.xml on clickhouse-01"',title:'"/etc/clickhouse-server/config.d/remote-servers.xml',on:!0,'clickhouse-01"':!0},'<clickhouse>\n    <remote_servers replace="true">\n        <cluster_1S_2R>\n            <secret>mysecretphrase</secret>\n            <shard>\n                <internal_replication>true</internal_replication>\n                <replica>\n                    <host>clickhouse-01</host>\n                    <port>9000</port>\n                </replica>\n                <replica>\n                    <host>clickhouse-02</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n        </cluster_1S_2R>\n    </remote_servers>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"configuring-the-use-of-keeper"},"Configuring the use of Keeper"),(0,i.kt)("p",null,"This configuration file ",(0,i.kt)("inlineCode",{parentName:"p"},"use-keeper.xml")," is configuring ClickHouse Server to use ClickHouse Keeper for the coordination of replication and distributed DDL.  This file specifies that ClickHouse Server should use Keeper on nodes clickhouse-keeper-01 - 03 on port 9181, and the file is the same on ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-01")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-02"),".  "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/use-keeper.xml on clickhouse-01"',title:'"/etc/clickhouse-server/config.d/use-keeper.xml',on:!0,'clickhouse-01"':!0},"<clickhouse>\n    <zookeeper>\n        \x3c!-- where are the ZK nodes --\x3e\n        <node>\n            <host>clickhouse-keeper-01</host>\n            <port>9181</port>\n        </node>\n        <node>\n            <host>clickhouse-keeper-02</host>\n            <port>9181</port>\n        </node>\n        <node>\n            <host>clickhouse-keeper-03</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"clickhouse-02-configuration"},"clickhouse-02 configuration"),(0,i.kt)("p",null,"As the configuration is very similar on clickhouse-01 and clickhouse-02 only the differences will be pointed out here."),(0,i.kt)("h3",{id:"network-and-logging-configuration-1"},"Network and logging configuration"),(0,i.kt)("p",null,"This file is the same on both clickhouse-01 and clickhouse-02, with the exception of ",(0,i.kt)("inlineCode",{parentName:"p"},"display_name"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/network-and-logging.xml on clickhouse-02"',title:'"/etc/clickhouse-server/config.d/network-and-logging.xml',on:!0,'clickhouse-02"':!0},"<clickhouse>\n    <logger>\n        <level>debug</level>\n        <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n        <size>1000M</size>\n        <count>3</count>\n    </logger>\n    \x3c!-- highlight-next-line --\x3e\n    <display_name>cluster_1S_2R node 2</display_name>\n    <listen_host>0.0.0.0</listen_host>\n    <http_port>8123</http_port>\n    <tcp_port>9000</tcp_port>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"macros-configuration-1"},"Macros configuration"),(0,i.kt)("p",null,"The macros configuration is different between clickhouse-01 and clickhouse-02.  ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"02")," on this node."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/macros.xml on clickhouse-02"',title:'"/etc/clickhouse-server/config.d/macros.xml',on:!0,'clickhouse-02"':!0},"<clickhouse>\n    <macros>\n        <shard>01</shard>\n        \x3c!-- highlight-next-line --\x3e\n        <replica>02</replica>\n        <cluster>cluster_1S_2R</cluster>\n    </macros>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"replication-and-sharding-configuration-1"},"Replication and sharding configuration"),(0,i.kt)("p",null,"This file is the same on both clickhouse-01 and clickhouse-02."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/remote-servers.xml on clickhouse-02"',title:'"/etc/clickhouse-server/config.d/remote-servers.xml',on:!0,'clickhouse-02"':!0},'<clickhouse>\n    <remote_servers replace="true">\n        <cluster_1S_2R>\n            <secret>mysecretphrase</secret>\n            <shard>\n                <internal_replication>true</internal_replication>\n                <replica>\n                    <host>clickhouse-01</host>\n                    <port>9000</port>\n                </replica>\n                <replica>\n                    <host>clickhouse-02</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n        </cluster_1S_2R>\n    </remote_servers>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"configuring-the-use-of-keeper-1"},"Configuring the use of Keeper"),(0,i.kt)("p",null,"This file is the same on both clickhouse-01 and clickhouse-02."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/use-keeper.xml on clickhouse-02"',title:'"/etc/clickhouse-server/config.d/use-keeper.xml',on:!0,'clickhouse-02"':!0},"<clickhouse>\n    <zookeeper>\n        \x3c!-- where are the ZK nodes --\x3e\n        <node>\n            <host>clickhouse-keeper-01</host>\n            <port>9181</port>\n        </node>\n        <node>\n            <host>clickhouse-keeper-02</host>\n            <port>9181</port>\n        </node>\n        <node>\n            <host>clickhouse-keeper-03</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"clickhouse-keeper-01-configuration"},"clickhouse-keeper-01 configuration"),(0,i.kt)(c,{mdxType:"KeeperConfigFileNote"}),(0,i.kt)("p",null,"ClickHouse Keeper provides the coordination system for data replication and distributed DDL queries execution. ClickHouse Keeper is compatible with Apache ZooKeeper.  This configuration enables ClickHouse Keeper on port 9181.  The highlighted line specifies that this instance of Keeper has server_id of 1.  This is the only difference in the ",(0,i.kt)("inlineCode",{parentName:"p"},"enable-keeper.xml")," file across the three servers.  ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-02")," will have ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," set to ",(0,i.kt)("inlineCode",{parentName:"p"},"2"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-03")," will have ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," set to ",(0,i.kt)("inlineCode",{parentName:"p"},"3"),".  The raft configuration section is the same on all three servers, it is highlighted below to show you the relationship between ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," and the ",(0,i.kt)("inlineCode",{parentName:"p"},"server")," instance within the raft configuration."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"If for any reason a Keeper node is replaced or rebuilt, do not reuse an existing ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id"),".  For example, if the Keeper node with ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"2")," is rebuilt, give it server_id of ",(0,i.kt)("inlineCode",{parentName:"p"},"4")," or higher.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-keeper/config.d/keeper.xml on clickhouse-keeper-01"',title:'"/etc/clickhouse-keeper/config.d/keeper.xml',on:!0,'clickhouse-keeper-01"':!0},"<clickhouse>\n    <logger>\n        <level>trace</level>\n        <log>/var/log/clickhouse-keeper/clickhouse-keeper.log</log>\n        <errorlog>/var/log/clickhouse-keeper/clickhouse-keeper.err.log</errorlog>\n        <size>1000M</size>\n        <count>3</count>\n    </logger>\n    <listen_host>0.0.0.0</listen_host>\n    <keeper_server>\n        <tcp_port>9181</tcp_port>\n        \x3c!-- highlight-next-line --\x3e\n        <server_id>1</server_id>\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n        <coordination_settings>\n            <operation_timeout_ms>10000</operation_timeout_ms>\n            <session_timeout_ms>30000</session_timeout_ms>\n            <raft_logs_level>trace</raft_logs_level>\n        </coordination_settings>\n        <raft_configuration>\n            \x3c!-- highlight-start --\x3e\n            <server>\n                <id>1</id>\n                <hostname>clickhouse-keeper-01</hostname>\n                <port>9234</port>\n            </server>\n            \x3c!-- highlight-end --\x3e\n            <server>\n                <id>2</id>\n                <hostname>clickhouse-keeper-02</hostname>\n                <port>9234</port>\n            </server>\n            <server>\n                <id>3</id>\n                <hostname>clickhouse-keeper-03</hostname>\n                <port>9234</port>\n            </server>\n        </raft_configuration>\n    </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"clickhouse-keeper-02-configuration"},"clickhouse-keeper-02 configuration"),(0,i.kt)("p",null,"There is only one line difference between ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-01")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-02"),".  ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"2")," on this node."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-keeper/config.d/keeper.xml on clickhouse-keeper-02"',title:'"/etc/clickhouse-keeper/config.d/keeper.xml',on:!0,'clickhouse-keeper-02"':!0},"<clickhouse>\n    <logger>\n        <level>trace</level>\n        <log>/var/log/clickhouse-keeper/clickhouse-keeper.log</log>\n        <errorlog>/var/log/clickhouse-keeper/clickhouse-keeper.err.log</errorlog>\n        <size>1000M</size>\n        <count>3</count>\n    </logger>\n    <listen_host>0.0.0.0</listen_host>\n    <keeper_server>\n        <tcp_port>9181</tcp_port>\n        \x3c!-- highlight-next-line --\x3e\n        <server_id>2</server_id>\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n        <coordination_settings>\n            <operation_timeout_ms>10000</operation_timeout_ms>\n            <session_timeout_ms>30000</session_timeout_ms>\n            <raft_logs_level>trace</raft_logs_level>\n        </coordination_settings>\n        <raft_configuration>\n            <server>\n                <id>1</id>\n                <hostname>clickhouse-keeper-01</hostname>\n                <port>9234</port>\n            </server>\n            \x3c!-- highlight-start --\x3e\n            <server>\n                <id>2</id>\n                <hostname>clickhouse-keeper-02</hostname>\n                <port>9234</port>\n            </server>\n            \x3c!-- highlight-end --\x3e\n            <server>\n                <id>3</id>\n                <hostname>clickhouse-keeper-03</hostname>\n                <port>9234</port>\n            </server>\n        </raft_configuration>\n    </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"clickhouse-keeper-02-configuration-1"},"clickhouse-keeper-02 configuration"),(0,i.kt)("p",null,"There is only one line difference between ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-01")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-03"),".  ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"3")," on this node."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-keeper/config.d/keeper.xml on clickhouse-keeper-03"',title:'"/etc/clickhouse-keeper/config.d/keeper.xml',on:!0,'clickhouse-keeper-03"':!0},"<clickhouse>\n    <logger>\n        <level>trace</level>\n        <log>/var/log/clickhouse-keeper/clickhouse-keeper.log</log>\n        <errorlog>/var/log/clickhouse-keeper/clickhouse-keeper.err.log</errorlog>\n        <size>1000M</size>\n        <count>3</count>\n    </logger>\n    <listen_host>0.0.0.0</listen_host>\n    <keeper_server>\n        <tcp_port>9181</tcp_port>\n        \x3c!-- highlight-next-line --\x3e\n        <server_id>3</server_id>\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n        <coordination_settings>\n            <operation_timeout_ms>10000</operation_timeout_ms>\n            <session_timeout_ms>30000</session_timeout_ms>\n            <raft_logs_level>trace</raft_logs_level>\n        </coordination_settings>\n        <raft_configuration>\n            <server>\n                <id>1</id>\n                <hostname>clickhouse-keeper-01</hostname>\n                <port>9234</port>\n            </server>\n            <server>\n                <id>2</id>\n                <hostname>clickhouse-keeper-02</hostname>\n                <port>9234</port>\n            </server>\n            \x3c!-- highlight-start --\x3e\n            <server>\n                <id>3</id>\n                <hostname>clickhouse-keeper-03</hostname>\n                <port>9234</port>\n            </server>\n            \x3c!-- highlight-end --\x3e\n        </raft_configuration>\n    </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"testing"},"Testing"),(0,i.kt)("p",null,"To gain experience with ReplicatedMergeTree and ClickHouse Keeper you can run the following commands which will have you:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Create a database on the cluster configured above"),(0,i.kt)("li",{parentName:"ul"},"Create a table on the database using the ReplicatedMergeTree table engine"),(0,i.kt)("li",{parentName:"ul"},"Insert data on one node and query it on another node"),(0,i.kt)("li",{parentName:"ul"},"Stop one ClickHouse server node"),(0,i.kt)("li",{parentName:"ul"},"Insert more data on the running node"),(0,i.kt)("li",{parentName:"ul"},"Restart the stopped node"),(0,i.kt)("li",{parentName:"ul"},"Verify that the data is available when querying the restarted node")),(0,i.kt)("h3",{id:"verify-that-clickhouse-keeper-is-running"},"Verify that ClickHouse Keeper is running"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"mntr")," command is used to verify that the ClickHouse Keeper is running and to get state information about the relationship of the three Keeper nodes.  In the configuration used in this example there are three nodes working together.  The nodes will elect a leader, and the remaining nodes will be followers.  The ",(0,i.kt)("inlineCode",{parentName:"p"},"mntr")," command gives information related to performance, and whether a particular node is a follower or a leader."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"You may need to install ",(0,i.kt)("inlineCode",{parentName:"p"},"netcat")," in order to send the ",(0,i.kt)("inlineCode",{parentName:"p"},"mntr")," command to Keeper.  Please see the ",(0,i.kt)("a",{parentName:"p",href:"https://nmap.org/ncat/"},"nmap.org")," page for download information.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run from a shell on clickhouse-keeper-01, clickhouse-keeper-02, and clickhouse-keeper-03"',title:'"run',from:!0,a:!0,shell:!0,on:!0,"clickhouse-keeper-01,":!0,"clickhouse-keeper-02,":!0,and:!0,'clickhouse-keeper-03"':!0},"echo mntr | nc localhost 9181\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response",metastring:'title="response from a follower"',title:'"response',from:!0,a:!0,'follower"':!0},"zk_version  v23.3.1.2823-testing-46e85357ce2da2a99f56ee83a079e892d7ec3726\nzk_avg_latency  0\nzk_max_latency  0\nzk_min_latency  0\nzk_packets_received 0\nzk_packets_sent 0\nzk_num_alive_connections    0\nzk_outstanding_requests 0\n# highlight-next-line\nzk_server_state follower\nzk_znode_count  6\nzk_watch_count  0\nzk_ephemerals_count 0\nzk_approximate_data_size    1271\nzk_key_arena_size   4096\nzk_latest_snapshot_size 0\nzk_open_file_descriptor_count   46\nzk_max_file_descriptor_count    18446744073709551615\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response",metastring:'title="response from a leader"',title:'"response',from:!0,a:!0,'leader"':!0},"zk_version  v23.3.1.2823-testing-46e85357ce2da2a99f56ee83a079e892d7ec3726\nzk_avg_latency  0\nzk_max_latency  0\nzk_min_latency  0\nzk_packets_received 0\nzk_packets_sent 0\nzk_num_alive_connections    0\nzk_outstanding_requests 0\n# highlight-next-line\nzk_server_state leader\nzk_znode_count  6\nzk_watch_count  0\nzk_ephemerals_count 0\nzk_approximate_data_size    1271\nzk_key_arena_size   4096\nzk_latest_snapshot_size 0\nzk_open_file_descriptor_count   48\nzk_max_file_descriptor_count    18446744073709551615\n# highlight-start\nzk_followers    2\nzk_synced_followers 2\n# highlight-end\n")),(0,i.kt)("h3",{id:"verify-clickhouse-cluster-functionality"},"Verify ClickHouse cluster functionality"),(0,i.kt)("p",null,"Connect to node ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-01")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse client")," in one shell, and connect to node ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-02")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse client")," in another shell."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create a database on the cluster configured above")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on either node clickhouse-01 or clickhouse-02"',title:'"run',on:!0,either:!0,node:!0,"clickhouse-01":!0,or:!0,'clickhouse-02"':!0},"CREATE DATABASE db1 ON CLUSTER cluster_1S_2R\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 clickhouse-02 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 clickhouse-01 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Create a table on the database using the ReplicatedMergeTree table engine")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on either node clickhouse-01 or clickhouse-02"',title:'"run',on:!0,either:!0,node:!0,"clickhouse-01":!0,or:!0,'clickhouse-02"':!0},"CREATE TABLE db1.table1 ON CLUSTER cluster_1S_2R\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = ReplicatedMergeTree\nORDER BY id\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 clickhouse-02 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 clickhouse-01 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Insert data on one node and query it on another node")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on node clickhouse-01"',title:'"run',on:!0,node:!0,'clickhouse-01"':!0},"INSERT INTO db1.table1 (id, column1) VALUES (1, 'abc');\n")),(0,i.kt)("ol",{start:4},(0,i.kt)("li",{parentName:"ol"},"Query the table on the node ",(0,i.kt)("inlineCode",{parentName:"li"},"clickhouse-02"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on node clickhouse-02"',title:'"run',on:!0,node:!0,'clickhouse-02"':!0},"SELECT *\nFROM db1.table1\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:5},(0,i.kt)("li",{parentName:"ol"},"Insert data on the other node and query it on the node ",(0,i.kt)("inlineCode",{parentName:"li"},"clickhouse-01"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on node clickhouse-02"',title:'"run',on:!0,node:!0,'clickhouse-02"':!0},"INSERT INTO db1.table1 (id, column1) VALUES (2, 'def');\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on node clickhouse-01"',title:'"run',on:!0,node:!0,'clickhouse-01"':!0},"SELECT *\nFROM db1.table1\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:6},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Stop one ClickHouse server node\nStop one of the ClickHouse server nodes by running an operating system command similar to the command used to start the node.  If you used ",(0,i.kt)("inlineCode",{parentName:"p"},"systemctl start")," to start the node, then use ",(0,i.kt)("inlineCode",{parentName:"p"},"systemctl stop")," to stop it.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Insert more data on the running node"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on the running node"',title:'"run',on:!0,the:!0,running:!0,'node"':!0},"INSERT INTO db1.table1 (id, column1) VALUES (3, 'ghi');\n")),(0,i.kt)("p",null,"Select the data:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on the running node"',title:'"run',on:!0,the:!0,running:!0,'node"':!0},"SELECT *\nFROM db1.table1\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  3 \u2502 ghi     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:8},(0,i.kt)("li",{parentName:"ol"},"Restart the stopped node and select from there also")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql",metastring:'title="run on the restarted node"',title:'"run',on:!0,the:!0,restarted:!0,'node"':!0},"SELECT *\nFROM db1.table1\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  3 \u2502 ghi     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")))}f.isMDXComponent=!0},11246:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/Architecture.1S_2R_ReplicatedMergeTree_5-nodes.3.CH.Keeper.nodes.2.CH.nodes-68d9eac1202e94de78505876428a047f.png"}}]);