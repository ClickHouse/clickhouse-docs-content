"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[6729],{42425:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/en/faq/integration/file-export","metadata":{"permalink":"/docs/knowledgebase/en/faq/integration/file-export","source":"@site/knowledgebase/file-export.md","title":"How do I export data from ClickHouse to a file?","description":"Using INTO OUTFILE Clause","date":"2023-03-22T00:00:00.000Z","formattedDate":"March 22, 2023","tags":[],"readingTime":1.51,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/en/faq/integration/file-export","title":"How do I export data from ClickHouse to a file?","toc_hidden":true,"toc_priority":10,"date":"2023-03-22T00:00:00.000Z"},"nextItem":{"title":"How to ingest Parquet files from an S3 bucket","permalink":"/docs/knowledgebase/ingest-parquet-files-in-s3"}},"content":"## Using INTO OUTFILE Clause {#using-into-outfile-clause}\\n\\nAdd an [INTO OUTFILE](https://clickhouse.com/docs/en/sql-reference/statements/select/into-outfile.md) clause to your query.\\n\\nFor example:\\n\\n``` sql\\nSELECT * FROM table INTO OUTFILE \'file\'\\n```\\n\\nBy default, ClickHouse uses the file extension of the filename to deteremine the output format and compression. For example, all of the rows in `nyc_taxi` will be exported to the `nyc_taxi.parquet` using the Parquet format:\\n\\n``` sql\\nSELECT *\\nFROM nyc_taxi\\nINTO OUTFILE \'taxi_rides.parquet\'\\n```\\n\\nAnd the following file will be a compressed, tab-separated file:\\n\\n``` sql\\nSELECT *\\nFROM nyc_taxi\\nINTO OUTFILE \'taxi_rides.tsv.gz\'\\n```\\n\\nIf ClickHouse can not determine the format from the file extension, then the output format defaults to [TabSeparated](https://clickhouse.com/docs/en/interfaces/formats.md) for output data. To specify the [output format](https://clickhouse.com/docs/en/interfaces/formats.md), use the [FORMAT clause](https://clickhouse.com/docs/en/sql-reference/statements/select/format.md).\\n\\nFor example:\\n\\n``` sql\\nSELECT *\\nFROM nyc_taxi\\nINTO OUTFILE \'taxi_rides.txt\'\\nFORMAT CSV\\n```\\n\\n## Using a File-Engine Table {#using-a-file-engine-table}\\n\\nAnother option is to use the [File](https://clickhouse.com/docs/en/engines/table-engines/special/file.md) table engine, where ClickHouse uses the file to store the data. You can perform queries and inserts directly on the file.\\n\\nFor example:\\n\\n```sql\\nCREATE TABLE my_table (\\n   x UInt32,\\n   y String,\\n   z DateTime\\n)\\nENGINE = File(Parquet)\\n```\\n\\nInsert a few rows:\\n\\n```sql\\nINSERT INTO my_table VALUES\\n   (1, \'Hello\', now()),\\n   (2, \'World\', now()),\\n   (3, \'Goodbye\', now())\\n```\\n\\nThe file is stored in the `data` folder of your ClickHouse server - specifically in `/data/default/my_table` in a file named `data.Parquet`.\\n\\n:::note\\nUsing the `File` table engine is incredibly handy for creating and querying files on your file system, but keep in mind that `File` tables are not `MergeTree` tables, so you don\'t get all the benefits that come with `MergeTree`. Use `File` for convenience when exporting data out of ClickHouse in convenient formats.\\n:::\\n\\n## Using Command-Line Redirection {#using-command-line-redirection}\\n\\n``` bash\\n$ clickhouse-client --query \\"SELECT * from table\\" --format FormatName > result.txt\\n```\\n\\nSee [clickhouse-client](https://clickhouse.com/docs/en/interfaces/cli.md)."},{"id":"/ingest-parquet-files-in-s3","metadata":{"permalink":"/docs/knowledgebase/ingest-parquet-files-in-s3","source":"@site/knowledgebase/ingest-parquet-files-in-s3.md","title":"How to ingest Parquet files from an S3 bucket","description":"Below are some basics of using the S3 table engine to read parquet files.","date":"2023-03-22T00:00:00.000Z","formattedDate":"March 22, 2023","tags":[],"readingTime":2.88,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-22T00:00:00.000Z"},"prevItem":{"title":"How do I export data from ClickHouse to a file?","permalink":"/docs/knowledgebase/en/faq/integration/file-export"},"nextItem":{"title":"Converting Files from Parquet to CSV or JSON","permalink":"/docs/knowledgebase/parquet-to-csv-json"}},"content":"Below are some basics of using the S3 table engine to read parquet files.\\n\\n- create access and secret keys for an IAM service user.\\nnormal login users usually don\'t work since they may have been configured with an MFA policy.\\n\\n- set the permissions on the policy to allow the service user to access the bucket and folders.\\n\\nThe following is a very simple example that you can use to test the mechanics of accessing your parquet files successfully prior to applying to your actual data.\\n\\nIf you need an example of creating a user and bucket, you can follow the first two sections (create user and create bucket):\\nhttps://clickhouse.com/docs/en/guides/sre/configuring-s3-for-clickhouse-use/\\n\\nI used this sample file: https://github.com/Teradata/kylo/tree/master/samples/sample-data/parquet\\nand uploaded it to my test bucket\\n\\nYou can set the policy something like this on the bucket:\\n(adjust as needed, this one is fairly open for privileges but will help in testing. you can narrow your permissions as necessary)\\n```\\n{\\n    \\"Version\\": \\"2012-10-17\\",\\n    \\"Id\\": \\"Policy123456\\",\\n    \\"Statement\\": [\\n        {\\n            \\"Sid\\": \\"abc123\\",\\n            \\"Effect\\": \\"Allow\\",\\n            \\"Principal\\": {\\n                \\"AWS\\": [\\n                    \\"arn:aws:iam::1234567890:user/mars-s3-user\\"\\n                ]\\n            },\\n            \\"Action\\": \\"s3:*\\",\\n            \\"Resource\\": [\\n                \\"arn:aws:s3:::mars-doc-test\\",\\n                \\"arn:aws:s3:::mars-doc-test/*\\"\\n            ]\\n        }\\n    ]\\n}\\n```\\n\\nYou can run queries with this type of syntax using the S3 table engine:\\nhttps://clickhouse.com/docs/en/sql-reference/table-functions/s3/\\n\\n```\\nclickhouse-cloud :)  select count(*) from s3(\'https://mars-doc-test.s3.amazonaws.com/s3-parquet-test/userdata1.parquet\',\'ABC123\', \'abc+123\', \'Parquet\', \'first_name String\');\\n\\nSELECT count(*)\\nFROM s3(\'https://mars-doc-test.s3.amazonaws.com/s3-parquet-test/userdata1.parquet\', \'ABC123\', \'abc+123\', \'Parquet\', \'first_name String\')\\n\\nQuery id: fd4f1193-d604-4ac0-9a46-bdd2d5e14727\\n\\n\u250c\u2500count()\u2500\u2510\\n\u2502    1000 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 1.274 sec. Processed 1.00 thousand rows, 14.64 KB (784.81 rows/s., 11.49 KB/s.)\\n```\\n\\nThe data types reference for parquet format are here:\\nhttps://clickhouse.com/docs/en/interfaces/formats/#data-format-parquet\\n\\nTo bring in the data into a native ClickHouse table:\\n\\ncreate the table, something like this (just chose a couple of the columns in the parquet file):\\n```\\nclickhouse-cloud :) CREATE TABLE my_parquet_table (id UInt64, first_name String) ENGINE = MergeTree ORDER BY id;\\n\\nCREATE TABLE my_parquet_table\\n(\\n    `id` UInt64,\\n    `first_name` String\\n)\\nENGINE = MergeTree\\nORDER BY id\\n\\nQuery id: 412e3994-bf8e-444e-ac43-a7c82642b7da\\n\\nOk.\\n\\n0 rows in set. Elapsed: 0.600 sec.\\n```\\n\\nSelect the data from the S3 bucket to insert into the new table:\\n\\n```\\nclickhouse-cloud :) INSERT INTO my_parquet_table (id, first_name) SELECT id, first_name FROM s3(\'https://mars-doc-test.s3.amazonaws.com/s3-parquet-test/userdata1.parquet\', \'ABC123\',\'abc+123\', \'Parquet\', \'id UInt64, first_name String\') FORMAT Parquet\\n\\nINSERT INTO my_parquet_table (id, first_name) SELECT\\n    id,\\n    first_name\\nFROM s3(\'https://mars-doc-test.s3.amazonaws.com/s3-parquet-test/userdata1.parquet\', \'ABC123\', \'abc+123\', \'Parquet\', \'id UInt64, first_name String\')\\n\\nQuery id: c3cdc871-f338-462d-8797-6751b45a0b58\\n\\nOk.\\n\\n0 rows in set. Elapsed: 1.220 sec. Processed 1.00 thousand rows, 22.64 KB (819.61 rows/s., 18.56 KB/s.)\\n```\\n\\nVerify the import:\\n\\n```\\nclickhouse-cloud :) SELECT * FROM my_parquet_table LIMIT 10;\\n\\nSELECT *\\nFROM my_parquet_table\\nLIMIT 10\\n\\nQuery id: 1ccf59dd-d804-46a9-aadd-ed5c57b9e1a0\\n\\n\u250c\u2500id\u2500\u252c\u2500first_name\u2500\u2510\\n\u2502  1 \u2502 Amanda     \u2502\\n\u2502  2 \u2502 Albert     \u2502\\n\u2502  3 \u2502 Evelyn     \u2502\\n\u2502  4 \u2502 Denise     \u2502\\n\u2502  5 \u2502 Carlos     \u2502\\n\u2502  6 \u2502 Kathryn    \u2502\\n\u2502  7 \u2502 Samuel     \u2502\\n\u2502  8 \u2502 Harry      \u2502\\n\u2502  9 \u2502 Jose       \u2502\\n\u2502 10 \u2502 Emily      \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nWhen you are ready to import your real data, you can use some special syntax like wildcards and ranges to specify your folders, subfolders and files in your bucket.\\nI\'d recommend to filter a few directories and files to test the import, maybe a certain year, a couple months and some date range to test first.\\n\\nbesides the path options here, newly released is syntax `**` which specifies all subdirectories recursively.\\nhttps://clickhouse.com/docs/en/sql-reference/table-functions/s3/\\n\\nFor example, assuming the paths and bucket structure is something like this:\\n`https://your_s3_bucket.s3.amazonaws.com/<your_folder>/<year>/<month>/<day>/<filename>.parquet`\\n`https://mars-doc-test.s3.amazonaws.com/system_logs/2022/11/01/my-app-logs-0001.parquet`\\n\\nThis would get all files for 1st day of every month in 2021-2022\\n`https://mars-doc-test.s3.amazonaws.com/system_logs/{2021-2022}/**/01/*.parquet`"},{"id":"/parquet-to-csv-json","metadata":{"permalink":"/docs/knowledgebase/parquet-to-csv-json","source":"@site/knowledgebase/parquet-to-csv-json.md","title":"Converting Files from Parquet to CSV or JSON","description":"You can use clickhouse-local to convert files between any of the input and output formats that ClickHouse supports (which is over 70 different formats!). In this article, we are convert a Parquet file in S3 into a CSV and JSON file.","date":"2023-03-22T00:00:00.000Z","formattedDate":"March 22, 2023","tags":[],"readingTime":2.765,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-22T00:00:00.000Z"},"prevItem":{"title":"How to ingest Parquet files from an S3 bucket","permalink":"/docs/knowledgebase/ingest-parquet-files-in-s3"},"nextItem":{"title":"Export PostgreSQL data to Parquet, CSV or JSON","permalink":"/docs/knowledgebase/postgresql-to-parquet-csv-json"}},"content":"You can use `clickhouse-local` to convert files between any of the [input and output formats](https://clickhouse.com/docs/en/interfaces/formats) that ClickHouse supports (which is over 70 different formats!). In this article, we are convert a Parquet file in S3 into a CSV and JSON file.\\n\\nLet\'s start at the beginning. ClickHouse has a collection of [table functions](https://clickhouse.com/docs/en/sql-reference/table-functions/) that read from files, databases and other resoures and converts the data to a table. To demonstrate, suppose we have a Parquet file in S3. We will use the `s3` table function to read it (ClickHouse knows it\'s a Parquet file based on the filename).\\n\\nBut first, let\'s download the `clickhouse` binary:\\n\\n```bash\\ncurl https://clickhouse.com/ | sh\\n```\\n\\n## Accessing the data using a table function\\n\\nLet\'s verify we can read the file by using `DESCRIBE` on the resulting table that the `s3` table function creates:\\n\\n```bash\\n./clickhouse local -q \\"DESCRIBE s3(\'https://datasets-documentation.s3.eu-west-3.amazonaws.com/house_parquet/house_0.parquet\')\\"\\n```\\n\\nThis particular file contains home prices of properties sold in the United Kingdom. The response looks like:\\n\\n```response\\nprice\\tNullable(Int64)\\ndate\\tNullable(UInt16)\\npostcode1\\tNullable(String)\\npostcode2\\tNullable(String)\\ntype\\tNullable(String)\\nis_new\\tNullable(UInt8)\\nduration\\tNullable(String)\\naddr1\\tNullable(String)\\naddr2\\tNullable(String)\\nstreet\\tNullable(String)\\nlocality\\tNullable(String)\\ntown\\tNullable(String)\\ndistrict\\tNullable(String)\\ncounty\\tNullable(String)\\n```\\n\\nYou can run any query you want on the data. For example, let\'s see which towns have the highest average price of homes:\\n\\n```bash\\n./clickhouse local -q \\"SELECT\\n   town,\\n   avg(price) AS avg_price\\nFROM s3(\'https://datasets-documentation.s3.eu-west-3.amazonaws.com/house_parquet/house_0.parquet\')\\nGROUP BY town\\nORDER BY avg_price DESC\\nLIMIT 10\\"\\n```\\n\\nThe response looks like:\\n\\n```bash\\nGATWICK\\t16818750\\nCHALFONT ST GILES\\t938090.0985915493\\nVIRGINIA WATER\\t789301.1320224719\\nCOBHAM\\t699874.7111622555\\nBEACONSFIELD\\t677247.5483146068\\nESHER\\t616004.6888297872\\nKESTON\\t607585.8597560975\\nGERRARDS CROSS\\t566330.2959086584\\nASCOT\\t551491.2975753123\\nWEYBRIDGE\\t548974.828692494\\n```\\n\\n## Convert the Parquet file to a CSV\\n\\nYou can send the result of any SQL query to a file. Let\'s grab all the columns from our Parquet file in S3 and send the output to a new CSV file. Because the output file ends in `.csv`, ClickHouse knows to use the `CSV` output format:\\n\\n```bash\\n./clickhouse local -q \\"SELECT *\\nFROM s3(\'https://datasets-documentation.s3.eu-west-3.amazonaws.com/house_parquet/house_0.parquet\')\\nINTO OUTFILE \'house_prices.csv\'\\"\\n```\\n\\nLet\'s verify it worked:\\n\\n```response\\n$ tail house_prices.csv\\n70000,10508,\\"YO8\\",\\"9XN\\",\\"detached\\",0,\\"freehold\\",\\"7\\",\\"\\",\\"POPPY CLOSE\\",\\"SELBY\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n130000,14274,\\"YO8\\",\\"9XP\\",\\"detached\\",0,\\"freehold\\",\\"10\\",\\"\\",\\"HEATHER CLOSE\\",\\"\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n150000,18180,\\"YO8\\",\\"9XP\\",\\"detached\\",0,\\"freehold\\",\\"11\\",\\"\\",\\"HEATHER CLOSE\\",\\"\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n157000,18088,\\"YO8\\",\\"9XP\\",\\"detached\\",0,\\"freehold\\",\\"12\\",\\"\\",\\"HEATHER CLOSE\\",\\"\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n134000,17333,\\"YO8\\",\\"9XP\\",\\"semi-detached\\",0,\\"freehold\\",\\"16\\",\\"\\",\\"HEATHER CLOSE\\",\\"\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n250000,13405,\\"YO8\\",\\"9YA\\",\\"detached\\",0,\\"freehold\\",\\"6\\",\\"\\",\\"YORKDALE COURT\\",\\"HAMBLETON\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n59500,11166,\\"YO8\\",\\"9YB\\",\\"semi-detached\\",0,\\"freehold\\",\\"4\\",\\"\\",\\"YORKDALE DRIVE\\",\\"HAMBLETON\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n142500,17648,\\"YO8\\",\\"9YB\\",\\"semi-detached\\",0,\\"freehold\\",\\"4A\\",\\"\\",\\"YORKDALE DRIVE\\",\\"HAMBLETON\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n230000,15125,\\"YO8\\",\\"9YD\\",\\"detached\\",0,\\"freehold\\",\\"1\\",\\"\\",\\"ONE ACRE GARTH\\",\\"HAMBLETON\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n250000,15950,\\"YO8\\",\\"9YD\\",\\"detached\\",0,\\"freehold\\",\\"3\\",\\"\\",\\"ONE ACRE GARTH\\",\\"HAMBLETON\\",\\"SELBY\\",\\"SELBY\\",\\"NORTH YORKSHIRE\\"\\n```\\n\\n## Convert the Parquet file to a JSON\\n\\nTo convert the Parquet file to JSON, simply change the extension on the output filename:\\n\\n```bash\\n./clickhouse local -q \\"SELECT *\\nFROM s3(\'https://datasets-documentation.s3.eu-west-3.amazonaws.com/house_parquet/house_0.parquet\')\\nINTO OUTFILE \'house_prices.ndjson\'\\"\\n```\\n\\nLet\'s verify it worked:\\n\\n```response\\n $ tail house_prices.ndjson\\n{\\"price\\":\\"70000\\",\\"date\\":10508,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9XN\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"7\\",\\"addr2\\":\\"\\",\\"street\\":\\"POPPY CLOSE\\",\\"locality\\":\\"SELBY\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"130000\\",\\"date\\":14274,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9XP\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"10\\",\\"addr2\\":\\"\\",\\"street\\":\\"HEATHER CLOSE\\",\\"locality\\":\\"\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"150000\\",\\"date\\":18180,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9XP\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"11\\",\\"addr2\\":\\"\\",\\"street\\":\\"HEATHER CLOSE\\",\\"locality\\":\\"\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"157000\\",\\"date\\":18088,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9XP\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"12\\",\\"addr2\\":\\"\\",\\"street\\":\\"HEATHER CLOSE\\",\\"locality\\":\\"\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"134000\\",\\"date\\":17333,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9XP\\",\\"type\\":\\"semi-detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"16\\",\\"addr2\\":\\"\\",\\"street\\":\\"HEATHER CLOSE\\",\\"locality\\":\\"\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"250000\\",\\"date\\":13405,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9YA\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"6\\",\\"addr2\\":\\"\\",\\"street\\":\\"YORKDALE COURT\\",\\"locality\\":\\"HAMBLETON\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"59500\\",\\"date\\":11166,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9YB\\",\\"type\\":\\"semi-detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"4\\",\\"addr2\\":\\"\\",\\"street\\":\\"YORKDALE DRIVE\\",\\"locality\\":\\"HAMBLETON\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"142500\\",\\"date\\":17648,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9YB\\",\\"type\\":\\"semi-detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"4A\\",\\"addr2\\":\\"\\",\\"street\\":\\"YORKDALE DRIVE\\",\\"locality\\":\\"HAMBLETON\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"230000\\",\\"date\\":15125,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9YD\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"1\\",\\"addr2\\":\\"\\",\\"street\\":\\"ONE ACRE GARTH\\",\\"locality\\":\\"HAMBLETON\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n{\\"price\\":\\"250000\\",\\"date\\":15950,\\"postcode1\\":\\"YO8\\",\\"postcode2\\":\\"9YD\\",\\"type\\":\\"detached\\",\\"is_new\\":0,\\"duration\\":\\"freehold\\",\\"addr1\\":\\"3\\",\\"addr2\\":\\"\\",\\"street\\":\\"ONE ACRE GARTH\\",\\"locality\\":\\"HAMBLETON\\",\\"town\\":\\"SELBY\\",\\"district\\":\\"SELBY\\",\\"county\\":\\"NORTH YORKSHIRE\\"}\\n```\\n\\n## Convert CSV to Parquet\\n\\nIt works both ways - we can easily read in the new CSV file and output it into a Parquet file. The local file `house_prices.csv` can be read in ClickHouse using the `file` table function, and ClickHouse outputs the file in Parquet format based on the filename ending in `.parquet` (or we could have added the `FORMAT Parquet` clause):\\n\\n```bash\\n./clickhouse local -q \\"SELECT *\\nFROM file(\'house_prices.csv\')\\nINTO OUTFILE \'house_prices.parquet\'\\"\\n```\\n\\nAs we mentioned above, you can use any of the ClickHouse [input and output formats](https://clickhouse.com/docs/en/interfaces/formats) along with `clickhouse local` to easily convert files into different formats."},{"id":"/postgresql-to-parquet-csv-json","metadata":{"permalink":"/docs/knowledgebase/postgresql-to-parquet-csv-json","source":"@site/knowledgebase/postgresql-to-parquet-csv-json.md","title":"Export PostgreSQL data to Parquet, CSV or JSON","description":"This one is easy with clickhouse-local:","date":"2023-03-22T00:00:00.000Z","formattedDate":"March 22, 2023","tags":[],"readingTime":2.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-22T00:00:00.000Z"},"prevItem":{"title":"Converting Files from Parquet to CSV or JSON","permalink":"/docs/knowledgebase/parquet-to-csv-json"},"nextItem":{"title":"Export MySQL data to Parquet, CSV or JSON","permalink":"/docs/knowledgebase/mysql-to-parquet-csv-json"}},"content":"This one is easy with `clickhouse-local`:\\n\\n- Use the [`postgresql` table function](https://clickhouse.com/docs/en/sql-reference/table-functions/postgresql) to read the data\\n- Use the `INTO OUTFILE _filename_ FORMAT` clause and specify the desired output format\\n\\nThe output format can be any of the supported [ouput formats](https://clickhouse.com/docs/en/interfaces/formats) in ClickHouse. Let\'s look at a few examples...\\n\\nThese examples use `clickhouse-local`, which is a part of the ClickHouse binary. Download it using the following:\\n\\n```bash\\ncurl https://clickhouse.com/ | sh\\n```\\n\\n## Export PostgreSQL to Parquet\\n\\nThe `postgresql` table function allows `SELECT` (and `INSERT`) queries to be performed on data that is stored on a remote PostgreSQL server. For example, to view the entire contents of a table in PostgreSQL:\\n\\n```bash\\nSELECT *\\nFROM\\n   postgresql(\\n    \'localhost:5432\',\\n    \'postgres_database\',\\n    \'postgres_table\',\\n    \'user\',\\n    \'password\'\\n);\\n```\\n\\nWe can pipe the output of this query to a file using `INTO OUTFILE`. Use `FORMAT` to specify the format of the file to be created. Let\'s grab the entire contents of the PostgreSQL table, and send its contents to a Parquet file:\\n\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   postgresql(\\n    \'localhost:5432\',\\n    \'postgres_database\',\\n    \'postgres_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.parquet\'\\"\\n```\\n\\n:::note\\nBecause the name of the output file has a `.parquet` extension, ClickHouse assumes we want the Parquet format, so notice we omitted the `FORMAT Parquet` clause.\\n:::\\n\\n## Export PostgreSQL to CSV\\n\\nIt\'s the same as for Parquet, except we specify a more approriate filename for the output:\\n\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   postgresql(\\n    \'localhost:5432\',\\n    \'postgres_database\',\\n    \'postgres_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.csv\'\\"\\n```\\n\\nThat\'s it! ClickHouse sees the `.csv` extension on the output file name and outputs the data as comma-separated. Otherwise, it\'s the exact same command as above.\\n\\n## Export PostgreSQL to JSON\\n\\nTo go from PostgreSQL to JSON, we just change the filename and ClickHouse will figure out the format:\\n\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   postgresql(\\n    \'localhost:5432\',\\n    \'postgres_database\',\\n    \'postgres_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.ndjson\'\\"\\n```\\n\\n:::note\\nYou don\'t have to stop here - you can use `clickhouse-local` to pull data from PostgreSQL and send it to [all types of output formats](https://clickhouse.com/docs/en/sql-reference/formats/).\\n\\nIf ClickHouse can not determine the output type by the filename extension, or if you want to specifically choose a format, add the `FOMRAT` clause:\\n\\n```sql\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   postgresql(\\n    \'localhost:5432\',\\n    \'postgres_database\',\\n    \'postgres_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.ndjson\'\\nFORMAT JSONEachRow\\"\\n```\\n:::\\n\\n## Stream PostgreSQL to another process\\n\\nInstead of using `INTO OUTFILE`, you can stream the results of a table function to another process. Here\'s a simple example to demonstrate the syntax - we count the number of rows using the Linux `wc -l` command:\\n\\n```bash\\n./clickhouse local -q \\"SELECT *\\nFROM s3(\'https://datasets-documentation.s3.eu-west-3.amazonaws.com/house_parquet/house_0.parquet\'\\nFORMAT JSONEachRow\\n)\\" | wc -l\\n```\\n\\nHowever, we could easily stream the rows to a shell script, Python script, or any other process that you want."},{"id":"/mysql-to-parquet-csv-json","metadata":{"permalink":"/docs/knowledgebase/mysql-to-parquet-csv-json","source":"@site/knowledgebase/mysql-to-parquet-csv-json.md","title":"Export MySQL data to Parquet, CSV or JSON","description":"The clickhouse-local tool makes it quick and easy to read data from MySQL and output the data into lots of different formats, including Parquet, CSV, and JSON. We are going to:","date":"2023-03-21T00:00:00.000Z","formattedDate":"March 21, 2023","tags":[],"readingTime":1.665,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-21T00:00:00.000Z"},"prevItem":{"title":"Export PostgreSQL data to Parquet, CSV or JSON","permalink":"/docs/knowledgebase/postgresql-to-parquet-csv-json"},"nextItem":{"title":"Code: 279. DB::NetException: All connection tries failed.","permalink":"/docs/knowledgebase/connection_timeout_remote_remoteSecure"}},"content":"The `clickhouse-local` tool makes it quick and easy to read data from MySQL and output the data into lots of different formats, including Parquet, CSV, and JSON. We are going to:\\n\\n- Use the [`mysql` table function](https://clickhouse.com/docs/en/sql-reference/table-functions/mysql) to read the data\\n- Use the `INTO OUTFILE _filename_ FORMAT` clause and specify the desired output format\\n\\nThe `clickhouse-local` tool is a part of the ClickHouse binary. Download it using the following:\\n\\n```bash\\ncurl https://clickhouse.com/ | sh\\n```\\n\\n## Export MySQL to Parquet\\n\\nThe `mysql` table function creates a table based on the results of a query sent to a MySQL instance. For example:\\n\\n```bash\\nSELECT *\\nFROM\\n   mysql(\\n    \'localhost:3306\',\\n    \'my_sql_database\',\\n    \'my_sql_table\',\\n    \'user\',\\n    \'password\'\\n);\\n```\\n\\nWe can pipe the output of this query to a file using `INTO OUTFILE`. Use `FORMAT` to specify the format of the file to be created. Let\'s grab the entire contents of a MySQL table, and send its contents to a Parquet file:\\n\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   mysql(\\n    \'localhost:3306\',\\n    \'my_sql_database\',\\n    \'my_sql_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.parquet\'\\"\\n```\\n\\n:::note\\nBecause the name of the output file has a `.parquet` extension, ClickHouse assumes we want the Parquet format, so notice we omitted the `FORMAT Parquet` clause.\\n:::\\n\\n## Export MySQL to CSV\\n\\nIt\'s the same as for Parquet, except this time we use a `.csv` extension on the filename. ClickHouse will realize we want a comma-separated output and that\'s how the data will be written to the file:\\n\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   mysql(\\n    \'localhost:3306\',\\n    \'my_sql_database\',\\n    \'my_sql_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.csv\'\\"\\n```\\n\\n## Export MySQL to JSON\\n\\nTo go from MySQL to JSON, just change the extension on the filename to `jsonl` or `ndjson`:\\n\\n```bash\\n./clickhouse local -q \\"SELECT * FROM\\n   mysqlql(\\n    \'localhost:3306\',\\n    \'my_sql_database\',\\n    \'my_sql_table\',\\n    \'user\',\\n    \'password\'\\n)\\nINTO OUTFILE \'my_output_file.ndjson\'\\"\\n```\\n\\nIt\'s impressive how simple yet powerful the `clickhouse-local` tool really is. You can easily read data from a database like MySQL and output it into [all types of different output formats](https://clickhouse.com/docs/en/sql-reference/formats/)."},{"id":"/connection_timeout_remote_remoteSecure","metadata":{"permalink":"/docs/knowledgebase/connection_timeout_remote_remoteSecure","source":"@site/knowledgebase/connection_timeout_remote_remoteSecure.md","title":"Code: 279. DB::NetException: All connection tries failed.","description":"When using the `remote` or `remoteSecure` table functions on a node that is located more than 100ms (latency wise) away from the remote node, it is common to encounter the following timeout error.","date":"2023-03-20T00:00:00.000Z","formattedDate":"March 20, 2023","tags":[],"readingTime":1.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"description":"When using the `remote` or `remoteSecure` table functions on a node that is located more than 100ms (latency wise) away from the remote node, it is common to encounter the following timeout error.","date":"2023-03-20T00:00:00.000Z"},"prevItem":{"title":"Export MySQL data to Parquet, CSV or JSON","permalink":"/docs/knowledgebase/mysql-to-parquet-csv-json"},"nextItem":{"title":"DB::Exception: Too many parts (600). Merges are processing significantly slower than inserts","permalink":"/docs/knowledgebase/exception-too-many-parts"}},"content":"**Problem**\\n[`remote()` or `remoteSecure()`](https://clickhouse.com/docs/en/sql-reference/table-functions/remote/) table function allows the access of remote table from another ClickHouse node.\\n\\nWhen using these functions on a node that is located more than 100ms (latency wise) away from the remote node, it is common to encounter the following timeout error.\\n\\n```\\n4776d4bd8190 :) SELECT * FROM remoteSecure(\'HOSTNAME.us-east-2.aws.clickhouse.cloud\', DATABASE, TABLE, \'USER\', \'USER_PASSWORD\')\\n\\nSELECT *\\nFROM remoteSecure(\'HOSTNAME.us-east-2.aws.clickhouse.cloud\', DATABASE, TABLE, \'USER\', \'USER_PASSWORD\')\\n\\nQuery id: 2bd6ddd0-66d9-4d19-830f-87e3cec3724b\\n\\n\\n0 rows in set. Elapsed: 1.213 sec.\\n\\nReceived exception from server (version 22.6.9):\\nCode: 519. DB::Exception: Received from localhost:9000. DB::NetException. DB::NetException: All attempts to get table structure failed. Log:\\n\\nCode: 279. DB::NetException: All connection tries failed. Log:\\n\\nCode: 209. DB::NetException: Timeout: connect timed out: 18.218.245.169:9440 (hc7d963h1t.us-east-2.aws.clickhouse.cloud:9440, connection timeout 100 ms). (SOCKET_TIMEOUT) (version 22.6.9.11 (official build))\\nCode: 209. DB::NetException: Timeout: connect timed out: 18.218.245.169:9440 (hc7d963h1t.us-east-2.aws.clickhouse.cloud:9440, connection timeout 100 ms). (SOCKET_TIMEOUT) (version 22.6.9.11 (official build))\\nCode: 209. DB::NetException: Timeout: connect timed out: 18.218.245.169:9440 (hc7d963h1t.us-east-2.aws.clickhouse.cloud:9440, connection timeout 100 ms). (SOCKET_TIMEOUT) (version 22.6.9.11 (official build))\\n\\n. (ALL_CONNECTION_TRIES_FAILED) (version 22.6.9.11 (official build))\\n\\n. (NO_REMOTE_SHARD_AVAILABLE)\\n```\\n\\n**Workaround**\\nTo get increase the connection timeout, set [`connect_timeout_with_failover_secure_ms`](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.h#L67) to a higher value (e.g. 1 second) from the default 100ms.\\n\\n```\\n4776d4bd8190 :) SELECT * FROM remoteSecure(\'HOSTNAME.us-east-2.aws.clickhouse.cloud:9440\', DATABASE, TABLE, \'USER\', \'USER_PASSWORD\') SETTINGS connect_timeout_with_failover_secure_ms = 1000\\n\\nSELECT *\\nFROM remoteSecure(\'HOSTNAME.us-east-2.aws.clickhouse.cloud:9440\', DATABASE, TABLE, \'USER\', \'USER_PASSWORD\')\\nSETTINGS connect_timeout_with_failover_secure_ms = 1000\\n\\nQuery id: 8e2f4d41-307b-4e61-abb8-809190023247\\n\\n\u250c\u2500x\u2500\u2510\\n\u2502 1 \u2502\\n\u2514\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 2.403 sec.\\n```"},{"id":"/exception-too-many-parts","metadata":{"permalink":"/docs/knowledgebase/exception-too-many-parts","source":"@site/knowledgebase/exception-too-many-parts.md","title":"DB::Exception: Too many parts (600). Merges are processing significantly slower than inserts","description":"The main requirement about inserting into Clickhouse: you should never send too many INSERT statements per second. Ideally - one insert per second / per few seconds.","date":"2023-03-20T00:00:00.000Z","formattedDate":"March 20, 2023","tags":[],"readingTime":2.58,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-20T00:00:00.000Z"},"prevItem":{"title":"Code: 279. DB::NetException: All connection tries failed.","permalink":"/docs/knowledgebase/connection_timeout_remote_remoteSecure"},"nextItem":{"title":"How to import JSON into ClickHouse?","permalink":"/docs/knowledgebase/json-import"}},"content":"The main requirement about inserting into Clickhouse: you should never send too many `INSERT` statements per second. Ideally - one insert per second / per few seconds.\\n\\nSo you can insert 100K rows per second but only with one big bulk `INSERT` statement. When you send hundreds / thousands insert statements per second to *MergeTree table you will always get some errors, and it can not be changed by adjusting some settings.\\n\\nIf you can\'t combine lot of inserts into one big bulk insert statement outside - then you should create Buffer table before *MergeTree table.\\n\\n1. Each insert create a folder in  `/var/lib/clickhouse/.../table_name/`. Inside that folder there are 2 files per each column - one with data (compressed), second with index. Data is physically sorted by primary key inside those files. Those folders are called \'**parts**\'.\\n\\n2. ClickHouse merges those smaller parts to bigger parts in the background. It chooses parts to merge according to some rules. After merging two (or more) parts one bigger part is being created and old parts are queued to be removed. The settings you list allow finetuning the rules of merging parts. The goal of merging process - is to leave one big part for each partition (or few big parts per partition which are not worth to merge because they are too big). Please check also that [comment](https://github.com/yandex/ClickHouse/issues/1661#issuecomment-352739726).\\n\\n3. If you create new parts too fast (for example by doing lot of small inserts) and ClickHouse is not able to merge them with proper speed (so new parts come faster than ClickHouse can merge them) - then you get the exception \'Merges are processing significantly slower than inserts\'. You can try to increase the limit but you can get the situation then you get filesystem problems caused by the too big number of files / directories (like inodes limit).\\n\\n4. If you insert to lot of partitions at once the problem is multiplied by the number of partitions affected by insert.\\n\\n5. You can try to adjust the behaviour of clickhouse with one of the listed settings, or with max_insert_block_size / max_block_size  / insert_format_max_block_size / max_client_network_bandwidth.  But: the better solution is just to insert data in expected tempo. The expected tempo is: **one insert per 1-2 sec, each insert containing 10K-500K rows of data**.\\n\\n6. So proper solution to solve \\"Merges are processing significantly slower than inserts\\"  is to adjust the number of inserts per second and number of rows in each insert. Use batch insert to combine small inserts into one bigger if data comes row-by-row. Throttle huge inserts if you have too much data to insert at once. Don\'t change clickhouse internals, unless you really understand well what does they it mean.\\n\\n7. If your data comes faster than 500K rows per second - most probably you need more servers in the cluster to serve that traffic, not the adjustment of settings.\\n\\n8. The speed of background merges usually depends on storage speed, used compression settings, and mergetree option, i.e. merge algorithm - plain merge / aggregating / summing / collapsing etc. &  used soring key."},{"id":"/json-import","metadata":{"permalink":"/docs/knowledgebase/json-import","source":"@site/knowledgebase/json-import.md","title":"How to import JSON into ClickHouse?","description":"There are multiple JSON variations among them, but the most commonly used for data ingestion is JSONEachRow. It expects one JSON object per row, each object separated by a newline.","date":"2023-03-20T00:00:00.000Z","formattedDate":"March 20, 2023","tags":[],"readingTime":0.755,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to import JSON into ClickHouse?","description":"There are multiple JSON variations among them, but the most commonly used for data ingestion is JSONEachRow. It expects one JSON object per row, each object separated by a newline.","date":"2023-03-20T00:00:00.000Z"},"prevItem":{"title":"DB::Exception: Too many parts (600). Merges are processing significantly slower than inserts","permalink":"/docs/knowledgebase/exception-too-many-parts"},"nextItem":{"title":"Useful queries for troubleshooting","permalink":"/docs/knowledgebase/useful-queries-for-troubleshooting"}},"content":"ClickHouse supports a wide range of [data formats for input and output](https://clickhouse.com/docs/en/interfaces/formats/). There are multiple JSON variations among them, but the most commonly used for data ingestion is [JSONEachRow](https://clickhouse.com/docs/en/interfaces/formats/#jsoneachrow). It expects one JSON object per row, each object separated by a newline.\\n\\n## Examples {#examples}\\n\\nUsing [HTTP interface](https://clickhouse.com/docs/en/interfaces/http/):\\n\\n``` bash\\n$ echo \'{\\"foo\\":\\"bar\\"}\' | curl \'http://localhost:8123/?query=INSERT%20INTO%20test%20FORMAT%20JSONEachRow\' --data-binary @-\\n```\\n\\nUsing [CLI interface](https://clickhouse.com/docs/en/interfaces/cli/):\\n\\n``` bash\\n$ echo \'{\\"foo\\":\\"bar\\"}\'  | clickhouse-client --query=\\"INSERT INTO test FORMAT JSONEachRow\\"\\n```\\n\\nInstead of inserting data manually, you might consider to use one of [client libraries](https://clickhouse.com/docs/en/interfaces/) instead.\\n\\n## Useful Settings {#useful-settings}\\n\\n-   `input_format_skip_unknown_fields` allows to insert JSON even if there were additional fields not present in table schema (by discarding them).\\n-   `input_format_import_nested_json` allows to insert nested JSON objects into columns of [Nested](https://clickhouse.com/docs/en/sql-reference/data-types/nested-data-structures/nested/) type.\\n\\n:::note\\nSettings are specified as `GET` parameters for the HTTP interface or as additional command-line arguments prefixed with `--` for the `CLI` interface.\\n:::"},{"id":"/useful-queries-for-troubleshooting","metadata":{"permalink":"/docs/knowledgebase/useful-queries-for-troubleshooting","source":"@site/knowledgebase/useful-queries-for-troubleshooting.md","title":"Useful queries for troubleshooting","description":"In no particular order, here are some handy queries for troubleshooting ClickHouse and figuring out what is happening. We also have a great blog with some essential queries for monitoring ClickHouse.","date":"2023-03-17T00:00:00.000Z","formattedDate":"March 17, 2023","tags":[],"readingTime":3.74,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-17T00:00:00.000Z"},"prevItem":{"title":"How to import JSON into ClickHouse?","permalink":"/docs/knowledgebase/json-import"},"nextItem":{"title":"It\'s Pi Day! Let\'s calculate pi using SQL","permalink":"/docs/knowledgebase/calculate-pi-using-sql"}},"content":"In no particular order, here are some handy queries for troubleshooting ClickHouse and figuring out what is happening. We also have a great blog with some [essential queries for monitoring ClickHouse](https://clickhouse.com/blog/monitoring-troubleshooting-select-queries-clickhouse).\\n\\n## View which settings have been changed from the default\\n\\n```sql\\nSELECT\\n    name,\\n    value\\nFROM system.settings\\nWHERE changed\\n```\\n\\n## Get the size of all your tables\\n\\n```sql\\nSELECT table,\\n    formatReadableSize(sum(bytes)) as size\\n    FROM system.parts\\n    WHERE active\\nGROUP BY table\\n```\\n\\nThe response looks like:\\n\\n```response\\n\u250c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500size\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 stat            \u2502 38.89 MiB \u2502\\n\u2502 customers       \u2502 525.00 B  \u2502\\n\u2502 my_sparse_table \u2502 40.73 MiB \u2502\\n\u2502 crypto_prices   \u2502 32.18 MiB \u2502\\n\u2502 hackernews      \u2502 6.23 GiB  \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n## Row count and average day size of your table\\n\\n```sql\\nSELECT\\n    table,\\n    formatReadableSize(size) AS size,\\n    rows,\\n    days,\\n    formatReadableSize(avgDaySize) AS avgDaySize\\nFROM\\n(\\n    SELECT\\n        table,\\n        sum(bytes) AS size,\\n        sum(rows) AS rows,\\n        min(min_date) AS min_date,\\n        max(max_date) AS max_date,\\n        max_date - min_date AS days,\\n        size / (max_date - min_date) AS avgDaySize\\n    FROM system.parts\\n    WHERE active\\n    GROUP BY table\\n    ORDER BY rows DESC\\n)\\n```\\n\\n## Compression columns percentage as well as the size of primary index in memory\\n\\nYou can see how compressed your data is by column. This query also returns the size of your primary indexes in memory - useful to know because primary indexes must fit in memory.\\n\\n```sql\\nSELECT\\n    parts.*,\\n    columns.compressed_size,\\n    columns.uncompressed_size,\\n    columns.compression_ratio,\\n    columns.compression_percentage\\nFROM\\n(\\n    SELECT\\n        table,\\n        formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\\n        formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\\n        round(sum(data_compressed_bytes) / sum(data_uncompressed_bytes), 3) AS compression_ratio,\\n        round(100 - ((sum(data_compressed_bytes) * 100) / sum(data_uncompressed_bytes)), 3) AS compression_percentage\\n    FROM system.columns\\n    GROUP BY table\\n) AS columns\\nRIGHT JOIN\\n(\\n    SELECT\\n        table,\\n        sum(rows) AS rows,\\n        max(modification_time) AS latest_modification,\\n        formatReadableSize(sum(bytes)) AS disk_size,\\n        formatReadableSize(sum(primary_key_bytes_in_memory)) AS primary_keys_size,\\n        any(engine) AS engine,\\n        sum(bytes) AS bytes_size\\n    FROM system.parts\\n    WHERE active\\n    GROUP BY\\n        database,\\n        table\\n) AS parts ON columns.table = parts.table\\nORDER BY parts.bytes_size DESC\\n```\\n\\n## Number of queries sent by client in the last 10 minutes\\n\\nFeel free to increase or decrease the time interval in the `toIntervalMinute(10)` function:\\n\\n```sql\\nSELECT\\n    client_name,\\n    count(),\\n    query_kind,\\n    toStartOfMinute(event_time) AS event_time_m\\nFROM system.query_log\\nWHERE (type = \'QueryStart\') AND (event_time > (now() - toIntervalMinute(10)))\\nGROUP BY\\n    event_time_m,\\n    client_name,\\n    query_kind\\nORDER BY\\n    event_time_m DESC,\\n    count() ASC\\n```\\n\\n## Number of parts in each partition\\n\\n```sql\\nSELECT\\n    concat(database, \'.\', table),\\n    partition_id,\\n    count()\\nFROM system.parts\\nWHERE active\\nGROUP BY\\n    database,\\n    table,\\n    partition_id\\n```\\n\\n## Finding long running queries\\n\\nThis can help find queries that are stuck:\\n\\n```sql\\nSELECT\\n    elapsed,\\n    initial_user,\\n    client_name,\\n    hostname(),\\n    query_id,\\n    query\\nFROM clusterAllReplicas(default, system.processes)\\nORDER BY elapsed DESC\\n```\\n\\nUsing the query id of the worst running query, we can get a stack trace that can help when debugging.\\n\\n```\\nSET allow_introspection_functions=1;\\n\\nSELECT\\n    arrayStringConcat(\\n        arrayMap(\\n            x,\\n            y -> concat(x, \': \', y),\\n            arrayMap(x -> addressToLine(x), trace),\\n            arrayMap(x -> demangle(addressToSymbol(x)), trace)\\n        ),\\n        \'\\\\n\'\\n    ) as trace\\nFROM\\n    system.stack_trace\\nWHERE\\n    query_id = \'0bb6e88b-9b9a-4ffc-b612-5746c859e360\';\\n```\\n\\n## View the most recent errors\\n\\n```\\nSELECT *\\nFROM system.errors\\nORDER BY last_error_time DESC\\n```\\n\\nThe response looks like:\\n\\n```response\\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500code\u2500\u252c\u2500value\u2500\u252c\u2500\u2500\u2500\u2500\u2500last_error_time\u2500\u252c\u2500last_error_message\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500last_error_trace\u2500\u252c\u2500remote\u2500\u2510\\n\u2502 UNKNOWN_TABLE         \u2502   60 \u2502     3 \u2502 2023-03-14 01:02:35 \u2502 Table system.stack_trace doesn\'t exist                                                                                                              \u2502 []               \u2502      0 \u2502\\n\u2502 BAD_GET               \u2502  170 \u2502     1 \u2502 2023-03-14 00:58:55 \u2502 Requested cluster \'default\' not found                                                                                                               \u2502 []               \u2502      0 \u2502\\n\u2502 UNKNOWN_IDENTIFIER    \u2502   47 \u2502     1 \u2502 2023-03-14 00:49:12 \u2502 Missing columns: \'parts.table\' \'table\' while processing query: \'table = parts.table\', required columns: \'table\' \'parts.table\' \'table\' \'parts.table\' \u2502 []               \u2502      0 \u2502\\n\u2502 NO_ELEMENTS_IN_CONFIG \u2502  139 \u2502     2 \u2502 2023-03-14 00:42:11 \u2502 Certificate file is not set.                                                                                                                        \u2502 []               \u2502      0 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n## Top 10 queries that are using the most CPU and memory\\n\\n```sql\\nSELECT\\n    type,\\n    event_time,\\n    initial_query_id,\\n    formatReadableSize(memory_usage) AS memory,\\n    `ProfileEvents.Values`[indexOf(`ProfileEvents.Names`, \'UserTimeMicroseconds\')] AS userCPU,\\n    `ProfileEvents.Values`[indexOf(`ProfileEvents.Names`, \'SystemTimeMicroseconds\')] AS systemCPU,\\n    normalizedQueryHash(query) AS normalized_query_hash\\nFROM system.query_log\\nORDER BY memory_usage DESC\\nLIMIT 10\\n```\\n\\n## How much disk space are my projection using\\n\\n```sql\\nSELECT\\n    name,\\n    parent_name,\\n    formatReadableSize(bytes_on_disk) AS bytes,\\n    formatReadableSize(parent_bytes_on_disk) AS parent_bytes,\\n    bytes_on_disk / parent_bytes_on_disk AS ratio\\nFROM system.projection_parts\\n```\\n\\n\\n## Show disk storage, number of parts, number of rows in system.parts and marks across databases\\n\\n```sql\\nSELECT\\n    database,\\n    table,\\n    partition,\\n    count() AS parts,\\n    formatReadableSize(sum(bytes_on_disk)) AS bytes_on_disk,\\n    formatReadableQuantity(sum(rows)) AS rows,\\n    sum(marks) AS marks\\nFROM system.parts\\nWHERE (database != \'system\') AND active\\nGROUP BY\\n    database,\\n    table,\\n    partition\\nORDER BY database ASC\\n```\\n\\n## List details of recently written new parts\\n\\nThe details include when they got created, how large they are, how many rows, and more:\\n\\n```sql\\nSELECT\\n    modification_time,\\n    rows,\\n    formatReadableSize(bytes_on_disk),\\n    *\\nFROM clusterAllReplicas(default, system.parts)\\nWHERE (database = \'default\') AND active AND (level = 0)\\nORDER BY modification_time DESC\\nLIMIT 100\\n```"},{"id":"/calculate-pi-using-sql","metadata":{"permalink":"/docs/knowledgebase/calculate-pi-using-sql","source":"@site/knowledgebase/calculate-pi-using-sql.md","title":"It\'s Pi Day! Let\'s calculate pi using SQL","description":"Happy Pi Day! We thought it would be fun to calculate pi using SQL queries in ClickHouse. Here is what we came up with so far...","date":"2023-03-14T00:00:00.000Z","formattedDate":"March 14, 2023","tags":[],"readingTime":2.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-14T00:00:00.000Z"},"prevItem":{"title":"Useful queries for troubleshooting","permalink":"/docs/knowledgebase/useful-queries-for-troubleshooting"},"nextItem":{"title":"Are Materialized Views inserted synchronously?","permalink":"/docs/knowledgebase/are_materialized_views_inserted_asynchronously"}},"content":"Happy Pi Day! We thought it would be fun to calculate pi using SQL queries in ClickHouse. Here is what we came up with so far...\\n\\n1. This one uses the ClickHouse `numbers_mt` table function to return 1B rows and only takes 40ms to compute the calculation:\\n\\n```sql\\nSELECT 4 * sum(if(number % 2, -1, 1) / ((number * 2) + 1)) AS pi\\nFROM numbers_mt(1000000000.)\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500pi\u2500\u2510\\n\u2502 3.141592652589797 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 0.432 sec. Processed 1.00 billion rows, 8.00 GB (2.32 billion rows/s., 18.53 GB/s.)\\n```\\n\\n2. The following example also processes 1B numbers, just not as quickly:\\n\\n```sql\\nSELECT 3 + (4 * sum(if((number % 2) = 0, if((number % 4) = 0, -1 / ((number * (number + 1)) * (number + 2)), 1 / ((number * (number + 1)) * (number + 2))), 0))) AS pi\\nFROM numbers_mt(2, 10000000000)\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500pi\u2500\u2510\\n\u2502 3.1415926525808087 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 9.825 sec. Processed 10.00 billion rows, 80.00 GB (1.02 billion rows/s., 8.14 GB/s.)\\n```\\n\\n3. This one is obviously our favorite in ClickHouse (and the most accurate!):\\n\\n```sql\\nSELECT pi()\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500pi()\u2500\u2510\\n\u2502 3.141592653589793 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 0.008 sec.\\n```\\n\\n4. Someone knew their trigonometry with this one:\\n\\n```sql\\nSELECT 2 * asin(1) AS pi\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500pi\u2500\u2510\\n\u2502 3.141592653589793 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 0.005 sec.\\n```\\n\\n5. Here is a handy API that lets you specify the number of digits you want:\\n\\n```sql\\nSELECT *\\nFROM url(\'https://api.pi.delivery/v1/pi?start=0&numberOfDigits=100\', \'JSONEachRow\')\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500content\u2500\u2510\\n\u2502 3.1415926535897933e99 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 0.556 sec.\\n```\\n\\n6. This one is clever  - it uses ClickHouse distance functions:\\n\\n```sql\\nWITH random_points AS\\n    (\\n        SELECT (rand64(1) / pow(2, 64), rand64(2) / pow(2, 64)) AS point\\n        FROM numbers(1000000000)\\n    )\\nSELECT (4 * countIf(L2Norm(point) < 1)) / count() AS pi\\nFROM random_points\\n\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500pi\u2500\u2510\\n\u2502 3.141627208 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 4.742 sec. Processed 1.00 billion rows, 8.00 GB (210.88 million rows/s., 1.69 GB/s.)\\n```\\n\\n7. If you\'re a physicist, you will be content with this one:\\n\\n```sql\\nSELECT 22 / 7\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500divide(22, 7)\u2500\u2510\\n\u2502 3.142857142857143 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n8. Another indirect mehthod (this one came from Alexey Milovidov) that is accurate to 7 decimal places - and it\'s quick:\\n\\n```sql\\nWITH\\n    10 AS length,\\n    (number / 1000000000.) * length AS x\\nSELECT pow((2 * length) * avg(exp(-(x * x))), 2) AS pi\\nFROM numbers_mt(1000000000.)\\n\\n\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500pi\u2500\u2510\\n\u2502 3.1415926890388595 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n1 row in set. Elapsed: 1.245 sec. Processed 1.00 billion rows, 8.00 GB (803.25 million rows/s., 6.43 GB/s.)\\n```\\n\\n:::note\\nIf you have any more, we\'d love for you to contribute. Thanks!\\n:::"},{"id":"/are_materialized_views_inserted_asynchronously","metadata":{"permalink":"/docs/knowledgebase/are_materialized_views_inserted_asynchronously","source":"@site/knowledgebase/are_materialized_views_inserted_asynchronously.md","title":"Are Materialized Views inserted synchronously?","description":"Question: When a source table has new rows inserted into it, those new rows are also sent to all of the materialized views of that source table. Are inserts into Materialized Views performed synchronously, meaning that once the insert is acknowledged successfully from the server to the client, it means that all Materialized Views have been fully updated and available for queries?","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":1.115,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"It\'s Pi Day! Let\'s calculate pi using SQL","permalink":"/docs/knowledgebase/calculate-pi-using-sql"},"nextItem":{"title":"Synchronous data reading","permalink":"/docs/knowledgebase/async_vs_optimize_read_in_order"}},"content":"**Question:** When a source table has new rows inserted into it, those new rows are also sent to all of the materialized views of that source table. Are inserts into Materialized Views performed synchronously, meaning that once the insert is acknowledged successfully from the server to the client, it means that all Materialized Views have been fully updated and available for queries?\\n\\n**Answer:**\\n\\n1. When an `INSERT` succeeds, the data is inserted both to the table and all materialized views.\\n2. The insert is not atomic with respect to materialized views. At the moment of time when the `INSERT` is in progress, concurrent clients may see the intermediate state, when the data is inserted to the main table, but not to materialized views, or vice versa.\\n3. If you are using [async inserts](https://clickhouse.com/docs/en/optimize/asynchronous-inserts/), they collect the data and perform a regular insert under the hood, returning the same type of answer to the client as for regular inserts. If the client received success from an async insert with the option `wait_for_async_insert` (as by default), the data is inserted into both the table and all of its materialized views.\\n\\n**Question:** How about chained/cascaded materialized views?\\n\\n**Answer:**\\nThe same rules apply - an `INSERT` with a successful response means that the data was inserted into every materialized view in the chain. The insert is non-atomic."},{"id":"/async_vs_optimize_read_in_order","metadata":{"permalink":"/docs/knowledgebase/async_vs_optimize_read_in_order","source":"@site/knowledgebase/async_vs_optimize_read_in_order.md","title":"Synchronous data reading","description":"The new setting allow_asynchronous_read_from_io_pool_for_merge_tree allows the number of reading threads (streams) to be higher than the number of threads in the rest of the query execution pipeline.","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":3.685,"hasTruncateMarker":false,"authors":[],"frontMatter":{"description":"The new setting allow_asynchronous_read_from_io_pool_for_merge_tree allows the number of reading threads (streams) to be higher than the number of threads in the rest of the query execution pipeline.","date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"Are Materialized Views inserted synchronously?","permalink":"/docs/knowledgebase/are_materialized_views_inserted_asynchronously"},"nextItem":{"title":"How to configure a setting for a user","permalink":"/docs/knowledgebase/configure-a-user-setting"}},"content":"Normally the [max_threads](https://clickhouse.com/docs/en/operations/settings/settings/#settings-max_threads) setting [controls](https://clickhouse.com/company/events/query-performance-introspection) the number of parallel reading threads and parallel query processing threads:\\n\\n![Untitled scene](https://user-images.githubusercontent.com/97666923/212138072-5410b684-d00d-4218-93c5-6f49523928a5.png)\\n\\nThe data is read \'in order\', column after column, from disk.\\n\\n### Asynchronous data reading\\nThe new setting [allow_asynchronous_read_from_io_pool_for_merge_tree](https://github.com/ClickHouse/ClickHouse/pull/43260) allows the number of reading threads (streams) to be higher than the number of threads in the rest of the query execution pipeline to **speed up cold queries on low-CPU ClickHouse Cloud services**, and to **increase performance for I/O bound queries**.\\nWhen the setting is enabled, then the amount of reading threads is controlled by the [max_streams_for_merge_tree_reading](https://github.com/ClickHouse/ClickHouse/pull/43260) setting:\\n\\n![Untitled scene](https://user-images.githubusercontent.com/97666923/212138124-82efba35-7948-4c16-8c44-cba5f0c5c5ae.png)\\n\\nThe data is read asynchronously, in parallel from different columns.\\n\\nNote that there is also the [max_streams_to_max_threads_ratio](https://github.com/ClickHouse/ClickHouse/pull/43260) setting for configuring the ratio between the number of reading threads (streams) and the number of threads in the rest of the query execution pipeline.\\nBut in benchmarks it did [not](https://github.com/ClickHouse/product/issues/637#issuecomment-1302644078) help as [much](https://github.com/ClickHouse/product/issues/637#issuecomment-1347067863) as the `max_streams_for_merge_tree_reading` setting\\n\\n### Benchmarks\\n\\n[Here](https://github.com/ClickHouse/product/issues/637#issuecomment-1347067863) and [here](https://github.com/ClickHouse/product/issues/637#issuecomment-1360369066) are some benchmarks regarding speeding up a cold query on a ClickHouse Cloud service.\\n\\n### What about optimize_read_in_order?\\n\\nWith the [optimize_read_in_order optimization](https://clickhouse.com/docs/en/sql-reference/statements/select/order-by/#optimization-of-data-reading), ClickHouse can [skip](https://clickhouse.com/blog/clickhouse-faster-queries-with-projections-and-primary-indexes) resorting data in memory if the queries sort order reflects the physical order of data on disk, **but that requires reading the data in order (in contrast to asynchronous reading)**:\\n\\n![Untitled scene](https://user-images.githubusercontent.com/97666923/212138180-1a4e29d5-43f1-4bfa-a1d6-df2824417508.png)\\n\\n### optimize_read_in_order has precedence over asynchronous reading\\n\\nWhen ClickHouse sees that `optimize_read_in_order optimization` can be applied, then the `allow_asynchronous_read_from_io_pool_for_merge_tree` setting will be ignored / disabled.\\n\\n### Example demonstrating all of the above\\n\\n- Create and load the [UK Property Price Paid table](https://clickhouse.com/docs/en/getting-started/example-datasets/uk-price-paid)\\n\\n- Check set value of max_threads (by default the amount of CPU cores that ClickHouse sees on the node executing the query\\n```\\nSELECT getSetting(\'max_threads\');\\n\\n\\n\u250c\u2500getSetting(\'max_threads\')\u2500\u2510\\n\u2502                        10 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n- Check query pipeline with default amount of threads for both reading and processing the data\\n```\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid;\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                 \u2502\\n\u2502 ExpressionTransform \xd7 10     \u2502\\n\u2502   (ReadFromMergeTree)        \u2502\\n\u2502   MergeTreeThread \xd7 10 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n- Check query pipeline with 60 async reading threads and default amount of threads for the rest of the query execution pipeline\\n```\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid\\nSETTINGS\\n    allow_asynchronous_read_from_io_pool_for_merge_tree = 1,\\n    max_streams_for_merge_tree_reading = 60;\\n\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                   \u2502\\n\u2502 ExpressionTransform \xd7 10       \u2502\\n\u2502   (ReadFromMergeTree)          \u2502\\n\u2502   Resize 60 \u2192 10               \u2502\\n\u2502     MergeTreeThread \xd7 60 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n- Check query pipeline with 20 threads for both reading and processing the data\\n```\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid\\nSETTINGS\\n    max_threads = 20;\\n\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                 \u2502\\n\u2502 ExpressionTransform \xd7 20     \u2502\\n\u2502   (ReadFromMergeTree)        \u2502\\n\u2502   MergeTreeThread \xd7 20 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n- Check query pipeline with 60 async reading threads and 20 threads for the rest of the query execution pipeline\\n```\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid\\nSETTINGS\\n    max_threads = 20,\\n    allow_asynchronous_read_from_io_pool_for_merge_tree = 1,\\n    max_streams_for_merge_tree_reading = 60;\\n\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                   \u2502\\n\u2502 ExpressionTransform \xd7 20       \u2502\\n\u2502   (ReadFromMergeTree)          \u2502\\n\u2502   Resize 60 \u2192 20               \u2502\\n\u2502     MergeTreeThread \xd7 60 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n- Check query pipeline with 60 async reading threads and 20 threads for the rest of the query execution pipeline\\nwhen `optimize_read_in_order optimization` can be applied\\n```\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid\\nORDER BY postcode1, postcode2\\nSETTINGS\\n    max_threads = 20,\\n    allow_asynchronous_read_from_io_pool_for_merge_tree= 1,\\n    max_streams_for_merge_tree_reading= 60;\\n\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                      \u2502\\n\u2502 ExpressionTransform               \u2502\\n\u2502   (Sorting)                       \u2502\\n\u2502   MergingSortedTransform 20 \u2192 1   \u2502\\n\u2502     (Expression)                  \u2502\\n\u2502     ExpressionTransform \xd7 20      \u2502\\n\u2502       (ReadFromMergeTree)         \u2502\\n\u2502       MergeTreeInOrder \xd7 20 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n\\n-- note that this is equivalent to disabling allow_asynchronous_read_from_io_pool_for_merge_tree\\n\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid\\nORDER BY postcode1, postcode2\\nSETTINGS\\n    max_threads = 20,\\n    allow_asynchronous_read_from_io_pool_for_merge_tree = 0,\\n    max_streams_for_merge_tree_reading = 0;\\n\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                      \u2502\\n\u2502 ExpressionTransform               \u2502\\n\u2502   (Sorting)                       \u2502\\n\u2502   MergingSortedTransform 20 \u2192 1   \u2502\\n\u2502     (Expression)                  \u2502\\n\u2502     ExpressionTransform \xd7 20      \u2502\\n\u2502       (ReadFromMergeTree)         \u2502\\n\u2502       MergeTreeInOrder \xd7 20 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n-- note that you can enforce allow_asynchronous_read_from_io_pool_for_merge_tree by disabling optimize_read_in_order\\n\\nEXPLAIN PIPELINE\\nSELECT *\\nFROM uk_price_paid\\nORDER BY\\n    postcode1 ASC,\\n    postcode2 ASC\\nSETTINGS\\n    max_threads = 20,\\n    allow_asynchronous_read_from_io_pool_for_merge_tree = 1,\\n    max_streams_for_merge_tree_reading = 60,\\n    optimize_read_in_order = 0;\\n\\n\\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 (Expression)                         \u2502\\n\u2502 ExpressionTransform                  \u2502\\n\u2502   (Sorting)                          \u2502\\n\u2502   MergingSortedTransform 20 \u2192 1      \u2502\\n\u2502     MergeSortingTransform \xd7 20       \u2502\\n\u2502       (Expression)                   \u2502\\n\u2502       ExpressionTransform \xd7 20       \u2502\\n\u2502         (ReadFromMergeTree)          \u2502\\n\u2502         Resize 60 \u2192 20               \u2502\\n\u2502           MergeTreeThread \xd7 60 0 \u2192 1 \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n\\n```"},{"id":"/configure-a-user-setting","metadata":{"permalink":"/docs/knowledgebase/configure-a-user-setting","source":"@site/knowledgebase/configure-a-user-setting.md","title":"How to configure a setting for a user","description":"There are several ways to define a setting for a user in ClickHouse, depending on the use case and how long you want the setting to be configured. Let\'s look at a few scenarios...","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":1.015,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"Synchronous data reading","permalink":"/docs/knowledgebase/async_vs_optimize_read_in_order"},"nextItem":{"title":"Execute SYSTEM statements on all nodes in ClickHouse Cloud","permalink":"/docs/knowledgebase/execute-system-queries-in-cloud"}},"content":"There are several ways to define a setting for a user in ClickHouse, depending on the use case and how long you want the setting to be configured. Let\'s look at a few scenarios...\\n\\n## Configure a setting for a single query\\n\\nA `SELECT` query can contain a `SETTINGS` clause where you can define any number of settings. The settings are only applied for that particular query. For example:\\n\\n```sql\\nSELECT *\\nFROM my_table\\nSETTINGS max_threads = 8;\\n```\\n\\nThe maximum number of threads will be 8 for this particular query.\\n\\n## Configure a setting for a session\\n\\nYou can define a setting for the lifetime of a client session using a `SET` clause. This is handy for ad-hoc testing or for when you want a setting to live for the lifetime of a few queries - but not longer.\\n\\n```sql\\nSET max_threads = 8;\\n\\nSELECT *\\nFROM my_table;\\n```\\n\\n## Configure a setting for a particular user\\n\\nUse `ALTER USER` to define a setting just for one user. For example:\\n\\n```sql\\nALTER USER my_user_name SETTINGS max_threads = 8;\\n```\\n\\nYou can verify it worked by logging out of your client, logging back in, then use the `getSetting` function:\\n\\n```sql\\nSELECT getSetting(\'max_threads\');\\n```"},{"id":"/execute-system-queries-in-cloud","metadata":{"permalink":"/docs/knowledgebase/execute-system-queries-in-cloud","source":"@site/knowledgebase/execute-system-queries-in-cloud.md","title":"Execute SYSTEM statements on all nodes in ClickHouse Cloud","description":"In order to execute the same query on all nodes of a ClickHouse Cloud service, we can use clusterAllReplicas.","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"How to configure a setting for a user","permalink":"/docs/knowledgebase/configure-a-user-setting"},"nextItem":{"title":"Filtered aggregates in ClickHouse","permalink":"/docs/knowledgebase/filtered-aggregates"}},"content":"In order to execute the same query on all nodes of a ClickHouse Cloud service, we can use [clusterAllReplicas](https://clickhouse.com/docs/en/sql-reference/table-functions/cluster/).\\n\\nFor example, in order to get entries from a (node-local) system table from all nodes, you can use:\\n```sql\\nSELECT ... FROM clusterAllReplicas(default, system.TABLE) ...;\\n```\\n\\nSimilarly, you can execute the same [SYSTEM statement](https://clickhouse.com/docs/en/sql-reference/statements/system/) on all nodes with a single statement, by using the [ON CLUSTER](https://clickhouse.com/docs/en/sql-reference/distributed-ddl/) clause:\\n```sql\\nSYSTEM ... ON CLUSTER default;\\n```\\n\\nFor example for [dropping the filesystem cache](https://clickhouse.com/docs/en/sql-reference/statements/system/#drop-filesystem-cache) from all nodes, you can use:\\n```sql\\nSYSTEM DROP FILESYSTEM CACHE ON CLUSTER default;\\n```"},{"id":"/filtered-aggregates","metadata":{"permalink":"/docs/knowledgebase/filtered-aggregates","source":"@site/knowledgebase/filtered-aggregates.md","title":"Filtered aggregates in ClickHouse","description":"ClickHouse provides a simple and intuitive way to write filtered aggregates. For example, compare the standard SQL way to write filtered aggregates (which work fine in ClickHouse) with the shorthand syntax using the -If aggregate function combinator, which can be appended to any aggregate function:","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":0.74,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"Execute SYSTEM statements on all nodes in ClickHouse Cloud","permalink":"/docs/knowledgebase/execute-system-queries-in-cloud"},"nextItem":{"title":"Ignoring incorrect settings","permalink":"/docs/knowledgebase/ignoring-incorrect-settings"}},"content":"ClickHouse provides a simple and intuitive way to write _filtered aggregates_. For example, compare the standard SQL way to write filtered aggregates (which work fine in ClickHouse) with the shorthand syntax using the `-If` [aggregate function combinator](https://clickhouse.com/docs/en/sql-reference/aggregate-functions/combinators/), which can be appended to any aggregate function:\\n\\n```sql\\n--standard SQL\\nSELECT\\n   avg(number)\\nFILTER (WHERE number > 50)\\nFROM numbers(100)\\n\\n--ClickHouse using an aggregate combinator\\nSELECT\\n   avgIf(number, number > 50)\\nFROM numbers(100)\\n```\\n\\nSimilarly, there is a `-Distinct` aggregate combinator:\\n\\n```sql\\n--standard SQL\\nSELECT avg(DISTINCT number)\\n\\n--ClickHouse using an aggregate combinator\\nSELECT avgDistinct(number)\\n```\\n\\nWhy are filtered aggregates are important? Because they allow you to implement the **\\"segment comparison\\"** feature in web analytics services.\\nFor example:\\n\\n```sql\\nWITH\\n   Region = \'us\' AS segment1,\\n   Browser = \'Chrome\' AS segment2\\nSELECT\\n   uniqIf(UserID, segment1),\\n   uniqIf(UserID, segment2)\\nWHERE segment1 OR segment2\\n```\\n\\nCheck out the [aggregate function combinator](https://clickhouse.com/docs/en/sql-reference/aggregate-functions/combinators/) page in the docs\\nfor more details."},{"id":"/ignoring-incorrect-settings","metadata":{"permalink":"/docs/knowledgebase/ignoring-incorrect-settings","source":"@site/knowledgebase/ignoring-incorrect-settings.md","title":"Ignoring incorrect settings","description":"When a user-level setting is specified in the wrong place, the server won\'t start and an exception message is sent to the log. However, you can tell ClickHouse to ignore the incorrect setting using the skipcheckforincorrectsettings setting:","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":0.345,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"Filtered aggregates in ClickHouse","permalink":"/docs/knowledgebase/filtered-aggregates"},"nextItem":{"title":"Capturing server logs of queries at the client","permalink":"/docs/knowledgebase/send_logs_level"}},"content":"When a user-level setting is specified in the wrong place, the server won\'t start and an exception message is sent to the log. However, you can tell ClickHouse to ignore the incorrect setting using the `skip_check_for_incorrect_settings` setting:\\n\\nAdd the following to `config.xml`:\\n\\n```xml\\n<skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings>\\n```\\n\\n:::note\\nUser-level settings should be specified in `users.xml` inside a `<profile>` section for the specific user profile, (or in `<default>` for default settings.\\n:::"},{"id":"/send_logs_level","metadata":{"permalink":"/docs/knowledgebase/send_logs_level","source":"@site/knowledgebase/send_logs_level.md","title":"Capturing server logs of queries at the client","description":"A client can view the server logs - even at a different level than what the server log level is configured to - by setting the sendlogslevel client setting.","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[],"readingTime":0.9,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2023-03-01T00:00:00.000Z"},"prevItem":{"title":"Ignoring incorrect settings","permalink":"/docs/knowledgebase/ignoring-incorrect-settings"},"nextItem":{"title":"Billing","permalink":"/docs/knowledgebase/en/faq/billing"}},"content":"A client can view the server logs - even at a different level than what the server log level is configured to - by setting the `send_logs_level` client setting.\\n\\nFor example, suppose the client runs:\\n\\n```sql\\nSET send_logs_level = \'trace\';\\n```\\n\\nThe client will receive trace logs even if the server has log level set to info.\\n\\nOne useful scenario is to use `send_logs_level` to monitor the insertion of rows into a `Distributed` table:\\n- Enable logs in `clickhouse-client` using `SET send_logs_level = \'trace\';`\\n- Run your `INSERT` query\\n- Inserts into a distributed table are asynchronous by default. The data is written into a local buffer on disk, then sent to remote servers in background.\\n- Logs will be sent from all nodes participating in the query processing (distributed tracing)\\n\\nTo check the status of distributed inserts, check the [`system.distribution_queue` table](https://clickhouse.com/docs/en/operations/system-tables/distribution_queue/). This table contains information about local files that are in the queue to be sent to the shards. These local files contain new parts that are created by inserting new data into the `Distributed` table in asynchronous mode."},{"id":"/en/faq/billing","metadata":{"permalink":"/docs/knowledgebase/en/faq/billing","source":"@site/knowledgebase/billing.md","title":"Billing","description":"","date":"2022-12-06T00:00:00.000Z","formattedDate":"December 6, 2022","tags":[],"readingTime":0.03,"hasTruncateMarker":false,"authors":[],"frontMatter":{"sidebar_position":1,"slug":"/en/faq/billing","title":"Billing","date":"2022-12-06T00:00:00.000Z"},"prevItem":{"title":"Capturing server logs of queries at the client","permalink":"/docs/knowledgebase/send_logs_level"},"nextItem":{"title":"Marketplace","permalink":"/docs/knowledgebase/en/faq/marketplace"}},"content":"import Content from \'@site/docs/en/cloud/manage/billing.md\';\\n\\n<Content />"},{"id":"/en/faq/marketplace","metadata":{"permalink":"/docs/knowledgebase/en/faq/marketplace","source":"@site/knowledgebase/marketplace.md","title":"Marketplace","description":"","date":"2022-12-06T00:00:00.000Z","formattedDate":"December 6, 2022","tags":[],"readingTime":0.03,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/en/faq/marketplace","title":"Marketplace","date":"2022-12-06T00:00:00.000Z"},"prevItem":{"title":"Billing","permalink":"/docs/knowledgebase/en/faq/billing"},"nextItem":{"title":"Troubleshooting","permalink":"/docs/knowledgebase/en/faq/troubleshooting"}},"content":"import Content from \'@site/docs/en/cloud/marketplace.md\';\\n\\n<Content />"},{"id":"/en/faq/troubleshooting","metadata":{"permalink":"/docs/knowledgebase/en/faq/troubleshooting","source":"@site/knowledgebase/troubleshooting.md","title":"Troubleshooting","description":"ClickHouse Cloud Troubleshooting","date":"2022-12-06T00:00:00.000Z","formattedDate":"December 6, 2022","tags":[],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"sidebar_position":1,"slug":"/en/faq/troubleshooting","title":"Troubleshooting","date":"2022-12-06T00:00:00.000Z"},"prevItem":{"title":"Marketplace","permalink":"/docs/knowledgebase/en/faq/marketplace"},"nextItem":{"title":"How to check grants and standard permissions for default user in ClickHouse Cloud","permalink":"/docs/knowledgebase/user-permissions-on-cloud"}},"content":"import SelfManagedTroubleshooting from \'@site/docs/en/operations/_troubleshooting.md\';\\n\\n## ClickHouse Cloud Troubleshooting\\n\\n### Unable to access a ClickHouse Cloud service\\n\\nIf you are seeing an error message like one of these, your IP Access List may be denying access:\\n\\n```response\\ncurl: (35) error:02FFF036:system library:func(4095):Connection reset by peer\\n```\\nor\\n```response\\ncurl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to HOSTNAME.clickhouse.cloud:8443\\n```\\nor\\n```response\\nCode: 210. DB::NetException: SSL connection unexpectedly closed (e46453teek.us-east-2.aws.clickhouse-staging.com:9440). (NETWORK_ERROR)\\n```\\n\\nCheck the [IP Access List](https://clickhouse.com/docs/en/cloud/security/ip-access-list.md), if you are attempting to connect from outside the allowed list then your connection will fail.\\n\\n## Self-managed ClickHouse Troubleshooting\\n\\n<SelfManagedTroubleshooting />"},{"id":"/user-permissions-on-cloud","metadata":{"permalink":"/docs/knowledgebase/user-permissions-on-cloud","source":"@site/knowledgebase/user-permissions-on-cloud.md","title":"How to check grants and standard permissions for default user in ClickHouse Cloud","description":"Improving Map performance","date":"2022-12-06T00:00:00.000Z","formattedDate":"December 6, 2022","tags":[],"readingTime":0.125,"hasTruncateMarker":false,"authors":[],"frontMatter":{"sidebar_position":1,"description":"Improving Map performance","date":"2022-12-06T00:00:00.000Z"},"prevItem":{"title":"Troubleshooting","permalink":"/docs/knowledgebase/en/faq/troubleshooting"},"nextItem":{"title":"Improving Map performance","permalink":"/docs/knowledgebase/improve-map-performance"}},"content":"You can check the current privileges and permission (RBAC) for default user and other users\' max privileges in [this part of the data plane code](https://github.com/ClickHouse/data-plane-application/blob/main/clickhouse-operator/util/clickhouseclient/database_user_supervisor.go#L54)"},{"id":"/improve-map-performance","metadata":{"permalink":"/docs/knowledgebase/improve-map-performance","source":"@site/knowledgebase/improve-map-performance.md","title":"Improving Map performance","description":"Map lookups such as `a[\'key\']\' works with linear complexity (mentioned [here](https://clickhouse.com/docs/en/sql-reference/data-types/map)) and can be inefficient.","date":"2022-10-30T00:00:00.000Z","formattedDate":"October 30, 2022","tags":[],"readingTime":2.65,"hasTruncateMarker":false,"authors":[],"frontMatter":{"sidebar_position":1,"description":"Map lookups such as `a[\'key\']\' works with linear complexity (mentioned [here](https://clickhouse.com/docs/en/sql-reference/data-types/map)) and can be inefficient.","date":"2022-10-30T00:00:00.000Z"},"prevItem":{"title":"How to check grants and standard permissions for default user in ClickHouse Cloud","permalink":"/docs/knowledgebase/user-permissions-on-cloud"},"nextItem":{"title":"Is it possible to delete old records from a ClickHouse table?","permalink":"/docs/knowledgebase/delete-old-data"}},"content":"**Problem**\\n\\nMap lookup such as `a[\'key\']` works with linear complexity (mentioned [here](https://clickhouse.com/docs/en/sql-reference/data-types/map)) and can be inefficient. This is because selecting a value with a specific key from a table would require iterating through all keys (~M) across all rows (N) in the Map column, resulting in ~MxN lookups.\\n\\nA lookup using Map can be 10x slower than a String column. The experiment below also shows ~10x slowdown for cold query, and difference in multiple magnitudes of data processed (7.21 MB vs 5.65 GB).\\n\\n```sql\\n-- create table with SpanNAme as String and ResourceAttributes as Map\\nDROP TABLE IF EXISTS tbl;\\nCREATE TABLE tbl (\\n    `Timestamp` DateTime64(9) CODEC (Delta(8), ZSTD(1)),\\n    `TraceId` String CODEC (ZSTD(1)),\\n    `ServiceName` LowCardinality(String) CODEC (ZSTD(1)),\\n    `Duration` UInt8 CODEC (ZSTD(1)), -- Int64\\n    `SpanName` LowCardinality(String) CODEC (ZSTD(1)),\\n    `ResourceAttributes` Map(LowCardinality(String), String) CODEC (ZSTD(1))\\n)\\nENGINE = MergeTree\\nPARTITION BY toDate(Timestamp)\\nORDER BY (ServiceName, SpanName, toUnixTimestamp(Timestamp), TraceId);\\n\\n-- create UDF to generate random Map data for ResourceAttributes\\nDROP FUNCTION IF EXISTS genmap;\\nCREATE FUNCTION genmap AS (n) -> arrayMap (x-> (x::String, (x*rand32())::String), range(1, n));\\n\\n-- check that genmap is working as intended\\nSELECT genmap(10)::Map(String, String);\\n\\n-- insert 1M rows\\nINSERT INTO tbl\\nSELECT\\n    now() - randUniform(1, 1000000.) as Timestamp,\\n    randomPrintableASCII(2) as TraceId,\\n    randomPrintableASCII(2) as ServiceName,\\n    rand32() as Duration,\\n    randomPrintableASCII(2) as SpanName,\\n    genmap(rand64()%500)::Map(String, String) as ResourceAttributes\\nFROM numbers(1_000_000);\\n\\n-- querying for SpanName is faster\\n-- [cold] 0 rows in set. Elapsed: 0.642 sec. Processed 1.00 million rows, 7.21 MB (1.56 million rows/s., 11.22 MB/s.)\\n-- [warm] 0 rows in set. Elapsed: 0.164 sec. Processed 1.00 million rows, 7.21 MB (6.10 million rows/s., 43.99 MB/s.)\\nSELECT\\n    COUNT(*),\\n    avg(Duration/1E6) as average,\\n    quantile(0.95)(Duration/1E6) as p95,\\n    quantile(0.99)(Duration/1E6) as p99,\\n    SpanName\\nFROM tbl\\nGROUP BY SpanName ORDER BY 1 DESC LIMIT 50 FORMAT Null;\\n\\n-- query for ResourceAttributes is slower\\n-- [cold] 0 rows in set. Elapsed: 6.432 sec. Processed 1.00 million rows, 5.65 GB (155.46 thousand rows/s., 879.07 MB/s.)\\n-- [warm] 0 rows in set. Elapsed: 5.935 sec. Processed 1.00 million rows, 5.65 GB (168.50 thousand rows/s., 952.81 MB/s.)\\nSELECT\\n    COUNT(*),\\n    avg(Duration/1E6) as average,\\n    quantile(0.95)(Duration/1E6) as p95,\\n    quantile(0.99)(Duration/1E6) as p99,\\n    ResourceAttributes[\'1\'] as hostname\\nFROM tbl\\nGROUP BY hostname ORDER BY 1 DESC LIMIT 50 FORMAT Null;\\n```\\n\\n**Solution**\\nTo improve the query, we can add another column with the value defaulting to a particular key in the Map column, and then materializing it to populate value for existing rows. This way, we extract and store the necessary value at insertion time, thereby speeding up the lookup at query time.\\n\\n```sql\\n-- solution is to add a column with value defaulting to a particular key in Map\\nALTER TABLE tbl ADD COLUMN hostname LowCardinality(String) DEFAULT ResourceAttributes[\'1\'];\\nALTER TABLE tbl MATERIALIZE COLUMN hostname;\\n\\n-- query for hostname (new column) is now faster\\n-- [cold] 0 rows in set. Elapsed: 2.215 sec. Processed 1.00 million rows, 21.67 MB (451.52 thousand rows/s., 9.78 MB/s.)\\n-- [warm] 0 rows in set. Elapsed: 0.541 sec. Processed 1.00 million rows, 21.67 MB (1.85 million rows/s., 40.04 MB/s.)\\nSELECT\\n    COUNT(*),\\n    avg(Duration/1E6) as average,\\n    quantile(0.95)(Duration/1E6) as p95,\\n    quantile(0.99)(Duration/1E6) as p99,\\n    hostname\\nFROM tbl\\nGROUP BY hostname ORDER BY 1 DESC LIMIT 50 FORMAT Null;\\n\\n-- drop cache to run query cold\\nSYSTEM DROP FILESYSTEM CACHE;\\n```"},{"id":"/delete-old-data","metadata":{"permalink":"/docs/knowledgebase/delete-old-data","source":"@site/knowledgebase/delete-old-data.md","title":"Is it possible to delete old records from a ClickHouse table?","description":"The short answer is \u201cyes\u201d. ClickHouse has multiple mechanisms that allow freeing up disk space by removing old data. Each mechanism is aimed for different scenarios.","date":"2022-10-19T00:00:00.000Z","formattedDate":"October 19, 2022","tags":[],"readingTime":1.845,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Is it possible to delete old records from a ClickHouse table?","description":"The short answer is \u201cyes\u201d. ClickHouse has multiple mechanisms that allow freeing up disk space by removing old data. Each mechanism is aimed for different scenarios.","date":"2022-10-19T00:00:00.000Z"},"prevItem":{"title":"Improving Map performance","permalink":"/docs/knowledgebase/improve-map-performance"},"nextItem":{"title":"Does ClickHouse support multi-region replication?","permalink":"/docs/knowledgebase/en/faq/operations/multi-region-replication"}},"content":"The short answer is \u201cyes\u201d. ClickHouse has multiple mechanisms that allow freeing up disk space by removing old data. Each mechanism is aimed for different scenarios.\\n\\n## TTL {#ttl}\\n\\nClickHouse allows to automatically drop values when some condition happens. This condition is configured as an expression based on any columns, usually just static offset for any timestamp column.\\n\\nThe key advantage of this approach is that it does not need any external system to trigger, once TTL is configured, data removal happens automatically in background.\\n\\n:::note\\nTTL can also be used to move data not only to [/dev/null](https://en.wikipedia.org/wiki/Null_device), but also between different storage systems, like from SSD to HDD.\\n:::\\n\\nMore details on [configuring TTL](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-ttl).\\n\\n## DELETE FROM\\n[DELETE FROM](https://clickhouse.com/docs/en/sql-reference/statements/delete) allows standard DELETE queries to be run in ClickHouse. The rows targeted in the filter clause are marked as deleted, and removed from future result sets.  Cleanup of the rows happens asynchronously.\\n\\n:::note\\nDELETE FROM is an experimental feature and must be enabled with:\\n```\\nSET allow_experimental_lightweight_delete = true;\\n```\\n:::\\n\\n## ALTER DELETE {#alter-delete}\\n\\nALTER DELETE removes rows using asynchronous batch operations. Unlike DELETE FROM, queries run after the ALTER DELETE and before the batch operations complete will include the rows targeted for deletion.  For more details see the [ALTER DELETE](https://clickhouse.com/docs/en/sql-reference/statements/alter/delete) docs.\\n\\n`ALTER DELETE` can be issued to flexibly remove old data. If you need to do it regularly, the main downside will be the need to have an external system to submit the query. There are also some performance considerations since mutations rewrite complete parts even there is only a single row to be deleted.\\n\\nThis is the most common approach to make your system based on ClickHouse [GDPR](https://gdpr-info.eu)-compliant.\\n\\nMore details on [mutations](https://clickhouse.com/docs/en/sql-reference/statements/alter/#alter-mutations).\\n\\n## DROP PARTITION {#drop-partition}\\n\\n`ALTER TABLE ... DROP PARTITION` provides a cost-efficient way to drop a whole partition. It\u2019s not that flexible and needs proper partitioning scheme configured on table creation, but still covers most common cases. Like mutations need to be executed from an external system for regular use.\\n\\nMore details on [manipulating partitions](https://clickhouse.com/docs/en/sql-reference/statements/alter/partition/#alter_drop-partition).\\n\\n## TRUNCATE {#truncate}\\n\\nIt\u2019s rather radical to drop all data from a table, but in some cases it might be exactly what you need.\\n\\nMore details on [table truncation](https://clickhouse.com/docs/en/sql-reference/statements/truncate)."},{"id":"/en/faq/operations/multi-region-replication","metadata":{"permalink":"/docs/knowledgebase/en/faq/operations/multi-region-replication","source":"@site/knowledgebase/multi-region-replication.md","title":"Does ClickHouse support multi-region replication?","description":"The short answer is yes. However, we recommend keeping latency between all regions/datacenters in two-digit range, otherwise write performance will suffer as it goes through distributed consensus protocol.","date":"2022-04-22T00:00:00.000Z","formattedDate":"April 22, 2022","tags":[],"readingTime":0.365,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/en/faq/operations/multi-region-replication","title":"Does ClickHouse support multi-region replication?","toc_hidden":true,"toc_priority":30,"description":"The short answer is yes. However, we recommend keeping latency between all regions/datacenters in two-digit range, otherwise write performance will suffer as it goes through distributed consensus protocol.","date":"2022-04-22T00:00:00.000Z"},"prevItem":{"title":"Is it possible to delete old records from a ClickHouse table?","permalink":"/docs/knowledgebase/delete-old-data"},"nextItem":{"title":"What if I have a problem with encodings when using Oracle via ODBC?","permalink":"/docs/knowledgebase/oracle-odbc"}},"content":"The short answer is \\"yes\\". However, we recommend keeping latency between all regions/datacenters in two-digit range, otherwise write performance will suffer as it goes through distributed consensus protocol. For example, replication between US coasts will likely work fine, but between the US and Europe won\'t.\\n\\nConfiguration-wise there\'s no difference compared to single-region replication, simply use hosts that are located in different locations for replicas.\\n\\nFor more information, see [full article on data replication](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication)."},{"id":"/oracle-odbc","metadata":{"permalink":"/docs/knowledgebase/oracle-odbc","source":"@site/knowledgebase/oracle-odbc.md","title":"What if I have a problem with encodings when using Oracle via ODBC?","description":"If you use Oracle as a source of ClickHouse external dictionaries via Oracle ODBC driver, you need to set the correct value for the `NLS_LANG` environment variable in `/etc/default/clickhouse`.","date":"2022-04-22T00:00:00.000Z","formattedDate":"April 22, 2022","tags":[],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What if I have a problem with encodings when using Oracle via ODBC?","description":"If you use Oracle as a source of ClickHouse external dictionaries via Oracle ODBC driver, you need to set the correct value for the `NLS_LANG` environment variable in `/etc/default/clickhouse`.","date":"2022-04-22T00:00:00.000Z"},"prevItem":{"title":"Does ClickHouse support multi-region replication?","permalink":"/docs/knowledgebase/en/faq/operations/multi-region-replication"},"nextItem":{"title":"What is a columnar database?","permalink":"/docs/knowledgebase/columnar-database"}},"content":"If you use Oracle as a source of ClickHouse external dictionaries via Oracle ODBC driver, you need to set the correct value for the `NLS_LANG` environment variable in `/etc/default/clickhouse`. For more information, see the [Oracle NLS_LANG FAQ](https://www.oracle.com/technetwork/products/globalization/nls-lang-099431.html).\\n\\n**Example**\\n\\n``` sql\\nNLS_LANG=RUSSIAN_RUSSIA.UTF8\\n```"},{"id":"/columnar-database","metadata":{"permalink":"/docs/knowledgebase/columnar-database","source":"@site/knowledgebase/columnar-database.md","title":"What is a columnar database?","description":"A columnar database stores the data of each column independently. This allows reading data from disk only for those columns that are used in any given query.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":1.015,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What is a columnar database?","description":"A columnar database stores the data of each column independently. This allows reading data from disk only for those columns that are used in any given query.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"What if I have a problem with encodings when using Oracle via ODBC?","permalink":"/docs/knowledgebase/oracle-odbc"},"nextItem":{"title":"What does \u201cClickHouse\u201d mean?","permalink":"/docs/knowledgebase/dbms-naming"}},"content":"A columnar database stores the data of each column independently. This allows reading data from disk only for those columns that are used in any given query. The cost is that operations that affect whole rows become proportionally more expensive. The synonym for a columnar database is a column-oriented database management system. ClickHouse is a typical example of such a system.\\n\\nKey columnar database advantages are:\\n\\n-   Queries that use only a few columns out of many.\\n-   Aggregating queries against large volumes of data.\\n-   Column-wise data compression.\\n\\nHere is the illustration of the difference between traditional row-oriented systems and columnar databases when building reports:\\n\\n**Traditional row-oriented**\\n![Traditional row-oriented](@site/docs/en/images/row-oriented.gif#)\\n\\n**Columnar**\\n![Columnar](@site/docs/en/images/column-oriented.gif#)\\n\\nA columnar database is the preferred choice for analytical applications because it allows having many columns in a table just in case, but to not pay the cost for unused columns on read query execution time (a traditional OLTP database reads all of the data during queries as the data is stored in rows and not columns). Column-oriented databases are designed for big data processing and data warehousing, they often natively scale using distributed clusters of low-cost hardware to increase throughput. ClickHouse does it with combination of [distributed](https://clickhouse.com/docs/en/engines/table-engines/special/distributed) and [replicated](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication) tables."},{"id":"/dbms-naming","metadata":{"permalink":"/docs/knowledgebase/dbms-naming","source":"@site/knowledgebase/dbms-naming.md","title":"What does \u201cClickHouse\u201d mean?","description":"It\u2019s a combination of **Click**stream and Data ware**House**. It comes from the original use case at Yandex.Metrica, where ClickHouse was supposed to keep records of all clicks by people from all over the Internet, and it still does the job.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":0.82,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What does \u201cClickHouse\u201d mean?","description":"It\u2019s a combination of **Click**stream and Data ware**House**. It comes from the original use case at Yandex.Metrica, where ClickHouse was supposed to keep records of all clicks by people from all over the Internet, and it still does the job.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"What is a columnar database?","permalink":"/docs/knowledgebase/columnar-database"},"nextItem":{"title":"How do I contribute code to ClickHouse?","permalink":"/docs/knowledgebase/how-do-i-contribute-code-to-clickhouse"}},"content":"It\u2019s a combination of \u201c**Click**stream\u201d and \u201cData ware**House**\u201d. It comes from the original use case at Yandex.Metrica, where ClickHouse was supposed to keep records of all clicks by people from all over the Internet, and it still does the job. You can read more about this use case on [ClickHouse history](https://clickhouse.com/docs/en/about-us/history) page.\\n\\nThis two-part meaning has two consequences:\\n\\n-   The only correct way to write Click**H**ouse is with capital H.\\n-   If you need to abbreviate it, use **CH**. For some historical reasons, abbreviating as CK is also popular in China, mostly because one of the first talks about ClickHouse in Chinese used this form.\\n\\n:::info\\nMany years after ClickHouse got its name, this approach of combining two words that are meaningful on their own has been highlighted as the best way to name a database in a [research by Andy Pavlo](https://www.cs.cmu.edu/~pavlo/blog/2020/03/on-naming-a-database-management-system.html), an Associate Professor of Databases at Carnegie Mellon University. ClickHouse shared his \u201cbest database name of all time\u201d award with Postgres.\\n:::"},{"id":"/how-do-i-contribute-code-to-clickhouse","metadata":{"permalink":"/docs/knowledgebase/how-do-i-contribute-code-to-clickhouse","source":"@site/knowledgebase/how-do-i-contribute-code-to-clickhouse.md","title":"How do I contribute code to ClickHouse?","description":"ClickHouse is an open-source project developed on GitHub. As customary, contribution instructions are published in CONTRIBUTING file in the root of the source code repository.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":0.36,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How do I contribute code to ClickHouse?","description":"ClickHouse is an open-source project developed on GitHub. As customary, contribution instructions are published in CONTRIBUTING file in the root of the source code repository.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"What does \u201cClickHouse\u201d mean?","permalink":"/docs/knowledgebase/dbms-naming"},"nextItem":{"title":"Can I use ClickHouse as a key-value storage?","permalink":"/docs/knowledgebase/key-value"}},"content":"ClickHouse is an open-source project [developed on GitHub](https://github.com/ClickHouse/ClickHouse).\\n\\nAs customary, contribution instructions are published in [CONTRIBUTING](https://github.com/ClickHouse/ClickHouse/blob/master/CONTRIBUTING) file in the root of the source code repository.\\n\\nIf you want to suggest a substantial change to ClickHouse, consider [opening a GitHub issue](https://github.com/ClickHouse/ClickHouse/issues/new/choose) explaining what you want to do, to discuss it with maintainers and community first. [Examples of such RFC issues](https://github.com/ClickHouse/ClickHouse/issues?q=is%3Aissue+is%3Aopen+rfc).\\n\\nIf your contributions are security related, please check out [our security policy](https://github.com/ClickHouse/ClickHouse/security/policy/) too."},{"id":"/key-value","metadata":{"permalink":"/docs/knowledgebase/key-value","source":"@site/knowledgebase/key-value.md","title":"Can I use ClickHouse as a key-value storage?","description":"The short answer is **\u201cno\u201d**. The key-value workload is among top positions in the list of cases when **NOT** to use ClickHouse.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":1.695,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Can I use ClickHouse as a key-value storage?","description":"The short answer is **\u201cno\u201d**. The key-value workload is among top positions in the list of cases when **NOT** to use ClickHouse.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"How do I contribute code to ClickHouse?","permalink":"/docs/knowledgebase/how-do-i-contribute-code-to-clickhouse"},"nextItem":{"title":"Why not use something like MapReduce?","permalink":"/docs/knowledgebase/mapreduce"}},"content":"The short answer is **\u201cno\u201d**. The key-value workload is among top positions in the list of cases when <span class=\\"text-danger\\">**NOT**</span> to use ClickHouse. It\u2019s an [OLAP](https://clickhouse.com/docs/en/faq/general/olap) system after all, while there are many excellent key-value storage systems out there.\\n\\nHowever, there might be situations where it still makes sense to use ClickHouse for key-value-like queries. Usually, it\u2019s some low-budget products where the main workload is analytical in nature and fits ClickHouse well, but there\u2019s also some secondary process that needs a key-value pattern with not so high request throughput and without strict latency requirements. If you had an unlimited budget, you would have installed a secondary key-value database for this secondary workload, but in reality, there\u2019s an additional cost of maintaining one more storage system (monitoring, backups, etc.) which might be desirable to avoid.\\n\\nIf you decide to go against recommendations and run some key-value-like queries against ClickHouse, here are some tips:\\n\\n-   The key reason why point queries are expensive in ClickHouse is its sparse primary index of main [MergeTree table engine family](https://clickhouse.com/docs/en//engines/table-engines/mergetree-family/mergetree). This index can\u2019t point to each specific row of data, instead, it points to each N-th and the system has to scan from the neighboring N-th row to the desired one, reading excessive data along the way. In a key-value scenario, it might be useful to reduce the value of N with the `index_granularity` setting.\\n-   ClickHouse keeps each column in a separate set of files, so to assemble one complete row it needs to go through each of those files. Their count increases linearly with the number of columns, so in the key-value scenario, it might be worth avoiding using many columns and put all your payload in a single `String` column encoded in some serialization format like JSON, Protobuf, or whatever makes sense.\\n-   There\u2019s an alternative approach that uses [Join](https://clickhouse.com/docs/en/engines/table-engines/special/join) table engine instead of normal `MergeTree` tables and [joinGet](https://clickhouse.com/docs/en/sql-reference/functions/other-functions/#joinget) function to retrieve the data. It can provide better query performance but might have some usability and reliability issues. Here\u2019s an [usage example](https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/00800_versatile_storage_join.sql#L49-L51)."},{"id":"/mapreduce","metadata":{"permalink":"/docs/knowledgebase/mapreduce","source":"@site/knowledgebase/mapreduce.md","title":"Why not use something like MapReduce?","description":"We can refer to systems like MapReduce as distributed computing systems in which the reduce operation is based on distributed sorting. The most common open-source solution in this class is Apache Hadoop.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":1.225,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Why not use something like MapReduce?","description":"We can refer to systems like MapReduce as distributed computing systems in which the reduce operation is based on distributed sorting. The most common open-source solution in this class is Apache Hadoop.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"Can I use ClickHouse as a key-value storage?","permalink":"/docs/knowledgebase/key-value"},"nextItem":{"title":"How many maximum databases or tables is recommended in a ClickHouse cluster?","permalink":"/docs/knowledgebase/maximum_number_of_tables_and_databases"}},"content":"We can refer to systems like MapReduce as distributed computing systems in which the reduce operation is based on distributed sorting. The most common open-source solution in this class is [Apache Hadoop](http://hadoop.apache.org).\\n\\nThese systems aren\u2019t appropriate for online queries due to their high latency. In other words, they can\u2019t be used as the back-end for a web interface. These types of systems aren\u2019t useful for real-time data updates. Distributed sorting isn\u2019t the best way to perform reduce operations if the result of the operation and all the intermediate results (if there are any) are located in the RAM of a single server, which is usually the case for online queries. In such a case, a hash table is an optimal way to perform reduce operations. A common approach to optimizing map-reduce tasks is pre-aggregation (partial reduce) using a hash table in RAM. The user performs this optimization manually. Distributed sorting is one of the main causes of reduced performance when running simple map-reduce tasks.\\n\\nMost MapReduce implementations allow you to execute arbitrary code on a cluster. But a declarative query language is better suited to OLAP to run experiments quickly. For example, Hadoop has Hive and Pig. Also consider Cloudera Impala or Shark (outdated) for Spark, as well as Spark SQL, Presto, and Apache Drill. Performance when running such tasks is highly sub-optimal compared to specialized systems, but relatively high latency makes it unrealistic to use these systems as the backend for a web interface."},{"id":"/maximum_number_of_tables_and_databases","metadata":{"permalink":"/docs/knowledgebase/maximum_number_of_tables_and_databases","source":"@site/knowledgebase/maximum_number_of_tables_and_databases.md","title":"How many maximum databases or tables is recommended in a ClickHouse cluster?","description":"We recommend no more than 10,000 tables and databases (combined total).","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":0.055,"hasTruncateMarker":false,"authors":[],"frontMatter":{"date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"Why not use something like MapReduce?","permalink":"/docs/knowledgebase/mapreduce"},"nextItem":{"title":"What does \u201c\u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d mean?","permalink":"/docs/knowledgebase/en/faq/general/ne-tormozit"}},"content":"We recommend no more than 10,000 tables and databases (combined total)."},{"id":"/en/faq/general/ne-tormozit","metadata":{"permalink":"/docs/knowledgebase/en/faq/general/ne-tormozit","source":"@site/knowledgebase/ne-tormozit.md","title":"What does \u201c\u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d mean?","description":"This question usually arises when people see official ClickHouse t-shirts. They have large words **\u201cClickHouse \u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d** on the front.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":1.675,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/en/faq/general/ne-tormozit","title":"What does \u201c\u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d mean?","toc_hidden":true,"toc_priority":11,"description":"This question usually arises when people see official ClickHouse t-shirts. They have large words **\u201cClickHouse \u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d** on the front.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"How many maximum databases or tables is recommended in a ClickHouse cluster?","permalink":"/docs/knowledgebase/maximum_number_of_tables_and_databases"},"nextItem":{"title":"What is OLAP?","permalink":"/docs/knowledgebase/olap"}},"content":"This question usually arises when people see official ClickHouse t-shirts. They have large words **\u201cClickHouse \u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d** on the front.\\n\\nBefore ClickHouse became open-source, it has been developed as an in-house storage system by the largest Russian IT company, [Yandex](https://yandex.com/company/). That\u2019s why it initially got its slogan in Russian, which is \u201c\u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d (pronounced as \u201cne tormozit\u201d). After the open-source release we first produced some of those t-shirts for events in Russia and it was a no-brainer to use the slogan as-is.\\n\\nOne of the following batches of those t-shirts was supposed to be given away on events outside of Russia and we tried to make the English version of the slogan. Unfortunately, the Russian language is kind of elegant in terms of expressing stuff and there was a restriction of limited space on a t-shirt, so we failed to come up with good enough translation (most options appeared to be either long or inaccurate) and decided to keep the slogan in Russian even on t-shirts produced for international events. It appeared to be a great decision because people all over the world get positively surprised and curious when they see it.\\n\\nSo, what does it mean? Here are some ways to translate *\u201c\u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d*:\\n\\n-   If you translate it literally, it\u2019d be something like *\u201cClickHouse does not press the brake pedal\u201d*.\\n-   If you\u2019d want to express it as close to how it sounds to a Russian person with IT background, it\u2019d be something like *\u201cIf your larger system lags, it\u2019s not because it uses ClickHouse\u201d*.\\n-   Shorter, but not so precise versions could be *\u201cClickHouse is not slow\u201d*, *\u201cClickHouse does not lag\u201d* or just *\u201cClickHouse is fast\u201d*.\\n\\nIf you haven\u2019t seen one of those t-shirts in person, you can check them out online in many ClickHouse-related videos. For example, this one:\\n\\n<iframe src=\\"//www.youtube.com/embed/bSyQahMVZ7w\\" frameborder=\\"0\\" allowfullscreen ></iframe>\\n\\nP.S. These t-shirts are not for sale, they are given away for free on most [ClickHouse Meetups](https://www.meetup.com/pro/clickhouse/), usually for best questions or other forms of active participation."},{"id":"/olap","metadata":{"permalink":"/docs/knowledgebase/olap","source":"@site/knowledgebase/olap.md","title":"What is OLAP?","description":"OLAP stands for Online Analytical Processing. It is a broad term that can be looked at from two perspectives: technical and business.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":2.64,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What is OLAP?","description":"OLAP stands for Online Analytical Processing. It is a broad term that can be looked at from two perspectives: technical and business.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"What does \u201c\u043d\u0435 \u0442\u043e\u0440\u043c\u043e\u0437\u0438\u0442\u201d mean?","permalink":"/docs/knowledgebase/en/faq/general/ne-tormozit"},"nextItem":{"title":"Which ClickHouse version to use in production?","permalink":"/docs/knowledgebase/production"}},"content":"[OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) stands for Online Analytical Processing. It is a broad term that can be looked at from two perspectives: technical and business. But at the very high level, you can just read these words backward:\\n\\nProcessing\\n:   Some source data is processed\u2026\\n\\nAnalytical\\n:   \u2026to produce some analytical reports and insights\u2026\\n\\nOnline\\n:   \u2026in real-time.\\n\\n## OLAP from the Business Perspective {#olap-from-the-business-perspective}\\n\\nIn recent years, business people started to realize the value of data. Companies who make their decisions blindly, more often than not fail to keep up with the competition. The data-driven approach of successful companies forces them to collect all data that might be remotely useful for making business decisions and need mechanisms to timely analyze them. Here\u2019s where OLAP database management systems (DBMS) come in.\\n\\nIn a business sense, OLAP allows companies to continuously plan, analyze, and report operational activities, thus maximizing efficiency, reducing expenses, and ultimately conquering the market share. It could be done either in an in-house system or outsourced to SaaS providers like web/mobile analytics services, CRM services, etc. OLAP is the technology behind many BI applications (Business Intelligence).\\n\\nClickHouse is an OLAP database management system that is pretty often used as a backend for those SaaS solutions for analyzing domain-specific data. However, some businesses are still reluctant to share their data with third-party providers and an in-house data warehouse scenario is also viable.\\n\\n## OLAP from the Technical Perspective {#olap-from-the-technical-perspective}\\n\\nAll database management systems could be classified into two groups: OLAP (Online **Analytical** Processing) and OLTP (Online **Transactional** Processing). Former focuses on building reports, each based on large volumes of historical data, but doing it not so frequently. While the latter usually handle a continuous stream of transactions, constantly modifying the current state of data.\\n\\nIn practice OLAP and OLTP are not categories, it\u2019s more like a spectrum. Most real systems usually focus on one of them but provide some solutions or workarounds if the opposite kind of workload is also desired. This situation often forces businesses to operate multiple storage systems integrated, which might be not so big deal but having more systems make it more expensive to maintain. So the trend of recent years is HTAP (**Hybrid Transactional/Analytical Processing**) when both kinds of the workload are handled equally well by a single database management system.\\n\\nEven if a DBMS started as a pure OLAP or pure OLTP, they are forced to move towards that HTAP direction to keep up with their competition. And ClickHouse is no exception, initially, it has been designed as [fast-as-possible OLAP system](https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast) and it still does not have full-fledged transaction support, but some features like consistent read/writes and mutations for updating/deleting data had to be added.\\n\\nThe fundamental trade-off between OLAP and OLTP systems remains:\\n\\n-   To build analytical reports efficiently it\u2019s crucial to be able to read columns separately, thus most OLAP databases are [columnar](https://clickhouse.com/docs/en/faq/general/columnar-database),\\n-   While storing columns separately increases costs of operations on rows, like append or in-place modification, proportionally to the number of columns (which can be huge if the systems try to collect all details of an event just in case). Thus, most OLTP systems store data arranged by rows."},{"id":"/production","metadata":{"permalink":"/docs/knowledgebase/production","source":"@site/knowledgebase/production.md","title":"Which ClickHouse version to use in production?","description":"First of all, let\u2019s discuss why people ask this question in the first place. There are two key reasons...","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":5.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Which ClickHouse version to use in production?","description":"First of all, let\u2019s discuss why people ask this question in the first place. There are two key reasons...","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"What is OLAP?","permalink":"/docs/knowledgebase/olap"},"nextItem":{"title":"Can I use ClickHouse as a time-series database?","permalink":"/docs/knowledgebase/time-series"}},"content":"First of all, let\u2019s discuss why people ask this question in the first place. There are two key reasons:\\n\\n1.  ClickHouse is developed with pretty high velocity, and usually there are 10+ stable releases per year. That makes a wide range of releases to choose from, which is not so trivial of a choice.\\n2.  Some users want to avoid spending time figuring out which version works best for their use case and just follow someone else\u2019s advice.\\n\\nThe second reason is more fundamental, so we\u2019ll start with that one and then get back to navigating through various ClickHouse releases.\\n\\n## Which ClickHouse Version Do You Recommend? {#which-clickhouse-version-do-you-recommend}\\n\\nIt\u2019s tempting to hire consultants or trust some known experts to get rid of responsibility for your production environment. You install some specific ClickHouse version that someone else recommended; if there\u2019s some issue with it - it\u2019s not your fault, it\u2019s someone else\u2019s. This line of reasoning is a big trap. No external person knows better than you what\u2019s going on in your company\u2019s production environment.\\n\\nSo how do you properly choose which ClickHouse version to upgrade to? Or how do you choose your first ClickHouse version? First of all, you need to invest in setting up a **realistic pre-production environment**. In an ideal world, it could be a completely identical shadow copy, but that\u2019s usually expensive.\\n\\nHere are some key points to get reasonable fidelity in a pre-production environment with not-so-high costs:\\n\\n-   Pre-production environment needs to run an as close of a set of queries as you intend to run in production:\\n    -   Don\u2019t make it read-only with some frozen data.\\n    -   Don\u2019t make it write-only with just copying data without building some typical reports.\\n    -   Don\u2019t wipe it clean instead of applying schema migrations.\\n-   Use a sample of real production data and queries. Try to choose a sample that\u2019s still representative and makes `SELECT` queries return reasonable results. Use obfuscation if your data is sensitive and internal policies do not allow it to leave the production environment.\\n-   Make sure that pre-production is covered by your monitoring and alerting software the same way as your production environment does.\\n-   If your production spans across multiple datacenters or regions, make your pre-production do the same.\\n-   If your production uses complex features like replication, distributed tables and cascading materialized views, make sure they are configured similarly in pre-production.\\n-   There\u2019s a trade-off on using the roughly same number of servers or VMs in pre-production as in production but of smaller size, or much less of them but of the same size. The first option might catch extra network-related issues, while the latter is easier to manage.\\n\\nThe second area to invest in is **automated testing infrastructure**. Don\u2019t assume that if some kind of query has executed successfully once, it\u2019ll continue to do so forever. It\u2019s OK to have some unit tests where ClickHouse is mocked, but make sure your product has a reasonable set of automated tests that are run against real ClickHouse and check that all important use cases are still working as expected.\\n\\nAn extra step forward could be contributing those automated tests to [ClickHouse\u2019s open-source test infrastructure](https://github.com/ClickHouse/ClickHouse/tree/master/tests) that are continuously used in its day-to-day development. It definitely will take some additional time and effort to learn [how to run it](@site/docs/en/development/tests.md) and then how to adapt your tests to this framework, but it\u2019ll pay off by ensuring that ClickHouse releases are already tested against them when they are announced stable, instead of repeatedly losing time on reporting the issue after the fact and then waiting for a bugfix to be implemented, backported and released. Some companies even have such test contributions to infrastructure by its use as an internal policy, (called [Beyonce\u2019s Rule](https://www.oreilly.com/library/view/software-engineering-at/9781492082781/ch01.html#policies_that_scale_well) at Google).\\n\\nWhen you have your pre-production environment and testing infrastructure in place, choosing the best version is straightforward:\\n\\n1.  Routinely run your automated tests against new ClickHouse releases. You can do it even for ClickHouse releases that are marked as `testing`, but going forward to the next steps with them is not recommended.\\n2.  Deploy the ClickHouse release that passed the tests to pre-production and check that all processes are running as expected.\\n3.  Report any issues you discovered to [ClickHouse GitHub Issues](https://github.com/ClickHouse/ClickHouse/issues).\\n4.  If there were no major issues, it should be safe to start deploying ClickHouse release to your production environment. Investing in gradual release automation that implements an approach similar to [canary releases](https://martinfowler.com/bliki/CanaryRelease.html) or [green-blue deployments](https://martinfowler.com/bliki/BlueGreenDeployment.html) might further reduce the risk of issues in production.\\n\\nAs you might have noticed, there\u2019s nothing specific to ClickHouse in the approach described above - people do that for any piece of infrastructure they rely on if they take their production environment seriously.\\n\\n## How to Choose Between ClickHouse Releases? {#how-to-choose-between-clickhouse-releases}\\n\\nIf you look into the contents of the ClickHouse package repository, you\u2019ll see two kinds of packages:\\n\\n1.  `stable`\\n2.  `lts` (long-term support)\\n\\nHere is some guidance on how to choose between them:\\n\\n-   `stable` is the kind of package we recommend by default. They are released roughly monthly (and thus provide new features with reasonable delay) and three latest stable releases are supported in terms of diagnostics and backporting of bugfixes.\\n-   `lts` are released twice a year and are supported for a year after their initial release. You might prefer them over `stable` in the following cases:\\n    -   Your company has some internal policies that do not allow for frequent upgrades or using non-LTS software.\\n    -   You are using ClickHouse in some secondary products that either do not require any complex ClickHouse features or do not have enough resources to keep it updated.\\n\\nMany teams who initially think that `lts` is the way to go often switch to `stable` anyway because of some recent feature that\u2019s important for their product.\\n\\n:::warning\\nOne more thing to keep in mind when upgrading ClickHouse: we\u2019re always keeping an eye on compatibility across releases, but sometimes it\u2019s not reasonable to keep and some minor details might change. So make sure you check the [changelog](@site/docs/en/whats-new/changelog/index.md) before upgrading to see if there are any notes about backward-incompatible changes.\\n:::"},{"id":"/time-series","metadata":{"permalink":"/docs/knowledgebase/time-series","source":"@site/knowledgebase/time-series.md","title":"Can I use ClickHouse as a time-series database?","description":"ClickHouse is a generic data storage solution for OLAP workloads, while there are many specialized time-series database management systems.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":1.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Can I use ClickHouse as a time-series database?","description":"ClickHouse is a generic data storage solution for OLAP workloads, while there are many specialized time-series database management systems.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"Which ClickHouse version to use in production?","permalink":"/docs/knowledgebase/production"},"nextItem":{"title":"Who is using ClickHouse?","permalink":"/docs/knowledgebase/who-is-using-clickhouse"}},"content":"_Note: Please see the blog [Working with Time series data in ClickHouse](https://clickhouse.com/blog/working-with-time-series-data-and-functions-ClickHouse) for additional examples of using ClickHouse for time series analysis._\\n\\nClickHouse is a generic data storage solution for [OLAP](https://clickhouse.com/docs/en/faq/general/olap) workloads, while there are many specialized time-series database management systems. Nevertheless, ClickHouse\u2019s [focus on query execution speed](https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast) allows it to outperform specialized systems in many cases. There are many independent benchmarks on this topic out there, so we\u2019re not going to conduct one here. Instead, let\u2019s focus on ClickHouse features that are important to use if that\u2019s your use case.\\n\\nFirst of all, there are **[specialized codecs](https://clickhouse.com/docs/en/sql-reference/statements/create/table#specialized-codecs)** which make typical time-series. Either common algorithms like `DoubleDelta` and `Gorilla` or specific to ClickHouse like `T64`.\\n\\nSecond, time-series queries often hit only recent data, like one day or one week old. It makes sense to use servers that have both fast nVME/SSD drives and high-capacity HDD drives. ClickHouse [TTL](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/##table_engine-mergetree-multiple-volumes) feature allows to configure keeping fresh hot data on fast drives and gradually move it to slower drives as it ages. Rollup or removal of even older data is also possible if your requirements demand it.\\n\\nEven though it\u2019s against ClickHouse philosophy of storing and processing raw data, you can use [materialized views](https://clickhouse.com/docs/en/sql-reference/statements/create/view) to fit into even tighter latency or costs requirements.\\n\\n## Related Content\\n\\n- Blog: [Working with time series data in ClickHouse](https://clickhouse.com/blog/working-with-time-series-data-and-functions-ClickHouse)"},{"id":"/who-is-using-clickhouse","metadata":{"permalink":"/docs/knowledgebase/who-is-using-clickhouse","source":"@site/knowledgebase/who-is-using-clickhouse.md","title":"Who is using ClickHouse?","description":"Being an open-source product makes this question not so straightforward to answer. You do not have to tell anyone if you want to start using ClickHouse, you just go grab source code or pre-compiled packages.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":1.96,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Who is using ClickHouse?","description":"Being an open-source product makes this question not so straightforward to answer. You do not have to tell anyone if you want to start using ClickHouse, you just go grab source code or pre-compiled packages.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"Can I use ClickHouse as a time-series database?","permalink":"/docs/knowledgebase/time-series"},"nextItem":{"title":"Why is ClickHouse so fast?","permalink":"/docs/knowledgebase/why-clickhouse-is-so-fast"}},"content":"Being an open-source product makes this question not so straightforward to answer. You do not have to tell anyone if you want to start using ClickHouse, you just go grab source code or pre-compiled packages. There\u2019s no contract to sign and the [Apache 2.0 license](https://github.com/ClickHouse/ClickHouse/blob/master/LICENSE) allows for unconstrained software distribution.\\n\\nAlso, the technology stack is often in a grey zone of what\u2019s covered by an NDA. Some companies consider technologies they use as a competitive advantage even if they are open-source and do not allow employees to share any details publicly. Some see some PR risks and allow employees to share implementation details only with their PR department approval.\\n\\nSo how to tell who is using ClickHouse?\\n\\nOne way is to **ask around**. If it\u2019s not in writing, people are much more willing to share what technologies are used in their companies, what the use cases are, what kind of hardware is used, data volumes, etc. We\u2019re talking with users regularly on [ClickHouse Meetups](https://www.youtube.com/channel/UChtmrD-dsdpspr42P_PyRAw/playlists) all over the world and have heard stories about 1000+ companies that use ClickHouse. Unfortunately, that\u2019s not reproducible and we try to treat such stories as if they were told under NDA to avoid any potential troubles. But you can come to any of our future meetups and talk with other users on your own. There are multiple ways how meetups are announced, for example, you can subscribe to [our Twitter](http://twitter.com/ClickHouseDB/).\\n\\nThe second way is to look for companies **publicly saying** that they use ClickHouse. It\u2019s more substantial because there\u2019s usually some hard evidence like a blog post, talk video recording, slide deck, etc. We collect the collection of links to such evidence on our **[Adopters](https://clickhouse.com/docs/en/about-us/adopters)** page. Feel free to contribute the story of your employer or just some links you\u2019ve stumbled upon (but try not to violate your NDA in the process).\\n\\nYou can find names of very large companies in the adopters list, like Bloomberg, Cisco, China Telecom, Tencent, or Uber, but with the first approach, we found that there are many more. For example, if you take [the list of largest IT companies by Forbes (2020)](https://www.forbes.com/sites/hanktucker/2020/05/13/worlds-largest-technology-companies-2020-apple-stays-on-top-zoom-and-uber-debut/) over half of them are using ClickHouse in some way. Also, it would be unfair not to mention [Yandex](https://clickhouse.com/docs/en/about-us/history), the company which initially open-sourced ClickHouse in 2016 and happens to be one of the largest IT companies in Europe."},{"id":"/why-clickhouse-is-so-fast","metadata":{"permalink":"/docs/knowledgebase/why-clickhouse-is-so-fast","source":"@site/knowledgebase/why-clickhouse-is-so-fast.md","title":"Why is ClickHouse so fast?","description":"It was designed to be fast. Query execution performance has always been a top priority during the development process, but other important characteristics like user-friendliness, scalability, and security were also considered so ClickHouse could become a real production system.","date":"2021-09-01T00:00:00.000Z","formattedDate":"September 1, 2021","tags":[],"readingTime":3.88,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Why is ClickHouse so fast?","description":"It was designed to be fast. Query execution performance has always been a top priority during the development process, but other important characteristics like user-friendliness, scalability, and security were also considered so ClickHouse could become a real production system.","date":"2021-09-01T00:00:00.000Z"},"prevItem":{"title":"Who is using ClickHouse?","permalink":"/docs/knowledgebase/who-is-using-clickhouse"}},"content":"It was designed to be fast. Query execution performance has always been a top priority during the development process, but other important characteristics like user-friendliness, scalability, and security were also considered so ClickHouse could become a real production system.\\n\\n### \\"Building for Fast\\", Alexey Milovidov (CTO, ClickHouse)\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/CAS2otEoerM\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n[\\"Building for Fast\\"](https://www.youtube.com/watch?v=CAS2otEoerM) talk from ClickHouse Meetup Amsterdam, June 2022.\\n\\n[\\"Secrets of ClickHouse Performance Optimizations\\"](https://www.youtube.com/watch?v=ZOZQCQEtrz8) talk from Big Data Technology Conference, December 2019, offers a more technical take on the same topic.\\n\\n## What Makes ClickHouse so Fast?\\n\\n### Architecture choices\\n\\nClickHouse was initially built as a prototype to do just a single task well: to filter and aggregate data as fast as possible. That\u2019s what needs to be done to build a typical analytical report, and that\u2019s what a typical [GROUP BY](https://clickhouse.com/docs/en/sql-reference/statements/select/group-by) query does. The ClickHouse team has made several high-level decisions that, when combined, made achieving this task possible:\\n\\n**Column-oriented storage:**   Source data often contain hundreds or even thousands of columns, while a report can use just a few of them. The system needs to avoid reading unnecessary columns to avoid expensive disk read operations.\\n\\n**Indexes:**  Memory resident ClickHouse data structures allow the reading of only the necessary columns, and only the necessary row ranges of those columns.\\n\\n**Data compression:**   Storing different values of the same column together often leads to better compression ratios (compared to row-oriented systems) because in real data a column often has the same, or not so many different, values for neighboring rows. In addition to general-purpose compression, ClickHouse supports [specialized codecs](https://clickhouse.com/docs/en/sql-reference/statements/create/table/#specialized-codecs) that can make data even more compact.\\n\\n**Vectorized query execution:**  ClickHouse not only stores data in columns but also processes data in columns. This leads to better CPU cache utilization and allows for [SIMD](https://en.wikipedia.org/wiki/SIMD) CPU instructions usage.\\n\\n**Scalability:**   ClickHouse can leverage all available CPU cores and disks to execute even a single query. Not only on a single server but all CPU cores and disks of a cluster as well.\\n\\n### Attention to Low-Level Details\\n\\nBut many other database management systems use similar techniques. What really makes ClickHouse stand out is **attention to low-level details**. Most programming languages provide implementations for most common algorithms and data structures, but they tend to be too generic to be effective. Every task can be considered as a landscape with various characteristics, instead of just throwing in random implementation. For example, if you need a hash table, here are some key questions to consider:\\n\\n-   Which hash function to choose?\\n-   Collision resolution algorithm: [open addressing](https://en.wikipedia.org/wiki/Open_addressing) vs [chaining](https://en.wikipedia.org/wiki/Hash_table#Separate_chaining)?\\n-   Memory layout: one array for keys and values or separate arrays? Will it store small or large values?\\n-   Fill factor: when and how to resize? How to move values around on resize?\\n-   Will values be removed and which algorithm will work better if they will?\\n-   Will we need fast probing with bitmaps, inline placement of string keys, support for non-movable values, prefetch, and batching?\\n\\nHash table is a key data structure for `GROUP BY` implementation and ClickHouse automatically chooses one of [30+ variations](https://github.com/ClickHouse/ClickHouse/blob/master/src/Interpreters/Aggregator.h) for each specific query.\\n\\nThe same goes for algorithms, for example, in sorting you might consider:\\n\\n-   What will be sorted: an array of numbers, tuples, strings, or structures?\\n-   Is all data available completely in RAM?\\n-   Do we need a stable sort?\\n-   Do we need a full sort? Maybe partial sort or n-th element will suffice?\\n-   How to implement comparisons?\\n-   Are we sorting data that has already been partially sorted?\\n\\nAlgorithms that they rely on characteristics of data they are working with can often do better than their generic counterparts. If it is not really known in advance, the system can try various implementations and choose the one that works best in runtime. For example, see an [article on how LZ4 decompression is implemented in ClickHouse](https://habr.com/en/company/yandex/blog/457612/).\\n\\nLast but not least, the ClickHouse team always monitors the Internet on people claiming that they came up with the best implementation, algorithm, or data structure to do something and tries it out. Those claims mostly appear to be false, but from time to time you\u2019ll indeed find a gem.\\n\\n:::info Tips for building your own high-performance software\\n-   Keep in mind low-level details when designing your system.\\n-   Design based on hardware capabilities.\\n-   Choose data structures and abstractions based on the needs of the task.\\n-   Provide specializations for special cases.\\n-   Try new, \\"best\\" algorithms, that you read about yesterday.\\n-   Choose an algorithm in runtime based on statistics.\\n-   Benchmark on real datasets.\\n-   Test for performance regressions in CI.\\n-   Measure and observe everything."}]}')}}]);