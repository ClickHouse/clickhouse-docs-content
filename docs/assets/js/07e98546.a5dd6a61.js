"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[13064],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>k});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,k=u["".concat(s,".").concat(m)]||u[m]||c[m]||o;return n?a.createElement(k,i(i({ref:t},d),{},{components:n})):a.createElement(k,i({ref:t},d))}));function k(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},45928:(e,t,n)=>{n.d(t,{ZP:()=>l});var a=n(87462),r=(n(67294),n(3905));const o={toc:[]},i="wrapper";function l(e){let{components:t,...n}=e;return(0,r.kt)(i,(0,a.Z)({},o,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This page is not applicable to ",(0,r.kt)("a",{parentName:"p",href:"https://clickhouse.com/cloud"},"ClickHouse Cloud"),". The procedure documented here is automated in ClickHouse Cloud services.")))}l.isMDXComponent=!0},88624:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var a=n(87462),r=(n(67294),n(3905)),o=n(45928);const i={slug:"/en/guides/sre/keeper/clickhouse-keeper",sidebar_label:"Configuring ClickHouse Keeper",sidebar_position:10,keywords:["Keeper","ZooKeeper","clickhouse-keeper","replication"],description:"ClickHouse Keeper, or clickhouse-keeper, replaces ZooKeeper and provides replication and coordination."},l="ClickHouse Keeper (clickhouse-keeper)",s={unversionedId:"en/guides/sre/keeper/index",id:"en/guides/sre/keeper/index",title:"ClickHouse Keeper (clickhouse-keeper)",description:"ClickHouse Keeper, or clickhouse-keeper, replaces ZooKeeper and provides replication and coordination.",source:"@site/docs/en/guides/sre/keeper/index.md",sourceDirName:"en/guides/sre/keeper",slug:"/en/guides/sre/keeper/clickhouse-keeper",permalink:"/docs/en/guides/sre/keeper/clickhouse-keeper",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/guides/sre/keeper/index.md",tags:[],version:"current",sidebarPosition:10,frontMatter:{slug:"/en/guides/sre/keeper/clickhouse-keeper",sidebar_label:"Configuring ClickHouse Keeper",sidebar_position:10,keywords:["Keeper","ZooKeeper","clickhouse-keeper","replication"],description:"ClickHouse Keeper, or clickhouse-keeper, replaces ZooKeeper and provides replication and coordination."},sidebar:"docs",previous:{title:"External Disks for Storing Data",permalink:"/docs/en/operations/storing-data"},next:{title:"Configuring SSL-TLS",permalink:"/docs/en/guides/sre/configuring-ssl"}},p={},d=[{value:"Implementation details",id:"implementation-details",level:3},{value:"Configuration",id:"configuration",level:3},{value:"How to run",id:"how-to-run",level:3},{value:"Four Letter Word Commands",id:"four-letter-word-commands",level:3},{value:"Migration from ZooKeeper",id:"migration-from-zookeeper",level:3},{value:"Recovering after losing quorum",id:"recovering-after-losing-quorum",level:3},{value:"ClickHouse Keeper User Guide",id:"clickhouse-keeper-user-guide",level:2},{value:"1. Configure Nodes with Keeper settings",id:"1-configure-nodes-with-keeper-settings",level:3},{value:"2.  Configure a cluster in ClickHouse",id:"2--configure-a-cluster-in-clickhouse",level:3},{value:"3. Create and test distributed table",id:"3-create-and-test-distributed-table",level:3},{value:"Summary",id:"summary",level:3},{value:"Configuring ClickHouse Keeper with unique paths",id:"configuring-clickhouse-keeper-with-unique-paths",level:2},{value:"Description",id:"description",level:3},{value:"Example Environment",id:"example-environment",level:3},{value:"Procedures to set up tables to use {uuid}",id:"procedures-to-set-up-tables-to-use-uuid",level:3},{value:"Testing",id:"testing",level:3},{value:"Alternatives",id:"alternatives",level:3},{value:"Troubleshooting",id:"troubleshooting",level:3}],u={toc:d},c="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(c,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"clickhouse-keeper-clickhouse-keeper"},"ClickHouse Keeper (clickhouse-keeper)"),(0,r.kt)(o.ZP,{mdxType:"SelfManaged"}),(0,r.kt)("p",null,"ClickHouse Keeper provides the coordination system for data ",(0,r.kt)("a",{parentName:"p",href:"/docs/en/engines/table-engines/mergetree-family/replication"},"replication")," and ",(0,r.kt)("a",{parentName:"p",href:"/docs/en/sql-reference/distributed-ddl"},"distributed DDL")," queries execution. ClickHouse Keeper is compatible with ZooKeeper."),(0,r.kt)("h3",{id:"implementation-details"},"Implementation details"),(0,r.kt)("p",null,"ZooKeeper is one of the first well-known open-source coordination systems. It's implemented in Java, and has quite a simple and powerful data model. ZooKeeper's coordination algorithm, ZooKeeper Atomic Broadcast (ZAB), doesn't provide linearizability guarantees for reads, because each ZooKeeper node serves reads locally. Unlike ZooKeeper ClickHouse Keeper is written in C++ and uses the ",(0,r.kt)("a",{parentName:"p",href:"https://raft.github.io/"},"RAFT algorithm")," ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/eBay/NuRaft"},"implementation"),". This algorithm allows linearizability for reads and writes, and has several open-source implementations in different languages."),(0,r.kt)("p",null,"By default, ClickHouse Keeper provides the same guarantees as ZooKeeper (linearizable writes, non-linearizable reads). It has a compatible client-server protocol, so any standard ZooKeeper client can be used to interact with ClickHouse Keeper. Snapshots and logs have an incompatible format with ZooKeeper, but the ",(0,r.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-converter")," tool enables the conversion of ZooKeeper data to ClickHouse Keeper snapshots. The interserver protocol in ClickHouse Keeper is also incompatible with ZooKeeper so a mixed ZooKeeper / ClickHouse Keeper cluster is impossible."),(0,r.kt)("p",null,"ClickHouse Keeper supports Access Control Lists (ACLs) the same way as ",(0,r.kt)("a",{parentName:"p",href:"https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_ZooKeeperAccessControl"},"ZooKeeper")," does. ClickHouse Keeper supports the same set of permissions and has the identical built-in schemes: ",(0,r.kt)("inlineCode",{parentName:"p"},"world"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"auth")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"digest"),". The digest authentication scheme uses the pair ",(0,r.kt)("inlineCode",{parentName:"p"},"username:password"),", the password is encoded in Base64."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"External integrations are not supported.")),(0,r.kt)("h3",{id:"configuration"},"Configuration"),(0,r.kt)("p",null,"ClickHouse Keeper can be used as a standalone replacement for ZooKeeper or as an internal part of the ClickHouse server. In both cases the configuration is almost the same ",(0,r.kt)("inlineCode",{parentName:"p"},".xml")," file. The main ClickHouse Keeper configuration tag is ",(0,r.kt)("inlineCode",{parentName:"p"},"<keeper_server>"),". Keeper configuration has the following parameters:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"tcp_port")," \u2014 Port for a client to connect (default for ZooKeeper is ",(0,r.kt)("inlineCode",{parentName:"li"},"2181"),")."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"tcp_port_secure")," \u2014 Secure port for an SSL connection between client and keeper-server."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"server_id")," \u2014 Unique server id, each participant of the ClickHouse Keeper cluster must have a unique number (1, 2, 3, and so on)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"log_storage_path")," \u2014 Path to coordination logs, just like ZooKeeper it is best to store logs on non-busy nodes."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"snapshot_storage_path")," \u2014 Path to coordination snapshots.")),(0,r.kt)("p",null,"Other common parameters are inherited from the ClickHouse server config (",(0,r.kt)("inlineCode",{parentName:"p"},"listen_host"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"logger"),", and so on)."),(0,r.kt)("p",null,"Internal coordination settings are located in the ",(0,r.kt)("inlineCode",{parentName:"p"},"<keeper_server>.<coordination_settings>")," section:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"operation_timeout_ms")," \u2014 Timeout for a single client operation (ms) (default: 10000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"min_session_timeout_ms")," \u2014 Min timeout for client session (ms) (default: 10000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"session_timeout_ms")," \u2014 Max timeout for client session (ms) (default: 100000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dead_session_check_period_ms")," \u2014 How often ClickHouse Keeper checks for dead sessions and removes them (ms) (default: 500)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"heart_beat_interval_ms")," \u2014 How often a ClickHouse Keeper leader will send heartbeats to followers (ms) (default: 500)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"election_timeout_lower_bound_ms")," \u2014 If the follower does not receive a heartbeat from the leader in this interval, then it can initiate leader election (default: 1000). Must be less than or equal to ",(0,r.kt)("inlineCode",{parentName:"li"},"election_timeout_upper_bound_ms"),". Ideally they shouldn't be equal."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"election_timeout_upper_bound_ms")," \u2014 If the follower does not receive a heartbeat from the leader in this interval, then it must initiate leader election (default: 2000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"rotate_log_storage_interval")," \u2014 How many log records to store in a single file (default: 100000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"reserved_log_items")," \u2014 How many coordination log records to store before compaction (default: 100000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"snapshot_distance")," \u2014 How often ClickHouse Keeper will create new snapshots (in the number of records in logs) (default: 100000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"snapshots_to_keep")," \u2014 How many snapshots to keep (default: 3)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"stale_log_gap")," \u2014 Threshold when leader considers follower as stale and sends the snapshot to it instead of logs (default: 10000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"fresh_log_gap")," \u2014 When node became fresh (default: 200)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"max_requests_batch_size")," - Max size of batch in requests count before it will be sent to RAFT (default: 100)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"force_sync")," \u2014 Call ",(0,r.kt)("inlineCode",{parentName:"li"},"fsync")," on each write to coordination log (default: true)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"quorum_reads")," \u2014 Execute read requests as writes through whole RAFT consensus with similar speed (default: false)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"raft_logs_level")," \u2014 Text logging level about coordination (trace, debug, and so on) (default: system default)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"auto_forwarding")," \u2014 Allow to forward write requests from followers to the leader (default: true)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"shutdown_timeout")," \u2014 Wait to finish internal connections and shutdown (ms) (default: 5000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"startup_timeout")," \u2014 If the server doesn't connect to other quorum participants in the specified timeout it will terminate (ms) (default: 30000)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"four_letter_word_white_list")," \u2014 White list of 4lw commands (default: ",(0,r.kt)("inlineCode",{parentName:"li"},"conf,cons,crst,envi,ruok,srst,srvr,stat,wchs,dirs,mntr,isro,rcvr,apiv,csnp,lgif,rqld"),").")),(0,r.kt)("p",null,"Quorum configuration is located in the ",(0,r.kt)("inlineCode",{parentName:"p"},"<keeper_server>.<raft_configuration>")," section and contain servers description."),(0,r.kt)("p",null,"The only parameter for the whole quorum is ",(0,r.kt)("inlineCode",{parentName:"p"},"secure"),", which enables encrypted connection for communication between quorum participants. The parameter can be set ",(0,r.kt)("inlineCode",{parentName:"p"},"true")," if SSL connection is required for internal communication between nodes, or left unspecified otherwise."),(0,r.kt)("p",null,"The main parameters for each ",(0,r.kt)("inlineCode",{parentName:"p"},"<server>")," are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"id")," \u2014 Server identifier in a quorum."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hostname")," \u2014 Hostname where this server is placed."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"port")," \u2014 Port where this server listens for connections.")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"In the case of a change in the topology of your ClickHouse Keeper cluster (e.g., replacing a server), please make sure to keep the mapping of ",(0,r.kt)("inlineCode",{parentName:"p"},"server_id")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"hostname")," consistent and avoid shuffling or reusing an existing ",(0,r.kt)("inlineCode",{parentName:"p"},"server_id")," for different servers (e.g., it can happen if your rely on automation scripts to deploy ClickHouse Keeper)")),(0,r.kt)("p",null,"Examples of configuration for quorum with three nodes can be found in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ClickHouse/ClickHouse/tree/master/tests/integration"},"integration tests")," with ",(0,r.kt)("inlineCode",{parentName:"p"},"test_keeper_")," prefix. Example configuration for server #1:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<keeper_server>\n    <tcp_port>2181</tcp_port>\n    <server_id>1</server_id>\n    <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n    <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n    <coordination_settings>\n        <operation_timeout_ms>10000</operation_timeout_ms>\n        <session_timeout_ms>30000</session_timeout_ms>\n        <raft_logs_level>trace</raft_logs_level>\n    </coordination_settings>\n\n    <raft_configuration>\n        <server>\n            <id>1</id>\n            <hostname>zoo1</hostname>\n            <port>9234</port>\n        </server>\n        <server>\n            <id>2</id>\n            <hostname>zoo2</hostname>\n            <port>9234</port>\n        </server>\n        <server>\n            <id>3</id>\n            <hostname>zoo3</hostname>\n            <port>9234</port>\n        </server>\n    </raft_configuration>\n</keeper_server>\n")),(0,r.kt)("h3",{id:"how-to-run"},"How to run"),(0,r.kt)("p",null,"ClickHouse Keeper is bundled into the ClickHouse server package, just add configuration of ",(0,r.kt)("inlineCode",{parentName:"p"},"<keeper_server>")," and start ClickHouse server as always. If you want to run standalone ClickHouse Keeper you can start it in a similar way with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"clickhouse-keeper --config /etc/your_path_to_config/config.xml\n")),(0,r.kt)("p",null,"If you don't have the symlink (",(0,r.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper"),") you can create it or specify ",(0,r.kt)("inlineCode",{parentName:"p"},"keeper")," as an argument to ",(0,r.kt)("inlineCode",{parentName:"p"},"clickhouse"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"clickhouse keeper --config /etc/your_path_to_config/config.xml\n")),(0,r.kt)("h3",{id:"four-letter-word-commands"},"Four Letter Word Commands"),(0,r.kt)("p",null,"ClickHouse Keeper also provides 4lw commands which are almost the same with Zookeeper. Each command is composed of four letters such as ",(0,r.kt)("inlineCode",{parentName:"p"},"mntr"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"stat")," etc. There are some more interesting commands: ",(0,r.kt)("inlineCode",{parentName:"p"},"stat")," gives some general information about the server and connected clients, while ",(0,r.kt)("inlineCode",{parentName:"p"},"srvr")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"cons")," give extended details on server and connections respectively."),(0,r.kt)("p",null,"The 4lw commands has a white list configuration ",(0,r.kt)("inlineCode",{parentName:"p"},"four_letter_word_white_list")," which has default value ",(0,r.kt)("inlineCode",{parentName:"p"},"conf,cons,crst,envi,ruok,srst,srvr,stat,wchs,dirs,mntr,isro,rcvr,apiv,csnp,lgif,rqld"),"."),(0,r.kt)("p",null,"You can issue the commands to ClickHouse Keeper via telnet or nc, at the client port."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"echo mntr | nc localhost 9181\n")),(0,r.kt)("p",null,"Bellow is the detailed 4lw commands:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ruok"),": Tests if server is running in a non-error state. The server will respond with ",(0,r.kt)("inlineCode",{parentName:"li"},"imok")," if it is running. Otherwise it will not respond at all. A response of ",(0,r.kt)("inlineCode",{parentName:"li"},"imok"),' does not necessarily indicate that the server has joined the quorum, just that the server process is active and bound to the specified client port. Use "stat" for details on state wrt quorum and client connection information.')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"imok\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"mntr"),": Outputs a list of variables that could be used for monitoring the health of the cluster.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"zk_version      v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7\nzk_avg_latency  0\nzk_max_latency  0\nzk_min_latency  0\nzk_packets_received     68\nzk_packets_sent 68\nzk_num_alive_connections        1\nzk_outstanding_requests 0\nzk_server_state leader\nzk_znode_count  4\nzk_watch_count  1\nzk_ephemerals_count     0\nzk_approximate_data_size        723\nzk_open_file_descriptor_count   310\nzk_max_file_descriptor_count    10240\nzk_followers    0\nzk_synced_followers     0\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"srvr"),": Lists full details for the server.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"ClickHouse Keeper version: v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7\nLatency min/avg/max: 0/0/0\nReceived: 2\nSent : 2\nConnections: 1\nOutstanding: 0\nZxid: 34\nMode: leader\nNode count: 4\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"stat"),": Lists brief details for the server and connected clients.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"ClickHouse Keeper version: v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7\nClients:\n 192.168.1.1:52852(recved=0,sent=0)\n 192.168.1.1:52042(recved=24,sent=48)\nLatency min/avg/max: 0/0/0\nReceived: 4\nSent : 4\nConnections: 1\nOutstanding: 0\nZxid: 36\nMode: leader\nNode count: 4\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"srst"),": Reset server statistics. The command will affect the result of ",(0,r.kt)("inlineCode",{parentName:"li"},"srvr"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"mntr")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"stat"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Server stats reset.\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"conf"),": Print details about serving configuration.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"server_id=1\ntcp_port=2181\nfour_letter_word_white_list=*\nlog_storage_path=./coordination/logs\nsnapshot_storage_path=./coordination/snapshots\nmax_requests_batch_size=100\nsession_timeout_ms=30000\noperation_timeout_ms=10000\ndead_session_check_period_ms=500\nheart_beat_interval_ms=500\nelection_timeout_lower_bound_ms=1000\nelection_timeout_upper_bound_ms=2000\nreserved_log_items=1000000000000000\nsnapshot_distance=10000\nauto_forwarding=true\nshutdown_timeout=5000\nstartup_timeout=240000\nraft_logs_level=information\nsnapshots_to_keep=3\nrotate_log_storage_interval=100000\nstale_log_gap=10000\nfresh_log_gap=200\nmax_requests_batch_size=100\nquorum_reads=false\nforce_sync=false\ncompress_logs=true\ncompress_snapshots_with_zstd_format=true\nconfiguration_change_tries_count=20\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"cons"),": List full connection/session details for all clients connected to this server. Includes information on numbers of packets received/sent, session id, operation latencies, last operation performed, etc...")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"}," 192.168.1.1:52163(recved=0,sent=0,sid=0xffffffffffffffff,lop=NA,est=1636454787393,to=30000,lzxid=0xffffffffffffffff,lresp=0,llat=0,minlat=0,avglat=0,maxlat=0)\n 192.168.1.1:52042(recved=9,sent=18,sid=0x0000000000000001,lop=List,est=1636454739887,to=30000,lcxid=0x0000000000000005,lzxid=0x0000000000000005,lresp=1636454739892,llat=0,minlat=0,avglat=0,maxlat=0)\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"crst"),": Reset connection/session statistics for all connections.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Connection stats reset.\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"envi"),": Print details about serving environment")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Environment:\nclickhouse.keeper.version=v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7\nhost.name=ZBMAC-C02D4054M.local\nos.name=Darwin\nos.arch=x86_64\nos.version=19.6.0\ncpu.count=12\nuser.name=root\nuser.home=/Users/JackyWoo/\nuser.dir=/Users/JackyWoo/project/jd/clickhouse/cmake-build-debug/programs/\nuser.tmp=/var/folders/b4/smbq5mfj7578f2jzwn602tt40000gn/T/\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dirs"),": Shows the total size of snapshot and log files in bytes")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"snapshot_dir_size: 0\nlog_dir_size: 3875\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"isro"),': Tests if server is running in read-only mode. The server will respond with "ro" if in read-only mode or "rw" if not in read-only mode.')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"rw\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"wchs"),": Lists brief information on watches for the server.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"1 connections watching 1 paths\nTotal watches:1\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"wchc"),": Lists detailed information on watches for the server, by session. This outputs a list of sessions (connections) with associated watches (paths). Note, depending on the number of watches this operation may be expensive (ie impact server performance), use it carefully.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"0x0000000000000001\n    /clickhouse/task_queue/ddl\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"wchp"),": Lists detailed information on watches for the server, by path. This outputs a list of paths (znodes) with associated sessions. Note, depending on the number of watches this operation may be expensive (i. e. impact server performance), use it carefully.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"/clickhouse/task_queue/ddl\n    0x0000000000000001\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dump"),": Lists the outstanding sessions and ephemeral nodes. This only works on the leader.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Sessions dump (2):\n0x0000000000000001\n0x0000000000000002\nSessions with Ephemerals (1):\n0x0000000000000001\n /clickhouse/task_queue/ddl\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"csnp"),": Schedule a snapshot creation task. Return the last committed log index of the scheduled snapshot if success or ",(0,r.kt)("inlineCode",{parentName:"li"},"Failed to schedule snapshot creation task.")," if failed. Note that ",(0,r.kt)("inlineCode",{parentName:"li"},"lgif")," command can help you determine whether the snapshot is done.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"100\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"lgif"),": Keeper log information. ",(0,r.kt)("inlineCode",{parentName:"li"},"first_log_idx")," : my first log index in log store; ",(0,r.kt)("inlineCode",{parentName:"li"},"first_log_term")," : my first log term; ",(0,r.kt)("inlineCode",{parentName:"li"},"last_log_idx")," : my last log index in log store; ",(0,r.kt)("inlineCode",{parentName:"li"},"last_log_term")," : my last log term; ",(0,r.kt)("inlineCode",{parentName:"li"},"last_committed_log_idx")," : my last committed log index in state machine; ",(0,r.kt)("inlineCode",{parentName:"li"},"leader_committed_log_idx")," : leader's committed log index from my perspective; ",(0,r.kt)("inlineCode",{parentName:"li"},"target_committed_log_idx")," : target log index should be committed to; ",(0,r.kt)("inlineCode",{parentName:"li"},"last_snapshot_idx")," : the largest committed log index in last snapshot.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"first_log_idx   1\nfirst_log_term  1\nlast_log_idx    101\nlast_log_term   1\nlast_committed_log_idx  100\nleader_committed_log_idx    101\ntarget_committed_log_idx    101\nlast_snapshot_idx   50\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"rqld"),": Request to become new leader. Return ",(0,r.kt)("inlineCode",{parentName:"li"},"Sent leadership request to leader.")," if request sent or ",(0,r.kt)("inlineCode",{parentName:"li"},"Failed to send leadership request to leader.")," if request not sent. Note that if node is already leader the outcome is same as the request is sent.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Sent leadership request to leader.\n")),(0,r.kt)("h3",{id:"migration-from-zookeeper"},"Migration from ZooKeeper"),(0,r.kt)("p",null,"Seamlessly migration from ZooKeeper to ClickHouse Keeper is impossible you have to stop your ZooKeeper cluster, convert data and start ClickHouse Keeper. ",(0,r.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-converter")," tool allows converting ZooKeeper logs and snapshots to ClickHouse Keeper snapshot. It works only with ZooKeeper > 3.4. Steps for migration:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Stop all ZooKeeper nodes.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Optional, but recommended: find ZooKeeper leader node, start and stop it again. It will force ZooKeeper to create a consistent snapshot.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Run ",(0,r.kt)("inlineCode",{parentName:"p"},"clickhouse-keeper-converter")," on a leader, for example:"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"clickhouse-keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/version-2 --output-dir /path/to/clickhouse/keeper/snapshots\n")),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Copy snapshot to ClickHouse server nodes with a configured ",(0,r.kt)("inlineCode",{parentName:"li"},"keeper")," or start ClickHouse Keeper instead of ZooKeeper. The snapshot must persist on all nodes, otherwise, empty nodes can be faster and one of them can become a leader.")),(0,r.kt)("h3",{id:"recovering-after-losing-quorum"},"Recovering after losing quorum"),(0,r.kt)("p",null,"Because ClickHouse Keeper uses Raft it can tolerate certain amount of node crashes depending on the cluster size. \\\nE.g. for a 3-node cluster, it will continue working correctly if only 1 node crashes."),(0,r.kt)("p",null,"Cluster configuration can be dynamically configured but there are some limitations. Reconfiguration relies on Raft also\nso to add/remove a node from the cluster you need to have a quorum. If you lose too many nodes in your cluster at the same time without any chance\nof starting them again, Raft will stop working and not allow you to reconfigure your cluster using the conventional way."),(0,r.kt)("p",null,"Nevertheless, ClickHouse Keeper has a recovery mode which allows you to forcefully reconfigure your cluster with only 1 node.\nThis should be done only as your last resort if you cannot start your nodes again, or start a new instance on the same endpoint."),(0,r.kt)("p",null,"Important things to note before continuing:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Make sure that the failed nodes cannot connect to the cluster again."),(0,r.kt)("li",{parentName:"ul"},"Do not start any of the new nodes until it's specified in the steps.")),(0,r.kt)("p",null,"After making sure that the above things are true, you need to do following:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Pick a single Keeper node to be your new leader. Be aware that the data of that node will be used for the entire cluster so we recommend to use a node with the most up to date state."),(0,r.kt)("li",{parentName:"ol"},"Before doing anything else, make a backup of the ",(0,r.kt)("inlineCode",{parentName:"li"},"log_storage_path")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"snapshot_storage_path")," folders of the picked node."),(0,r.kt)("li",{parentName:"ol"},"Reconfigure the cluster on all of the nodes you want to use."),(0,r.kt)("li",{parentName:"ol"},"Send the four letter command ",(0,r.kt)("inlineCode",{parentName:"li"},"rcvr")," to the node you picked which will move the node to the recovery mode OR stop Keeper instance on the picked node and start it again with the ",(0,r.kt)("inlineCode",{parentName:"li"},"--force-recovery")," argument."),(0,r.kt)("li",{parentName:"ol"},"One by one, start Keeper instances on the new nodes making sure that ",(0,r.kt)("inlineCode",{parentName:"li"},"mntr")," returns ",(0,r.kt)("inlineCode",{parentName:"li"},"follower")," for the ",(0,r.kt)("inlineCode",{parentName:"li"},"zk_server_state")," before starting the next one."),(0,r.kt)("li",{parentName:"ol"},"While in the recovery mode, the leader node will return error message for ",(0,r.kt)("inlineCode",{parentName:"li"},"mntr")," command until it achieves quorum with the new nodes and refuse any requests from the client and the followers."),(0,r.kt)("li",{parentName:"ol"},"After quorum is achieved, the leader node will return to the normal mode of operation, accepting all the requests using Raft - verify with ",(0,r.kt)("inlineCode",{parentName:"li"},"mntr")," which should return ",(0,r.kt)("inlineCode",{parentName:"li"},"leader")," for the ",(0,r.kt)("inlineCode",{parentName:"li"},"zk_server_state"),".")),(0,r.kt)("h2",{id:"clickhouse-keeper-user-guide"},"ClickHouse Keeper User Guide"),(0,r.kt)("p",null,"This guide provides simple and minimal settings to configure ClicKHouse Keeper with an example on how to test distributed operations. This example is performed using 3 nodes on Linux."),(0,r.kt)("h3",{id:"1-configure-nodes-with-keeper-settings"},"1. Configure Nodes with Keeper settings"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Install 3 ClickHouse instances on 3 hosts (chnode1, chnode2, chnode3). (View the ",(0,r.kt)("a",{parentName:"p",href:"/docs/en/getting-started/quick-start"},"Quick Start")," for details on installing ClickHouse.)")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"On each node, add the following entry to allow external communication through the network interface."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<listen_host>0.0.0.0</listen_host>\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Add the following ClickHouse Keeper configuration to all three servers updating the ",(0,r.kt)("inlineCode",{parentName:"p"},"<server_id>")," setting for each server; for ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode1")," would be ",(0,r.kt)("inlineCode",{parentName:"p"},"1"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode2")," would be ",(0,r.kt)("inlineCode",{parentName:"p"},"2"),", etc."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<keeper_server>\n    <tcp_port>9181</tcp_port>\n    <server_id>1</server_id>\n    <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n    <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n    <coordination_settings>\n        <operation_timeout_ms>10000</operation_timeout_ms>\n        <session_timeout_ms>30000</session_timeout_ms>\n        <raft_logs_level>warning</raft_logs_level>\n    </coordination_settings>\n\n    <raft_configuration>\n        <server>\n            <id>1</id>\n            <hostname>chnode1.domain.com</hostname>\n            <port>9234</port>\n        </server>\n        <server>\n            <id>2</id>\n            <hostname>chnode2.domain.com</hostname>\n            <port>9234</port>\n        </server>\n        <server>\n            <id>3</id>\n            <hostname>chnode3.domain.com</hostname>\n            <port>9234</port>\n        </server>\n    </raft_configuration>\n</keeper_server>\n")),(0,r.kt)("p",{parentName:"li"},"These are the basic settings used above:"),(0,r.kt)("table",{parentName:"li"},(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Example"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp_port"),(0,r.kt)("td",{parentName:"tr",align:null},"port to be used by clients of keeper"),(0,r.kt)("td",{parentName:"tr",align:null},"9181 default equivalent of 2181 as in zookeeper")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"server_id"),(0,r.kt)("td",{parentName:"tr",align:null},"identifier for each CLickHouse Keeper server used in raft configuration"),(0,r.kt)("td",{parentName:"tr",align:null},"1")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"coordination_settings"),(0,r.kt)("td",{parentName:"tr",align:null},"section to parameters such as timeouts"),(0,r.kt)("td",{parentName:"tr",align:null},"timeouts: 10000, log level: trace")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"server"),(0,r.kt)("td",{parentName:"tr",align:null},"definition of server participating"),(0,r.kt)("td",{parentName:"tr",align:null},"list of each server definition")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"raft_configuration"),(0,r.kt)("td",{parentName:"tr",align:null},"settings for each server in the keeper cluster"),(0,r.kt)("td",{parentName:"tr",align:null},"server and settings for each")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"id"),(0,r.kt)("td",{parentName:"tr",align:null},"numeric id of the server for keeper services"),(0,r.kt)("td",{parentName:"tr",align:null},"1")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hostname"),(0,r.kt)("td",{parentName:"tr",align:null},"hostname, IP or FQDN of each server in the keeper cluster"),(0,r.kt)("td",{parentName:"tr",align:null},"chnode1.domain.com")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"port"),(0,r.kt)("td",{parentName:"tr",align:null},"port to listen on for interserver keeper connections"),(0,r.kt)("td",{parentName:"tr",align:null},"9234")))))),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Enable the Zookeeper component. It will use the ClickHouse Keeper engine:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"    <zookeeper>\n        <node>\n            <host>chnode1.domain.com</host>\n            <port>9181</port>\n        </node>\n        <node>\n            <host>chnode2.domain.com</host>\n            <port>9181</port>\n        </node>\n        <node>\n            <host>chnode3.domain.com</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n")),(0,r.kt)("p",{parentName:"li"},"These are the basic settings used above:"),(0,r.kt)("table",{parentName:"li"},(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Example"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node"),(0,r.kt)("td",{parentName:"tr",align:null},"list of nodes for ClickHouse Keeper connections"),(0,r.kt)("td",{parentName:"tr",align:null},"settings entry for each server")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"host"),(0,r.kt)("td",{parentName:"tr",align:null},"hostname, IP or FQDN of each ClickHouse keepr node"),(0,r.kt)("td",{parentName:"tr",align:null},"chnode1.domain.com")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"port"),(0,r.kt)("td",{parentName:"tr",align:null},"ClickHouse Keeper client port"),(0,r.kt)("td",{parentName:"tr",align:null},"9181"))))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Restart ClickHouse and verify that each Keeper instance is running. Execute the following command on each server. The ",(0,r.kt)("inlineCode",{parentName:"p"},"ruok")," command returns ",(0,r.kt)("inlineCode",{parentName:"p"},"imok")," if Keeper is running and healthy:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# echo ruok | nc localhost 9181; echo\nimok\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"The ",(0,r.kt)("inlineCode",{parentName:"p"},"system")," database has a table named ",(0,r.kt)("inlineCode",{parentName:"p"},"zookeeper")," that contains the details of your ClickHouse Keeper instances. Let's view the table:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM system.zookeeper\nWHERE path IN ('/', '/clickhouse')\n")),(0,r.kt)("p",{parentName:"li"},"The table looks like:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u252c\u2500czxid\u2500\u252c\u2500mzxid\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500ctime\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500mtime\u2500\u252c\u2500version\u2500\u252c\u2500cversion\u2500\u252c\u2500aversion\u2500\u252c\u2500ephemeralOwner\u2500\u252c\u2500dataLength\u2500\u252c\u2500numChildren\u2500\u252c\u2500pzxid\u2500\u252c\u2500path\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 clickhouse \u2502       \u2502   124 \u2502   124 \u2502 2022-03-07 00:49:34 \u2502 2022-03-07 00:49:34 \u2502       0 \u2502        2 \u2502        0 \u2502              0 \u2502          0 \u2502           2 \u2502  5693 \u2502 /           \u2502\n\u2502 task_queue \u2502       \u2502   125 \u2502   125 \u2502 2022-03-07 00:49:34 \u2502 2022-03-07 00:49:34 \u2502       0 \u2502        1 \u2502        0 \u2502              0 \u2502          0 \u2502           1 \u2502   126 \u2502 /clickhouse \u2502\n\u2502 tables     \u2502       \u2502  5693 \u2502  5693 \u2502 2022-03-07 00:49:34 \u2502 2022-03-07 00:49:34 \u2502       0 \u2502        3 \u2502        0 \u2502              0 \u2502          0 \u2502           3 \u2502  6461 \u2502 /clickhouse \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")))),(0,r.kt)("h3",{id:"2--configure-a-cluster-in-clickhouse"},"2.  Configure a cluster in ClickHouse"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Let's configure a simple cluster with 2 shards and only one replica on 2 of the nodes. The third node will be used to achieve a quorum for the requirement in ClickHouse Keeper. Update the configuration on ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode2"),". The following cluster defines 1 shard on each node for a total of 2 shards with no replication. In this example, some of the data will be on node and some will be on the other node:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"    <remote_servers>\n        <cluster_2S_1R>\n            <shard>\n                <replica>\n                    <host>chnode1.domain.com</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>ClickHouse123!</password>\n                </replica>\n            </shard>\n            <shard>\n                <replica>\n                    <host>chnode2.domain.com</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>ClickHouse123!</password>\n                </replica>\n            </shard>\n        </cluster_2S_1R>\n    </remote_servers>\n")),(0,r.kt)("table",{parentName:"li"},(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Example"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"shard"),(0,r.kt)("td",{parentName:"tr",align:null},"list of replicas on the cluster definition"),(0,r.kt)("td",{parentName:"tr",align:null},"list of replicas for each shard")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"replica"),(0,r.kt)("td",{parentName:"tr",align:null},"list of settings for each replica"),(0,r.kt)("td",{parentName:"tr",align:null},"settings entries for each replica")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"host"),(0,r.kt)("td",{parentName:"tr",align:null},"hostname, IP or FQDN of server that will host a replica shard"),(0,r.kt)("td",{parentName:"tr",align:null},"chnode1.domain.com")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"port"),(0,r.kt)("td",{parentName:"tr",align:null},"port used to communicate using the native tcp protocol"),(0,r.kt)("td",{parentName:"tr",align:null},"9000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"user"),(0,r.kt)("td",{parentName:"tr",align:null},"username that will be used to authenticate to the cluster instances"),(0,r.kt)("td",{parentName:"tr",align:null},"default")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"password"),(0,r.kt)("td",{parentName:"tr",align:null},"password for the user define to allow connections to cluster instances"),(0,r.kt)("td",{parentName:"tr",align:null},"ClickHouse123!")))))),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Restart ClickHouse and verify the cluster was created:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"SHOW clusters;\n")),(0,r.kt)("p",{parentName:"li"},"You should see your cluster:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 cluster_2S_1R \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")))),(0,r.kt)("h3",{id:"3-create-and-test-distributed-table"},"3. Create and test distributed table"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a new database on the new cluster using ClickHouse client on ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode1"),". The ",(0,r.kt)("inlineCode",{parentName:"p"},"ON CLUSTER")," clause automatically creates the database on both nodes."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE DATABASE db1 ON CLUSTER 'cluster_2S_1R';\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a new table on the ",(0,r.kt)("inlineCode",{parentName:"p"},"db1")," database. Once again, ",(0,r.kt)("inlineCode",{parentName:"p"},"ON CLUSTER")," creates the table on both nodes."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE db1.table1 on cluster 'cluster_2S_1R'\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = MergeTree\nORDER BY column1\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"On the ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode1")," node, add a couple of rows:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO db1.table1\n    (id, column1)\nVALUES\n    (1, 'abc'),\n    (2, 'def')\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Add a couple of rows on the ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode2")," node:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO db1.table1\n    (id, column1)\nVALUES\n    (3, 'ghi'),\n    (4, 'jkl')\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Notice that running a ",(0,r.kt)("inlineCode",{parentName:"p"},"SELECT")," statement on each node only shows the data on that node. For example, on ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode1"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM db1.table1\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 7ef1edbc-df25-462b-a9d4-3fe6f9cb0b6d\n\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows in set. Elapsed: 0.006 sec.\n")),(0,r.kt)("p",{parentName:"li"},"On ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode2"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"SELECT *\nFROM db1.table1\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: c43763cc-c69c-4bcc-afbe-50e764adfcbf\n\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  3 \u2502 ghi     \u2502\n\u2502  4 \u2502 jkl     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"You can create a ",(0,r.kt)("inlineCode",{parentName:"p"},"Distributed")," table to represent the data on the two shards. Tables with the ",(0,r.kt)("inlineCode",{parentName:"p"},"Distributed")," table engine do not store any data of their own, but allow distributed query processing on multiple servers. Reads hit all the shards, and writes can be distributed across the shards. Run the following query on ",(0,r.kt)("inlineCode",{parentName:"p"},"chnode1"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE db1.dist_table (\n    id UInt64,\n    column1 String\n)\nENGINE = Distributed(cluster_2S_1R,db1,table1)\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Notice querying ",(0,r.kt)("inlineCode",{parentName:"p"},"dist_table")," returns all four rows of data from the two shards:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM db1.dist_table\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 495bffa0-f849-4a0c-aeea-d7115a54747a\n\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  3 \u2502 ghi     \u2502\n\u2502  4 \u2502 jkl     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n4 rows in set. Elapsed: 0.018 sec.\n")))),(0,r.kt)("h3",{id:"summary"},"Summary"),(0,r.kt)("p",null,"This guide demostrated how to setup a cluster using ClickHouse Keeper. With ClickHouse Keeper, you can configure clusters and define distributed tables that can be replicated across shards."),(0,r.kt)("h2",{id:"configuring-clickhouse-keeper-with-unique-paths"},"Configuring ClickHouse Keeper with unique paths"),(0,r.kt)(o.ZP,{mdxType:"SelfManaged"}),(0,r.kt)("h3",{id:"description"},"Description"),(0,r.kt)("p",null,"This article describes how to use the built-in ",(0,r.kt)("inlineCode",{parentName:"p"},"{uuid}")," macro setting\nto create unique entries in ClickHouse Keeper or ZooKeeper. Unique\npaths helps when creating and dropping tables frequently because\nthis avoids having to wait several minutes for Keeper garbage collection\nto remove path entries as each time a path is created a new ",(0,r.kt)("inlineCode",{parentName:"p"},"uuid")," is used\nin that path; paths are never reused."),(0,r.kt)("h3",{id:"example-environment"},"Example Environment"),(0,r.kt)("p",null,"A three node cluster that will be configured to have ClickHouse Keeper\non all three nodes, and ClickHouse on two of the nodes. This provides\nClickHouse Keeper with three nodes (including a tiebreaker node), and\na single ClickHouse shard made up of two replicas."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"node"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"chnode1.marsnet.local"),(0,r.kt)("td",{parentName:"tr",align:null},"data node - cluster cluster_1S_2R")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"chnode2.marsnet.local"),(0,r.kt)("td",{parentName:"tr",align:null},"data node - cluster cluster_1S_2R")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"chnode3.marsnet.local"),(0,r.kt)("td",{parentName:"tr",align:null},"ClickHouse Keeper tie breaker node")))),(0,r.kt)("p",null,"example config for cluster:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"    <remote_servers>\n        <cluster_1S_2R>\n            <shard>\n                <replica>\n                    <host>chnode1.marsnet.local</host>\n                    <port>9440</port>\n                    <user>default</user>\n                    <password>ClickHouse123!</password>\n                    <secure>1</secure>\n                </replica>\n                <replica>\n                    <host>chnode2.marsnet.local</host>\n                    <port>9440</port>\n                    <user>default</user>\n                    <password>ClickHouse123!</password>\n                    <secure>1</secure>\n                </replica>\n            </shard>\n        </cluster_1S_2R>\n    </remote_servers>\n")),(0,r.kt)("h3",{id:"procedures-to-set-up-tables-to-use-uuid"},"Procedures to set up tables to use {uuid}"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Configure Macros on each server\nexample for server 1:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"    <macros>\n        <shard>1</shard>\n        <replica>replica_1</replica>\n    </macros>\n")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Notice that we define macros for ",(0,r.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"replica"),", but that ",(0,r.kt)("inlineCode",{parentName:"p"},"{uuid}")," is not defined here, it is built-in and there is no need to define.")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Create a Database")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE DATABASE db_uuid\n      ON CLUSTER 'cluster_1S_2R'\n      ENGINE Atomic;\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"CREATE DATABASE db_uuid ON CLUSTER cluster_1S_2R\nENGINE = Atomic\n\nQuery id: 07fb7e65-beb4-4c30-b3ef-bd303e5c42b5\n\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode2.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode1.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Create a table on the cluster using the macros and ",(0,r.kt)("inlineCode",{parentName:"li"},"{uuid}"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE db_uuid.uuid_table1 ON CLUSTER 'cluster_1S_2R'\n   (\n     id UInt64,\n     column1 String\n   )\n   ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/db_uuid/{uuid}', '{replica}' )\n   ORDER BY (id);\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"CREATE TABLE db_uuid.uuid_table1 ON CLUSTER cluster_1S_2R\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/db_uuid/{uuid}', '{replica}')\nORDER BY id\n\nQuery id: 8f542664-4548-4a02-bd2a-6f2c973d0dc4\n\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode1.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode2.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Create a distributed table")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"create table db_uuid.dist_uuid_table1 on cluster 'cluster_1S_2R'\n   (\n     id UInt64,\n     column1 String\n   )\n   ENGINE = Distributed('cluster_1S_2R', 'db_uuid', 'uuid_table1' );\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"CREATE TABLE db_uuid.dist_uuid_table1 ON CLUSTER cluster_1S_2R\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = Distributed('cluster_1S_2R', 'db_uuid', 'uuid_table1')\n\nQuery id: 3bc7f339-ab74-4c7d-a752-1ffe54219c0e\n\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode2.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode1.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,r.kt)("h3",{id:"testing"},"Testing"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Insert data into first node (e.g ",(0,r.kt)("inlineCode",{parentName:"li"},"chnode1"),")")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO db_uuid.uuid_table1\n   ( id, column1)\n   VALUES\n   ( 1, 'abc');\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"INSERT INTO db_uuid.uuid_table1 (id, column1) FORMAT Values\n\nQuery id: 0f178db7-50a6-48e2-9a1b-52ed14e6e0f9\n\nOk.\n\n1 row in set. Elapsed: 0.033 sec.\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Insert data into second node (e.g ",(0,r.kt)("inlineCode",{parentName:"li"},"chnode2"),")")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO db_uuid.uuid_table1\n   ( id, column1)\n   VALUES\n   ( 2, 'def');\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"INSERT INTO db_uuid.uuid_table1 (id, column1) FORMAT Values\n\nQuery id: edc6f999-3e7d-40a0-8a29-3137e97e3607\n\nOk.\n\n1 row in set. Elapsed: 0.529 sec.\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"View records using distributed table")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM db_uuid.dist_uuid_table1;\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"SELECT *\nFROM db_uuid.dist_uuid_table1\n\nQuery id: 6cbab449-9e7f-40fe-b8c2-62d46ba9f5c8\n\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows in set. Elapsed: 0.007 sec.\n")),(0,r.kt)("h3",{id:"alternatives"},"Alternatives"),(0,r.kt)("p",null,"The default replication path can be defined before hand by macros and using also ",(0,r.kt)("inlineCode",{parentName:"p"},"{uuid}")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Set default for tables on each node")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<default_replica_path>/clickhouse/tables/{shard}/db_uuid/{uuid}</default_replica_path>\n<default_replica_name>{replica}</default_replica_name>\n")),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"You can also define a macro ",(0,r.kt)("inlineCode",{parentName:"p"},"{database}")," on each node if nodes are used for certain databases.")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Create table without explicit parameters:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE db_uuid.uuid_table1 ON CLUSTER 'cluster_1S_2R'\n   (\n     id UInt64,\n     column1 String\n   )\n   ENGINE = ReplicatedMergeTree\n   ORDER BY (id);\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"CREATE TABLE db_uuid.uuid_table1 ON CLUSTER cluster_1S_2R\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = ReplicatedMergeTree\nORDER BY id\n\nQuery id: ab68cda9-ae41-4d6d-8d3b-20d8255774ee\n\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode2.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode1.marsnet.local \u2502 9440 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows in set. Elapsed: 1.175 sec.\n")),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Verify it used the settings used in default config")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SHOW CREATE TABLE db_uuid.uuid_table1;\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"SHOW CREATE TABLE db_uuid.uuid_table1\n\nQuery id: 5925ecce-a54f-47d8-9c3a-ad3257840c9e\n\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 CREATE TABLE db_uuid.uuid_table1\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/db_uuid/{uuid}', '{replica}')\nORDER BY id\nSETTINGS index_granularity = 8192 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.003 sec.\n")),(0,r.kt)("h3",{id:"troubleshooting"},"Troubleshooting"),(0,r.kt)("p",null,"Example command to get table information and UUID:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM system.tables\nWHERE database = 'db_uuid' AND name = 'uuid_table1';\n")),(0,r.kt)("p",null,"Example command to get information about the table in zookeeper with UUID for the table above"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM system.zookeeper\nWHERE path = '/clickhouse/tables/1/db_uuid/9e8a3cc2-0dec-4438-81a7-c3e63ce2a1cf/replicas';\n")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Database must be ",(0,r.kt)("inlineCode",{parentName:"p"},"Atomic"),", if upgrading from a previous version, the\n",(0,r.kt)("inlineCode",{parentName:"p"},"default")," database is likely of ",(0,r.kt)("inlineCode",{parentName:"p"},"Ordinary")," type.")),(0,r.kt)("p",null,"To check:\nFor example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"SELECT name, engine FROM system.databases WHERE name = 'db_uuid';\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-response"},"SELECT\n    name,\n    engine\nFROM system.databases\nWHERE name = 'db_uuid'\n\nQuery id: b047d459-a1d2-4016-bcf9-3e97e30e49c2\n\n\u250c\u2500name\u2500\u2500\u2500\u2500\u252c\u2500engine\u2500\u2510\n\u2502 db_uuid \u2502 Atomic \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.004 sec.\n")))}m.isMDXComponent=!0}}]);