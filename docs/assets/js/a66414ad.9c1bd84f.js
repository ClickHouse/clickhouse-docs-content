"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[78739],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>g});var o=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=o.createContext({}),c=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},d=function(e){var n=c(e.components);return o.createElement(s.Provider,{value:n},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=c(t),u=i,g=p["".concat(s,".").concat(u)]||p[u]||h[u]||r;return t?o.createElement(g,a(a({ref:n},d),{},{components:t})):o.createElement(g,a({ref:n},d))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,a=new Array(r);a[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:i,a[1]=l;for(var c=2;c<r;c++)a[c]=t[c];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},76450:(e,n,t)=>{t.d(n,{ZP:()=>l});var o=t(87462),i=(t(67294),t(3905));const r={toc:[]},a="wrapper";function l(e){let{components:n,...t}=e;return(0,i.kt)(a,(0,o.Z)({},r,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("admonition",{title:"best practices",type:"important"},(0,i.kt)("p",{parentName:"admonition"},"When configuring ClickHouse Server by adding or editing configuration files you should:"),(0,i.kt)("ul",{parentName:"admonition"},(0,i.kt)("li",{parentName:"ul"},"Add files to ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/config.d/")," directory"),(0,i.kt)("li",{parentName:"ul"},"Add files to ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/users.d/")," directory"),(0,i.kt)("li",{parentName:"ul"},"Leave the ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/config.xml")," file as it is"),(0,i.kt)("li",{parentName:"ul"},"Leave the ",(0,i.kt)("inlineCode",{parentName:"li"},"/etc/clickhouse-server/users.xml")," file as it is "))))}l.isMDXComponent=!0},29214:(e,n,t)=>{t.d(n,{ZP:()=>l});var o=t(87462),i=(t(67294),t(3905));const r={toc:[{value:"Terminology",id:"terminology",level:2},{value:"Replica",id:"replica",level:3},{value:"Shard",id:"shard",level:3},{value:"Distributed coordination",id:"distributed-coordination",level:3}]},a="wrapper";function l(e){let{components:n,...t}=e;return(0,i.kt)(a,(0,o.Z)({},r,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"terminology"},"Terminology"),(0,i.kt)("h3",{id:"replica"},"Replica"),(0,i.kt)("p",null,"A copy of data.  ClickHouse always has at least one copy of your data, and so the minimum number of ",(0,i.kt)("strong",{parentName:"p"},"replicas")," is one.  This is an important detail, you may not be used to counting the original copy of your data as a replica, but that is the term used in ClickHouse code and documentation.  Adding a second replica of your data provides fault tolerance. "),(0,i.kt)("h3",{id:"shard"},"Shard"),(0,i.kt)("p",null,"A subset of data.  ClickHouse always has at least one shard for your data, so if you do not split the data across multiple servers, your data will be stored in one shard.  Sharding data across multiple servers can be used to divide the load if you exceed the capacity of a single server. The destination server is determined by the ",(0,i.kt)("strong",{parentName:"p"},"sharding key"),", and is defined when you create the distributed table. The sharding key can be random or as an output of a ",(0,i.kt)("a",{parentName:"p",href:"https://clickhouse.com/docs/en/sql-reference/functions/hash-functions"},"hash function"),".  The deployment examples involving sharding will use ",(0,i.kt)("inlineCode",{parentName:"p"},"rand()")," as the sharding key, and will provide further information on when and how to choose a different sharding key."),(0,i.kt)("h3",{id:"distributed-coordination"},"Distributed coordination"),(0,i.kt)("p",null,"ClickHouse Keeper provides the coordination system for data replication and distributed DDL queries execution. ClickHouse Keeper is compatible with Apache ZooKeeper."))}l.isMDXComponent=!0},77683:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>g,frontMatter:()=>l,metadata:()=>c,toc:()=>p});var o=t(87462),i=(t(67294),t(3905)),r=t(29214),a=t(76450);const l={slug:"/en/architecture/horizontal-scaling",sidebar_label:"Scaling out",sidebar_position:10,title:"Scaling out"},s=void 0,c={unversionedId:"en/deployment-guides/horizontal-scaling",id:"en/deployment-guides/horizontal-scaling",title:"Scaling out",description:"Description",source:"@site/docs/en/deployment-guides/horizontal-scaling.md",sourceDirName:"en/deployment-guides",slug:"/en/architecture/horizontal-scaling",permalink:"/docs/en/architecture/horizontal-scaling",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/deployment-guides/horizontal-scaling.md",tags:[],version:"current",sidebarPosition:10,frontMatter:{slug:"/en/architecture/horizontal-scaling",sidebar_label:"Scaling out",sidebar_position:10,title:"Scaling out"},sidebar:"docs",previous:{title:"Introduction",permalink:"/docs/en/architecture/introduction"},next:{title:"Replication for fault tolerance",permalink:"/docs/en/architecture/replication"}},d={},p=[{value:"Description",id:"description",level:2},{value:"Level: Basic",id:"level-basic",level:2},{value:"Environment",id:"environment",level:2},{value:"Architecture Diagram",id:"architecture-diagram",level:3},{value:"Install",id:"install",level:2},{value:"Editing configuration files",id:"editing-configuration-files",level:2},{value:"chnode1 configuration",id:"chnode1-configuration",level:2},{value:"Network and logging configuration",id:"network-and-logging-configuration",level:3},{value:"ClickHouse Keeper configuration",id:"clickhouse-keeper-configuration",level:3},{value:"Macros configuration",id:"macros-configuration",level:3},{value:"Replication and sharding configuration",id:"replication-and-sharding-configuration",level:3},{value:"Configuring the use of Keeper",id:"configuring-the-use-of-keeper",level:3},{value:"chnode2 configuration",id:"chnode2-configuration",level:2},{value:"Network and logging configuration",id:"network-and-logging-configuration-1",level:3},{value:"ClickHouse Keeper configuration",id:"clickhouse-keeper-configuration-1",level:3},{value:"Macros configuration",id:"macros-configuration-1",level:3},{value:"Replication and sharding configuration",id:"replication-and-sharding-configuration-1",level:3},{value:"Configuring the use of Keeper",id:"configuring-the-use-of-keeper-1",level:3},{value:"chnode3 configuration",id:"chnode3-configuration",level:2},{value:"Network and logging configuration",id:"network-and-logging-configuration-2",level:3},{value:"ClickHouse Keeper configuration",id:"clickhouse-keeper-configuration-2",level:3},{value:"Testing",id:"testing",level:2},{value:"More information about:",id:"more-information-about",level:2}],h={toc:p},u="wrapper";function g(e){let{components:n,...l}=e;return(0,i.kt)(u,(0,o.Z)({},h,l,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"description"},"Description"),(0,i.kt)("p",null,"This example architecture is designed to provide scalability.  It includes three nodes: two combined ClickHouse plus coordination (ClickHouse Keeper) servers, and a third server with only ClickHouse Keeper to finish the quorum of three. With this example, we'll create a database, table, and a distributed table that will be able to query the data on both of the nodes."),(0,i.kt)("h2",{id:"level-basic"},"Level: Basic"),(0,i.kt)(r.ZP,{mdxType:"ReplicationShardingTerminology"}),(0,i.kt)("h2",{id:"environment"},"Environment"),(0,i.kt)("h3",{id:"architecture-diagram"},"Architecture Diagram"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Architecture diagram for 2 shards and 1 replica",src:t(94717).Z,width:"985",height:"710"})),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Node"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"chnode1"),(0,i.kt)("td",{parentName:"tr",align:null},"Data + ClickHouse Keeper")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"chnode2"),(0,i.kt)("td",{parentName:"tr",align:null},"Data + ClickHouse Keeper")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"chnode3"),(0,i.kt)("td",{parentName:"tr",align:null},"Used for ClickHouse Keeper quorum")))),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"In more advanced configurations, ClickHouse Keeper will be running on separate servers.  This basic configuration runs the Keeper functionality within the ClickHouse Server process.  As you scale out, you may decide to separate the ClickHouse Servers from the Keeper servers.  The instructions for deploying ClickHouse Keeper standalone are available in the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#install-standalone-clickhouse-keeper"},"installation documentation"),".")),(0,i.kt)("h2",{id:"install"},"Install"),(0,i.kt)("p",null,"Install Clickhouse on three servers following the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#available-installation-options"},"instructions for your archive type")," (.deb, .rpm, .tar.gz, etc.). For this example, you will follow the installation instructions for ClickHouse Server and Client on all three machines.       "),(0,i.kt)("h2",{id:"editing-configuration-files"},"Editing configuration files"),(0,i.kt)(a.ZP,{mdxType:"ConfigFileNote"}),(0,i.kt)("h2",{id:"chnode1-configuration"},"chnode1 configuration"),(0,i.kt)("p",null,"For chnode1, there are five configuration files.  You may choose to combine these files into a single file, but for clarity in the documentation it may be simpler to look at them separately.  As you read through the configuration files, you will see that most of the configuration is the same between chnode1 and chnode2; the differences will be highlighted."),(0,i.kt)("h3",{id:"network-and-logging-configuration"},"Network and logging configuration"),(0,i.kt)("p",null,"These values can be customized as you wish.  This example configuration gives you a debug log that will roll over at 1000M three times.  ClickHouse will listen on the IPv4 network on ports 8123 and 9000, and will use port 9009 for interserver communication."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="network-and-logging.xml on chnode1"',title:'"network-and-logging.xml',on:!0,'chnode1"':!0},"<clickhouse>\n        <logger>\n                <level>debug</level>\n                <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n                <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n                <size>1000M</size>\n                <count>3</count>\n        </logger>\n        <display_name>clickhouse</display_name>\n        <listen_host>0.0.0.0</listen_host>\n        <http_port>8123</http_port>\n        <tcp_port>9000</tcp_port>\n        <interserver_http_port>9009</interserver_http_port>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"clickhouse-keeper-configuration"},"ClickHouse Keeper configuration"),(0,i.kt)("p",null,"ClickHouse Keeper provides the coordination system for data replication and distributed DDL queries execution. ClickHouse Keeper is compatible with Apache ZooKeeper.  This configuration enables ClickHouse Keeper on port 9181.  The highlighted line specifies that this instance of Keeper has ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," of 1.  This is the only difference in the ",(0,i.kt)("inlineCode",{parentName:"p"},"enable-keeper.xml")," file across the three servers.  ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2")," will have ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," set to ",(0,i.kt)("inlineCode",{parentName:"p"},"2"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode3")," will have ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," set to ",(0,i.kt)("inlineCode",{parentName:"p"},"3"),".  The raft configuration section is the same on all three servers, and it is highlighted below to show you the relationship between ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," and the ",(0,i.kt)("inlineCode",{parentName:"p"},"server")," instance within the raft configuration."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"If for any reason a Keeper node is replaced or rebuilt, do not reuse an existing ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id"),".  For example, if the Keeper node with ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"2")," is rebuilt, give it server_id of ",(0,i.kt)("inlineCode",{parentName:"p"},"4")," or higher.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="enable-keeper.xml on chnode1"',title:'"enable-keeper.xml',on:!0,'chnode1"':!0},"<clickhouse>\n  <keeper_server>\n    <tcp_port>9181</tcp_port>\n # highlight-next-line\n    <server_id>1</server_id>\n    <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n    <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n    <coordination_settings>\n        <operation_timeout_ms>10000</operation_timeout_ms>\n        <session_timeout_ms>30000</session_timeout_ms>\n        <raft_logs_level>trace</raft_logs_level>\n    </coordination_settings>\n\n    <raft_configuration>\n    # highlight-start\n        <server>\n            <id>1</id>\n            <hostname>chnode1</hostname>\n            <port>9234</port>\n        </server>\n    # highlight-end\n        <server>\n            <id>2</id>\n            <hostname>chnode2</hostname>\n            <port>9234</port>\n        </server>\n        <server>\n            <id>3</id>\n            <hostname>chnode3</hostname>\n            <port>9234</port>\n        </server>\n    </raft_configuration>\n  </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"macros-configuration"},"Macros configuration"),(0,i.kt)("p",null,"The macros ",(0,i.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," reduce the complexity of distributed DDL.  The values configured are automatically substituted in your DDL queries, which simplifies your DDL.  The macros for this configuration specify the shard and replica number for each node.",(0,i.kt)("br",{parentName:"p"}),"\n","In this 2 shard 1 replica example, the replica macro is ",(0,i.kt)("inlineCode",{parentName:"p"},"replica_1")," on both chnode1 and chnode2 as there is only one replica.  The shard macro is ",(0,i.kt)("inlineCode",{parentName:"p"},"1")," on chnode1 and ",(0,i.kt)("inlineCode",{parentName:"p"},"2")," on chnode2. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="macros.xml on chnode1"',title:'"macros.xml',on:!0,'chnode1"':!0},"<clickhouse>\n  <macros>\n # highlight-next-line\n    <shard>1</shard>\n    <replica>replica_1</replica>\n  </macros>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"replication-and-sharding-configuration"},"Replication and sharding configuration"),(0,i.kt)("p",null,"Starting from the top:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The ",(0,i.kt)("inlineCode",{parentName:"li"},"remote_servers")," section of the XML specifies each of the clusters in the environment. The attribute ",(0,i.kt)("inlineCode",{parentName:"li"},"replace=true")," replaces the sample ",(0,i.kt)("inlineCode",{parentName:"li"},"remote_servers")," in the default ClickHouse configuration with the ",(0,i.kt)("inlineCode",{parentName:"li"},"remote_servers")," configuration specified in this file.  Without this attribute, the remote servers in this file would be appended to the list of samples in the default.  "),(0,i.kt)("li",{parentName:"ul"},"In this example, there is one cluster named ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_2S_1R"),"."),(0,i.kt)("li",{parentName:"ul"},"A secret is created for the cluster named ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_2S_1R")," with the value ",(0,i.kt)("inlineCode",{parentName:"li"},"mysecretphrase"),".  The secret is shared across all of the remote servers in the environment to ensure that the correct servers are joined together."),(0,i.kt)("li",{parentName:"ul"},"The cluster ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_2S_1R")," has two shards, and each of those shards has one replica.  Take a look at the architecture diagram toward the beginning of this document, and compare it with the two ",(0,i.kt)("inlineCode",{parentName:"li"},"shard")," definitions in the XML below.  In each of the shard definitions there is one replica.  The replica is for that specific shard.  The host and port for that replica is specified.  The replica for the first shard in the configuration is stored on ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode1"),", and the replica for the second shard in the configuration is stored on ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode2"),"."),(0,i.kt)("li",{parentName:"ul"},"Internal replication for the shards is set to true.  Each shard can have the ",(0,i.kt)("inlineCode",{parentName:"li"},"internal_replication")," parameter defined in the config file. If this parameter is set to true, the write operation selects the first healthy replica and writes data to it.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="remote-servers.xml on chnode1"',title:'"remote-servers.xml',on:!0,'chnode1"':!0},'<clickhouse>\n  <remote_servers replace="true">\n    <cluster_2S_1R>\n    <secret>mysecretphrase</secret>\n        <shard>\n            <internal_replication>true</internal_replication>\n            <replica>\n                <host>chnode1</host>\n                <port>9000</port>\n            </replica>\n        </shard>\n        <shard>\n            <internal_replication>true</internal_replication>\n            <replica>\n                <host>chnode2</host>\n                <port>9000</port>\n            </replica>\n        </shard>\n    </cluster_2S_1R>\n  </remote_servers>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"configuring-the-use-of-keeper"},"Configuring the use of Keeper"),(0,i.kt)("p",null,"Up above a few files ClickHouse Keeper was configured.  This configuration file ",(0,i.kt)("inlineCode",{parentName:"p"},"use-keeper.xml")," is configuring ClickHouse Server to use ClickHouse Keeper for the coordination of replication and distributed DDL.  This file specifies that ClickHouse Server should use Keeper on nodes chnode1 - 3 on port 9181, and the file is the same on ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),".  "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="use-keeper.xml on chnode1"',title:'"use-keeper.xml',on:!0,'chnode1"':!0},'<clickhouse>\n    <zookeeper>\n        <node index="1">\n            <host>chnode1</host>\n            <port>9181</port>\n        </node>\n        <node index="2">\n            <host>chnode2</host>\n            <port>9181</port>\n        </node>\n        <node index="3">\n            <host>chnode3</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n')),(0,i.kt)("h2",{id:"chnode2-configuration"},"chnode2 configuration"),(0,i.kt)("p",null,"As the configuration is very similar on chnode1 and chnode2, only the differences will be pointed out here."),(0,i.kt)("h3",{id:"network-and-logging-configuration-1"},"Network and logging configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="network-and-logging.xml on chnode2"',title:'"network-and-logging.xml',on:!0,'chnode2"':!0},"<clickhouse>\n        <logger>\n                <level>debug</level>\n                <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n                <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n                <size>1000M</size>\n                <count>3</count>\n        </logger>\n        <display_name>clickhouse</display_name>\n        <listen_host>0.0.0.0</listen_host>\n        <http_port>8123</http_port>\n        <tcp_port>9000</tcp_port>\n        <interserver_http_port>9009</interserver_http_port>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"clickhouse-keeper-configuration-1"},"ClickHouse Keeper configuration"),(0,i.kt)("p",null,"This file contains one of the two differences between chnode1 and chnode2.  In the Keeper configuration the ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"2"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="enable-keeper.xml on chnode2"',title:'"enable-keeper.xml',on:!0,'chnode2"':!0},"<clickhouse>\n  <keeper_server>\n    <tcp_port>9181</tcp_port>\n # highlight-next-line\n    <server_id>2</server_id>\n    <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n    <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n    <coordination_settings>\n        <operation_timeout_ms>10000</operation_timeout_ms>\n        <session_timeout_ms>30000</session_timeout_ms>\n        <raft_logs_level>trace</raft_logs_level>\n    </coordination_settings>\n\n    <raft_configuration>\n        <server>\n            <id>1</id>\n            <hostname>chnode1</hostname>\n            <port>9234</port>\n        </server>\n        # highlight-start\n        <server>\n            <id>2</id>\n            <hostname>chnode2</hostname>\n            <port>9234</port>\n        </server>\n        # highlight-end\n        <server>\n            <id>3</id>\n            <hostname>chnode3</hostname>\n            <port>9234</port>\n        </server>\n    </raft_configuration>\n  </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"macros-configuration-1"},"Macros configuration"),(0,i.kt)("p",null,"The macros configuration has one of the differences between chnode1 and chnode2.  ",(0,i.kt)("inlineCode",{parentName:"p"},"shard")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"2")," on this node."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="macros.xml on chnode2"',title:'"macros.xml',on:!0,'chnode2"':!0},"<clickhouse>\n<macros>\n # highlight-next-line\n    <shard>2</shard>\n    <replica>replica_1</replica>\n</macros>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"replication-and-sharding-configuration-1"},"Replication and sharding configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="remote-servers.xml on chnode2"',title:'"remote-servers.xml',on:!0,'chnode2"':!0},'<clickhouse>\n  <remote_servers replace="true">\n    <cluster_2S_1R>\n    <secret>mysecretphrase</secret>\n        <shard>\n            <internal_replication>true</internal_replication>\n            <replica>\n                <host>chnode1</host>\n                <port>9000</port>\n            </replica>\n        </shard>\n            <shard>\n            <internal_replication>true</internal_replication>\n            <replica>\n                <host>chnode2</host>\n                <port>9000</port>\n            </replica>\n        </shard>\n    </cluster_2S_1R>\n  </remote_servers>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"configuring-the-use-of-keeper-1"},"Configuring the use of Keeper"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="use-keeper.xml on chnode2"',title:'"use-keeper.xml',on:!0,'chnode2"':!0},'<clickhouse>\n    <zookeeper>\n        <node index="1">\n            <host>chnode1</host>\n            <port>9181</port>\n        </node>\n        <node index="2">\n            <host>chnode2</host>\n            <port>9181</port>\n        </node>\n        <node index="3">\n            <host>chnode3</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n')),(0,i.kt)("h2",{id:"chnode3-configuration"},"chnode3 configuration"),(0,i.kt)("p",null,"As chnode3 is not storing data and is only used for ClickHouse Keeper to provide the third node in the quorum, chnode3 has only two configuration files, one to configure the network and logging, and one to configure ClickHouse Keeper."),(0,i.kt)("h3",{id:"network-and-logging-configuration-2"},"Network and logging configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="network-and-logging.xml on chnode3"',title:'"network-and-logging.xml',on:!0,'chnode3"':!0},"<clickhouse>\n        <logger>\n                <level>debug</level>\n                <log>/var/log/clickhouse-server/clickhouse-server.log</log>\n                <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>\n                <size>1000M</size>\n                <count>3</count>\n        </logger>\n        <display_name>clickhouse</display_name>\n        <listen_host>0.0.0.0</listen_host>\n        <http_port>8123</http_port>\n        <tcp_port>9000</tcp_port>\n        <interserver_http_port>9009</interserver_http_port>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"clickhouse-keeper-configuration-2"},"ClickHouse Keeper configuration"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="enable-keeper.xml on chnode3"',title:'"enable-keeper.xml',on:!0,'chnode3"':!0},"<clickhouse>\n  <keeper_server>\n    <tcp_port>9181</tcp_port>\n # highlight-next-line\n    <server_id>3</server_id>\n    <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n    <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n    <coordination_settings>\n        <operation_timeout_ms>10000</operation_timeout_ms>\n        <session_timeout_ms>30000</session_timeout_ms>\n        <raft_logs_level>trace</raft_logs_level>\n    </coordination_settings>\n\n    <raft_configuration>\n        <server>\n            <id>1</id>\n            <hostname>chnode1</hostname>\n            <port>9234</port>\n        </server>\n        <server>\n            <id>2</id>\n            <hostname>chnode2</hostname>\n            <port>9234</port>\n        </server>\n        # highlight-start\n        <server>\n            <id>3</id>\n            <hostname>chnode3</hostname>\n            <port>9234</port>\n        </server>\n        # highlight-end\n    </raft_configuration>\n  </keeper_server>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"testing"},"Testing"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Connect to ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode1")," and verify that the cluster ",(0,i.kt)("inlineCode",{parentName:"li"},"cluster_2S_1R")," configured above exists")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SHOW CLUSTERS\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 cluster_2S_1R \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Create a database on the cluster")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE DATABASE db1 ON CLUSTER cluster_2S_1R\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode2 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode1 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Create a table with MergeTree table engine on the cluster.",(0,i.kt)("admonition",{parentName:"li",type:"note"},(0,i.kt)("p",{parentName:"admonition"},"We do not need not to specify parameters on the table engine since these will be automatically defined based on our macros")))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE db1.table1 ON CLUSTER cluster_2S_1R\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = MergeTree\nORDER BY id\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode1 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode2 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:4},(0,i.kt)("li",{parentName:"ol"},"Connect to ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode1")," and insert a row")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO db1.table1 (id, column1) VALUES (1, 'abc');\n")),(0,i.kt)("ol",{start:5},(0,i.kt)("li",{parentName:"ol"},"Connect to ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode2")," and insert a row")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO db1.table1 (id, column1) VALUES (2, 'def');\n")),(0,i.kt)("ol",{start:6},(0,i.kt)("li",{parentName:"ol"},"Connect to either node, ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode1")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode2")," and you will see only the row that was inserted into that table on that node.\nfor example, on ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode2"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM db1.table1;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:7},(0,i.kt)("li",{parentName:"ol"},"Create a distributed table to query both shards on both nodes.\n(In this exmple, the ",(0,i.kt)("inlineCode",{parentName:"li"},"rand()")," function is set as the sharding key so that it randomly distributes each insert)")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE db1.table1_dist ON CLUSTER cluster_2S_1R\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = Distributed('cluster_2S_1R', 'db1', 'table1', rand())\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode2 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode1 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("ol",{start:8},(0,i.kt)("li",{parentName:"ol"},"Connect to either ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode1")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"chnode2")," and query the distributed table to see both rows.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"SELECT * FROM db1.table1_dist;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-reponse"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  2 \u2502 def     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("h2",{id:"more-information-about"},"More information about:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/engines/table-engines/special/distributed"},"Distributed Table Engine")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/en/guides/sre/keeper/clickhouse-keeper"},"ClickHouse Keeper"))))}g.isMDXComponent=!0},94717:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/scaling-out-1-4ac7bae3be3ccff3e55a29d167b2bff5.png"}}]);