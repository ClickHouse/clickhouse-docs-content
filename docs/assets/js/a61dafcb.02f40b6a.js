"use strict";(self.webpackChunknew_nav_docusaurus_2_2=self.webpackChunknew_nav_docusaurus_2_2||[]).push([[28472],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,s=e.originalType,l=e.parentName,p=r(e,["components","mdxType","originalType","parentName"]),d=c(n),m=i,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||s;return n?a.createElement(h,o(o({ref:t},p),{},{components:n})):a.createElement(h,o({ref:t},p))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var s=n.length,o=new Array(s);o[0]=m;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[d]="string"==typeof e?e:i,o[1]=r;for(var c=2;c<s;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},54964:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>k,frontMatter:()=>l,metadata:()=>p,toc:()=>u});var a=n(87462),i=(n(67294),n(3905));const s={toc:[]},o="wrapper";function r(e){let{components:t,...n}=e;return(0,i.kt)(o,(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"This page is not applicable to ",(0,i.kt)("a",{parentName:"p",href:"https://clickhouse.com/cloud"},"ClickHouse Cloud"),". The procedure documented here is only necessary in self-managed ClickHouse deployments.")))}r.isMDXComponent=!0;const l={slug:"/en/integrations/s3",sidebar_position:1,sidebar_label:"AWS S3"},c="Integrating S3 with ClickHouse",p={unversionedId:"en/integrations/data-ingestion/s3/index",id:"en/integrations/data-ingestion/s3/index",title:"Integrating S3 with ClickHouse",description:"You can insert data from S3 into ClickHouse and also use S3 as an export destination, thus allowing interaction with \u201cData Lake\u201d architectures. Furthermore, S3 can provide \u201ccold\u201d storage tiers and assist with separating storage and compute. In the sections below we use the New York City taxi dataset to demonstrate the process of moving data between S3 and ClickHouse, as well as identifying key configuration parameters and providing hints on optimizing performance.",source:"@site/docs/en/integrations/data-ingestion/s3/index.md",sourceDirName:"en/integrations/data-ingestion/s3",slug:"/en/integrations/s3",permalink:"/docs/en/integrations/s3",draft:!1,editUrl:"https://github.com/ClickHouse/clickhouse-docs/blob/main/docs/en/integrations/data-ingestion/s3/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{slug:"/en/integrations/s3",sidebar_position:1,sidebar_label:"AWS S3"},sidebar:"docs",previous:{title:"Integrations",permalink:"/docs/en/integrations"},next:{title:"Google Cloud Storage (GCS)",permalink:"/docs/en/integrations/gcs"}},d={},u=[{value:"S3 Table Functions",id:"s3-table-functions",level:2},{value:"Preparation",id:"preparation",level:3},{value:"Reading Data from s3",id:"reading-data-from-s3",level:3},{value:"Using clickhouse-local",id:"using-clickhouse-local",level:3},{value:"Inserting Data from S3",id:"inserting-data-from-s3",level:3},{value:"Remote Insert using ClickHouse Local",id:"remote-insert-using-clickhouse-local",level:3},{value:"Exporting Data",id:"exporting-data",level:3},{value:"Splitting Large Files",id:"splitting-large-files",level:3},{value:"Utilizing Clusters",id:"utilizing-clusters",level:3},{value:"S3 Table Engines",id:"s3-table-engines",level:2},{value:"Reading Data",id:"reading-data",level:3},{value:"Inserting Data",id:"inserting-data",level:3},{value:"Managing Credentials",id:"managing-credentials",level:2},{value:"Optimizing for Performance",id:"s3-optimizing-performance",level:2},{value:"Measuring Performance",id:"measuring-performance",level:3},{value:"Region Locality",id:"region-locality",level:3},{value:"Using Threads",id:"using-threads",level:3},{value:"Formats",id:"formats",level:3},{value:"Scaling with Nodes",id:"scaling-with-nodes",level:3},{value:"S3 Backed MergeTree",id:"s3-backed-mergetree",level:2},{value:"Storage Tiers",id:"storage-tiers",level:3},{value:"Creating a Disk",id:"creating-a-disk",level:3},{value:"Creating a Storage Policy",id:"creating-a-storage-policy",level:3},{value:"Creating a table",id:"creating-a-table",level:3},{value:"Modifying a Table",id:"modifying-a-table",level:3},{value:"Handling Replication",id:"handling-replication",level:3},{value:"Read &amp; Writes",id:"read--writes",level:3},{value:"Use S3 Object Storage as a ClickHouse disk",id:"configuring-s3-for-clickhouse-use",level:2},{value:"Create an AWS IAM user",id:"create-an-aws-iam-user",level:3},{value:"Create an S3 bucket",id:"create-an-s3-bucket",level:3},{value:"Configure ClickHouse to use the S3 bucket as a disk",id:"configure-clickhouse-to-use-the-s3-bucket-as-a-disk",level:3},{value:"Testing",id:"testing",level:3},{value:"Replicating a single shard across two AWS regions using S3 Object Storage",id:"s3-multi-region",level:2},{value:"Plan the deployment",id:"plan-the-deployment",level:3},{value:"Install software",id:"install-software",level:3},{value:"ClickHouse server nodes",id:"clickhouse-server-nodes",level:4},{value:"Deploy ClickHouse",id:"deploy-clickhouse",level:4},{value:"Deploy ClickHouse Keeper",id:"deploy-clickhouse-keeper",level:4},{value:"Create S3 Buckets",id:"create-s3-buckets",level:3},{value:"Configure ClickHouse Keeper",id:"configure-clickhouse-keeper",level:3},{value:"Configure ClickHouse Server",id:"configure-clickhouse-server",level:3},{value:"Define a cluster",id:"define-a-cluster",level:4},{value:"Disable zero-copy replication",id:"disable-zero-copy-replication",level:4},{value:"Configure networking",id:"configure-networking",level:3},{value:"Start the servers",id:"start-the-servers",level:3},{value:"Run ClickHouse Keeper",id:"run-clickhouse-keeper",level:4},{value:"Check ClickHouse Keeper status",id:"check-clickhouse-keeper-status",level:4},{value:"Run ClickHouse Server",id:"run-clickhouse-server",level:4},{value:"Verify ClickHouse Server",id:"verify-clickhouse-server",level:4},{value:"Testing",id:"testing-1",level:3}],m={toc:u},h="wrapper";function k(e){let{components:t,...s}=e;return(0,i.kt)(h,(0,a.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"integrating-s3-with-clickhouse"},"Integrating S3 with ClickHouse"),(0,i.kt)("p",null,"You can insert data from S3 into ClickHouse and also use S3 as an export destination, thus allowing interaction with \u201cData Lake\u201d architectures. Furthermore, S3 can provide \u201ccold\u201d storage tiers and assist with separating storage and compute. In the sections below we use the New York City taxi dataset to demonstrate the process of moving data between S3 and ClickHouse, as well as identifying key configuration parameters and providing hints on optimizing performance."),(0,i.kt)("h2",{id:"s3-table-functions"},"S3 Table Functions"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," table function allows you to read and write files from and to S3 compatible storage. The outline for this syntax is:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"s3(path, [aws_access_key_id, aws_secret_access_key,] [format, [structure, [compression]]])\n")),(0,i.kt)("p",null,"where:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"path \u2014 Bucket URL with a path to the file. This supports following wildcards in read-only mode: ",(0,i.kt)("inlineCode",{parentName:"li"},"*"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"?"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"{abc,def}")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"{N..M}")," where ",(0,i.kt)("inlineCode",{parentName:"li"},"N"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"M")," are numbers, ",(0,i.kt)("inlineCode",{parentName:"li"},"'abc'"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"'def'")," are strings. For more information, see the docs on ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/engines/table-engines/integrations/s3/#wildcards-in-path"},"using wildcards in path"),"."),(0,i.kt)("li",{parentName:"ul"},"format \u2014 The ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/interfaces/formats/#formats"},"format")," of the file."),(0,i.kt)("li",{parentName:"ul"},"structure \u2014 Structure of the table. Format ",(0,i.kt)("inlineCode",{parentName:"li"},"'column1_name column1_type, column2_name column2_type, ...'"),"."),(0,i.kt)("li",{parentName:"ul"},"compression \u2014 Parameter is optional. Supported values: ",(0,i.kt)("inlineCode",{parentName:"li"},"none"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"gzip/gz"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"brotli/br"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"xz/LZMA"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"zstd/zst"),". By default, it will autodetect compression by file extension.")),(0,i.kt)("p",null,"Using wildcards in the path expression allow multiple files to be referenced and opens the door for parallelism."),(0,i.kt)("h3",{id:"preparation"},"Preparation"),(0,i.kt)("p",null,"To interact with our S3-based dataset, we prepare a standard ",(0,i.kt)("inlineCode",{parentName:"p"},"MergeTree")," table as our destination. The statement below creates a table named ",(0,i.kt)("inlineCode",{parentName:"p"},"trips")," in the default database:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE trips\n(\n    `trip_id` UInt32,\n    `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15),\n    `pickup_date` Date,\n    `pickup_datetime` DateTime,\n    `dropoff_date` Date,\n    `dropoff_datetime` DateTime,\n    `store_and_fwd_flag` UInt8,\n    `rate_code_id` UInt8,\n    `pickup_longitude` Float64,\n    `pickup_latitude` Float64,\n    `dropoff_longitude` Float64,\n    `dropoff_latitude` Float64,\n    `passenger_count` UInt8,\n    `trip_distance` Float64,\n    `fare_amount` Float32,\n    `extra` Float32,\n    `mta_tax` Float32,\n    `tip_amount` Float32,\n    `tolls_amount` Float32,\n    `ehail_fee` Float32,\n    `improvement_surcharge` Float32,\n    `total_amount` Float32,\n    `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),\n    `trip_type` UInt8,\n    `pickup` FixedString(25),\n    `dropoff` FixedString(25),\n    `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),\n    `pickup_nyct2010_gid` Int8,\n    `pickup_ctlabel` Float32,\n    `pickup_borocode` Int8,\n    `pickup_ct2010` String,\n    `pickup_boroct2010` String,\n    `pickup_cdeligibil` String,\n    `pickup_ntacode` FixedString(4),\n    `pickup_ntaname` String,\n    `pickup_puma` UInt16,\n    `dropoff_nyct2010_gid` UInt8,\n    `dropoff_ctlabel` Float32,\n    `dropoff_borocode` UInt8,\n    `dropoff_ct2010` String,\n    `dropoff_boroct2010` String,\n    `dropoff_cdeligibil` String,\n    `dropoff_ntacode` FixedString(4),\n    `dropoff_ntaname` String,\n    `dropoff_puma` UInt16\n)\nENGINE = MergeTree\nPARTITION BY toYYYYMM(pickup_date)\nORDER BY pickup_datetime\nSETTINGS index_granularity = 8192\n")),(0,i.kt)("p",null,"Note the use of ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/engines/table-engines/mergetree-family/custom-partitioning-key/#custom-partitioning-key"},"partitioning")," on the ",(0,i.kt)("inlineCode",{parentName:"p"},"pickup_date")," field. Usually a partition key is for data management, but later on we will use this key to parallelize writes to S3."),(0,i.kt)("p",null,"Each entry in our taxi dataset contains a taxi trip. This anonymized data consists of 20M records compressed in the S3 bucket ",(0,i.kt)("a",{parentName:"p",href:"https://datasets-documentation.s3.eu-west-3.amazonaws.com/"},"https://datasets-documentation.s3.eu-west-3.amazonaws.com/")," under the folder ",(0,i.kt)("strong",{parentName:"p"},"nyc-taxi"),". The data is in the TSV format with approximately 1M rows per file."),(0,i.kt)("h3",{id:"reading-data-from-s3"},"Reading Data from s3"),(0,i.kt)("p",null,"We can query s3 data as a source without requiring persistence in ClickHouse.  In the following query, we sample 10 rows. Note the absence of credentials here as the bucket is publicly accessible:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT *\nFROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames')\nLIMIT 10;\n")),(0,i.kt)("p",null,"Note that we are not required to list the columns since the ",(0,i.kt)("inlineCode",{parentName:"p"},"TabSeparatedWithNames")," format encodes the column names in the first row. Other formats, such as ",(0,i.kt)("inlineCode",{parentName:"p"},"CSV")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"TSV"),", will return auto-generated columns for this query, e.g., ",(0,i.kt)("inlineCode",{parentName:"p"},"c1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"c2"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"c3")," etc."),(0,i.kt)("p",null,"Queries additionally support the virtual columns ",(0,i.kt)("inlineCode",{parentName:"p"},"_path")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"_file")," that provide information regarding the bucket path and filename respectively. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT  _path, _file, trip_id\nFROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz', 'TabSeparatedWithNames')\nLIMIT 5;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"| \\_path | \\_file | trip\\_id |\n| :--- | :--- | :--- |\n| datasets-documentation/nyc-taxi/trips\\_0.gz | trips\\_0.gz | 1199999902 |\n| datasets-documentation/nyc-taxi/trips\\_0.gz | trips\\_0.gz | 1199999919 |\n| datasets-documentation/nyc-taxi/trips\\_0.gz | trips\\_0.gz | 1199999944 |\n| datasets-documentation/nyc-taxi/trips\\_0.gz | trips\\_0.gz | 1199999969 |\n| datasets-documentation/nyc-taxi/trips\\_0.gz | trips\\_0.gz | 1199999990 |\n")),(0,i.kt)("p",null,"Confirm the number of rows in this sample dataset. Note the use of wildcards for file expansion, so we consider all twenty files. This query will take around 10 seconds, depending on the number of cores on the ClickHouse instance:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT count() AS count\nFROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames');\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"| count |\n| :--- |\n| 20000000 |\n")),(0,i.kt)("p",null,"While useful for sampling data and executing ae-hoc, exploratory queries, reading data directly from S3 is not something you want to do regularly. When it is time to get serious, import the data into a ",(0,i.kt)("inlineCode",{parentName:"p"},"MergeTree")," table in ClickHouse."),(0,i.kt)("h3",{id:"using-clickhouse-local"},"Using clickhouse-local"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-local")," program enables you to perform fast processing on local files without deploying and configuring the ClickHouse server. Any queries using the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," table function can be performed with this utility. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"clickhouse-local --query \"SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10\"\n")),(0,i.kt)("h3",{id:"inserting-data-from-s3"},"Inserting Data from S3"),(0,i.kt)("p",null,"To exploit the full capabilities of ClickHouse, we next read and insert the data into our instance.\nWe combine our ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," function with a simple ",(0,i.kt)("inlineCode",{parentName:"p"},"INSERT")," statement to achieve this. Note that we aren\u2019t required to list our columns because our target table provides the required structure. This requires the columns to appear in the order specified in the table DDL statement: columns are mapped according to their position in the ",(0,i.kt)("inlineCode",{parentName:"p"},"SELECT")," clause. The insertion of all 10m rows can take a few minutes depending on the ClickHouse instance. Below we insert 1M rows to ensure a prompt response. Adjust the ",(0,i.kt)("inlineCode",{parentName:"p"},"LIMIT")," clause or column selection to import subsets as required:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO trips\n   SELECT *\n   FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames')\n   LIMIT 1000000;\n")),(0,i.kt)("h3",{id:"remote-insert-using-clickhouse-local"},"Remote Insert using ClickHouse Local"),(0,i.kt)("p",null,"If network security policies prevent your ClickHouse cluster from making outbound connections, you can potentially insert S3 data using ",(0,i.kt)("inlineCode",{parentName:"p"},"clickhouse-local"),". In the example below, we read from an S3 bucket and insert into ClickHouse using the ",(0,i.kt)("inlineCode",{parentName:"p"},"remote")," function:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"clickhouse-local --query \"INSERT INTO TABLE FUNCTION remote('localhost:9000', 'default.trips', 'username', 'password') (*) SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10\"\n")),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"To execute this over a secure SSL connection, utilize the ",(0,i.kt)("inlineCode",{parentName:"p"},"remoteSecure")," function.")),(0,i.kt)("h3",{id:"exporting-data"},"Exporting Data"),(0,i.kt)("p",null,"You can write to files in S3 using the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," table function. This will require appropriate permissions. We pass the credentials needed in the request, but vie the ",(0,i.kt)("a",{parentName:"p",href:"#managing-credentials"},"Managing Credentials")," page for more options."),(0,i.kt)("p",null,"In the simple example below, we use the table function as a destination instead of a source. Here we stream 10,000 rows from the ",(0,i.kt)("inlineCode",{parentName:"p"},"trips")," table to a bucket, specifying ",(0,i.kt)("inlineCode",{parentName:"p"},"lz4")," compression and output type of ",(0,i.kt)("inlineCode",{parentName:"p"},"CSV"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO FUNCTION\n   s3(\n       'https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips.csv.lz4',\n       's3_key',\n       's3_secret',\n       'CSV'\n    )\nSELECT *\nFROM trips\nLIMIT 10000;\n")),(0,i.kt)("p",null,"Note here how the format of the file is inferred from the extension. We also don\u2019t need to specify the columns in the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," function - this can be inferred from the ",(0,i.kt)("inlineCode",{parentName:"p"},"SELECT"),"."),(0,i.kt)("h3",{id:"splitting-large-files"},"Splitting Large Files"),(0,i.kt)("p",null,"It is unlikely you will want to export your data as a single file. Most tools, including ClickHouse, will achieve higher throughput performance when reading and writing to multiple files due to the possibility of parallelism. We could execute our ",(0,i.kt)("inlineCode",{parentName:"p"},"INSERT")," command multiple times, targeting a subset of the data. ClickHouse offers a means of automatic splitting files using a ",(0,i.kt)("inlineCode",{parentName:"p"},"PARTITION")," key."),(0,i.kt)("p",null,"In the example below, we create ten files using a modulus of the ",(0,i.kt)("inlineCode",{parentName:"p"},"rand()")," function. Notice how the resulting partition ID is referenced in the filename. This results in ten files with a numerical suffix, e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"trips_0.csv.lz4"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"trips_1.csv.lz4")," etc...:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO FUNCTION\n   s3(\n       'https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4',\n       's3_key',\n       's3_secret',\n       'CSV'\n    )\n    PARTITION BY rand() % 10\nSELECT *\nFROM trips\nLIMIT 100000;\n")),(0,i.kt)("p",null,"Alternatively, we can reference a field in the data. For this dataset, the ",(0,i.kt)("inlineCode",{parentName:"p"},"payment_type")," provides a natural partitioning key with a cardinality of 5."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO FUNCTION\n   s3(\n       'https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4',\n       's3_key',\n       's3_secret',\n       'CSV'\n    )\n    PARTITION BY payment_type\nSELECT *\nFROM trips\nLIMIT 100000;\n")),(0,i.kt)("h3",{id:"utilizing-clusters"},"Utilizing Clusters"),(0,i.kt)("p",null,"The above functions are all limited to execution on a single node. Read speeds will scale linearly with CPU cores until other resources (typically network) are saturated, allowing users to vertically scale. However, this approach has its limitations. While users can alleviate some resource pressure by inserting into a distributed table when performing an ",(0,i.kt)("inlineCode",{parentName:"p"},"INSERT INTO SELECT")," query, this still leaves a single node reading, parsing, and processing the data. To address this challenge and allow us to scale reads horizontally, we have the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/sql-reference/table-functions/s3Cluster"},"s3Cluster")," function."),(0,i.kt)("p",null,"The node which receives the query, known as the initiator, creates a connection to every node in the cluster. The glob pattern determining which files need to be read is resolved to a set of files. The initiator distributes files to the nodes in the cluster, which act as workers. These workers, in turn, request files to process as they complete reads. This process ensures that we can scale reads horizontally."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"s3Cluster")," function takes the same format as the single node variants, except that a target cluster is required to denote the worker nodes:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"s3Cluster(cluster_name, source, [access_key_id, secret_access_key,] format, structure)\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"cluster_name")," \u2014 Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"source")," \u2014 URL to a file or a bunch of files. Supports following wildcards in read-only mode: *, ?, {'abc','def'} and {N..M} where N, M \u2014 numbers, abc, def \u2014 strings. For more information see ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/engines/table-engines/integrations/s3/#wildcards-in-path"},"Wildcards In Path"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"access_key_id")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"secret_access_key")," \u2014 Keys that specify credentials to use with the given endpoint. Optional."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"format")," \u2014 The ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/interfaces/formats/#formats"},"format")," of the file."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"structure")," \u2014 Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'.")),(0,i.kt)("p",null,"Like any ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," functions, the credentials are optional if the bucket is insecure or you define security through the environment, e.g., IAM roles. Unlike the s3 function, however, the structure must be specified in the request as of 22.3.1, i.e., the schema is not inferred."),(0,i.kt)("p",null,"This function will be used as part of an ",(0,i.kt)("inlineCode",{parentName:"p"},"INSERT INTO SELECT")," in most cases. In this case, you will often be inserting a distributed table. We illustrate a simple example below where trips_all is a distributed table. While this table uses the events cluster, the consistency of the nodes used for reads and writes is not a requirement:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO default.trips_all\n   SELECT *\n   FROM s3Cluster(\n       'events',\n       'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz',\n       'TabSeparatedWithNames'\n    )\n")),(0,i.kt)("p",null,"Inserts will occur against the initiator node. This means that while reads will occur on each node, the resulting rows will be routed to the initiator for distribution. In high throughput scenarios, this may prove a bottleneck. To address this, set the parameter ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/operations/settings/settings/#parallel_distributed_insert_select"},"parallel_distributed_insert_select")," for the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3cluster")," function."),(0,i.kt)("h2",{id:"s3-table-engines"},"S3 Table Engines"),(0,i.kt)("p",null,"While the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," functions allow ad-hoc queries to be performed on data stored in S3, they are syntactically verbose. The ",(0,i.kt)("inlineCode",{parentName:"p"},"S3")," table engine allows you to not have to specify the bucket URL and credentials over and over again. To address this, ClickHouse provides the S3 table engine."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE s3_engine_table (name String, value UInt32)\n    ENGINE = S3(path, [aws_access_key_id, aws_secret_access_key,] format, [compression])\n    [SETTINGS ...]\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"path")," \u2014 Bucket URL with a path to the file. Supports following wildcards in read-only mode: *, ?, {abc,def} and {N..M} where N, M \u2014 numbers, 'abc', 'def' \u2014 strings. For more information, see ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/engines/table-engines/integrations/s3#wildcards-in-path"},"here"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"format")," \u2014 The",(0,i.kt)("a",{parentName:"li",href:"/docs/en/interfaces/formats/#formats"}," format")," of the file."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"aws_access_key_id"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"aws_secret_access_key")," - Long-term credentials for the AWS account user. You can use these to authenticate your requests. The parameter is optional. If credentials are not specified, configuration file values are used. For more information, see ",(0,i.kt)("a",{parentName:"li",href:"#managing-credentials"},"Managing credentials"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"compression")," \u2014 Compression type. Supported values: none, gzip/gz, brotli/br, xz/LZMA, zstd/zst. The parameter is optional. By default, it will autodetect compression by file extension.")),(0,i.kt)("h3",{id:"reading-data"},"Reading Data"),(0,i.kt)("p",null,"In the following example, we create a table named ",(0,i.kt)("inlineCode",{parentName:"p"},"trips_raw")," using the first ten TSV files located in the ",(0,i.kt)("inlineCode",{parentName:"p"},"https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/")," bucket. Each of these contains 1M rows each:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE trips_raw\n(\n   `trip_id`               UInt32,\n   `vendor_id`             Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15),\n   `pickup_date`           Date,\n   `pickup_datetime`       DateTime,\n   `dropoff_date`          Date,\n   `dropoff_datetime`      DateTime,\n   `store_and_fwd_flag`    UInt8,\n   `rate_code_id`          UInt8,\n   `pickup_longitude`      Float64,\n   `pickup_latitude`       Float64,\n   `dropoff_longitude`     Float64,\n   `dropoff_latitude`      Float64,\n   `passenger_count`       UInt8,\n   `trip_distance`         Float64,\n   `fare_amount`           Float32,\n   `extra`                 Float32,\n   `mta_tax`               Float32,\n   `tip_amount`            Float32,\n   `tolls_amount`          Float32,\n   `ehail_fee`             Float32,\n   `improvement_surcharge` Float32,\n   `total_amount`          Float32,\n   `payment_type_`         Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),\n   `trip_type`             UInt8,\n   `pickup`                FixedString(25),\n   `dropoff`               FixedString(25),\n   `cab_type`              Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),\n   `pickup_nyct2010_gid`   Int8,\n   `pickup_ctlabel`        Float32,\n   `pickup_borocode`       Int8,\n   `pickup_ct2010`         String,\n   `pickup_boroct2010`     FixedString(7),\n   `pickup_cdeligibil`     String,\n   `pickup_ntacode`        FixedString(4),\n   `pickup_ntaname`        String,\n   `pickup_puma`           UInt16,\n   `dropoff_nyct2010_gid`  UInt8,\n   `dropoff_ctlabel`       Float32,\n   `dropoff_borocode`      UInt8,\n   `dropoff_ct2010`        String,\n   `dropoff_boroct2010`    FixedString(7),\n   `dropoff_cdeligibil`    String,\n   `dropoff_ntacode`       FixedString(4),\n   `dropoff_ntaname`       String,\n   `dropoff_puma`          UInt16\n) ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{0..9}.gz', 'TabSeparatedWithNames', 'gzip');\n")),(0,i.kt)("p",null,"Notice the use of the {0..9} pattern to limit to the first ten files. Once created, we can query this table like any other table:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT DISTINCT(pickup_ntaname)\nFROM trips_raw\nLIMIT 10;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500pickup_ntaname\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Lenox Hill-Roosevelt Island                      \u2502\n\u2502 Airport                                          \u2502\n\u2502 SoHo-TriBeCa-Civic Center-Little Italy           \u2502\n\u2502 West Village                                     \u2502\n\u2502 Chinatown                                        \u2502\n\u2502 Hudson Yards-Chelsea-Flatiron-Union Square       \u2502\n\u2502 Turtle Bay-East Midtown                          \u2502\n\u2502 Upper West Side                                  \u2502\n\u2502 Murray Hill-Kips Bay                             \u2502\n\u2502 DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,i.kt)("h3",{id:"inserting-data"},"Inserting Data"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"S3")," table engine supports parallel reads. Writes are only supported if the table definition does not contain glob patterns. The above table, therefore, would block writes."),(0,i.kt)("p",null,"To demonstrate writes, create a table that points to a writeable S3 bucket:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE trips_dest\n(\n   `trip_id`               UInt32,\n   `pickup_date`           Date,\n   `pickup_datetime`       DateTime,\n   `dropoff_datetime`      DateTime,\n   `tip_amount`            Float32,\n   `total_amount`          Float32\n) ENGINE = S3('<bucket path>/trips.bin', 'Native');\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO trips_dest\n   SELECT\n      trip_id,\n      pickup_date,\n      pickup_datetime,\n      dropoff_datetime,\n      tip_amount,\n      total_amount\n   FROM trips\n   LIMIT 10;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM trips_dest LIMIT 5;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"| trip\\_id | pickup\\_date | pickup\\_datetime | dropoff\\_datetime | tip\\_amount | total\\_amount |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 14 | 2013-08-02 | 2013-08-02 09:43:58 | 2013-08-02 09:44:13 | 0 | 2 |\n| 15 | 2013-08-02 | 2013-08-02 09:44:43 | 2013-08-02 09:45:15 | 0 | 2 |\n| 21 | 2013-08-02 | 2013-08-02 11:30:00 | 2013-08-02 17:08:00 | 0 | 172 |\n| 21 | 2013-08-02 | 2013-08-02 12:30:00 | 2013-08-02 18:08:00 | 0 | 172 |\n| 23 | 2013-08-02 | 2013-08-02 18:00:50 | 2013-08-02 18:01:55 | 0 | 6.5 |\n")),(0,i.kt)("p",null,"Note that rows can only be inserted into new files. There are no merge cycles or file split operations. Once a file is written, subsequent inserts will fail. Users have two options here:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Specify the setting ",(0,i.kt)("inlineCode",{parentName:"li"},"s3_create_new_file_on_insert=1"),". This will cause the creation of new files on each insert. A numeric suffix will be appended to the end of each file that will monotonically increase for each insert operation. For the above example, a subsequent insert would cause the creation of a trips_1.bin file."),(0,i.kt)("li",{parentName:"ul"},"Specify the setting ",(0,i.kt)("inlineCode",{parentName:"li"},"s3_truncate_on_insert=1"),". This will cause a truncation of the file, i.e. it will only contain the newly inserted rows once complete.")),(0,i.kt)("p",null,"Both of these settings default to 0 - thus forcing the user to set one of them. ",(0,i.kt)("inlineCode",{parentName:"p"},"s3_truncate_on_insert")," will take precedence if both are set."),(0,i.kt)("p",null,"Some notes about the ",(0,i.kt)("inlineCode",{parentName:"p"},"S3")," table engine:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Unlike a traditional ",(0,i.kt)("inlineCode",{parentName:"li"},"MergeTree")," family table, dropping an ",(0,i.kt)("inlineCode",{parentName:"li"},"S3")," table will not delete the underlying data."),(0,i.kt)("li",{parentName:"ul"},"Full settings for this table type can be found ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/engines/table-engines/integrations/s3/#settings"},"here"),"."),(0,i.kt)("li",{parentName:"ul"},"Be aware of the following caveats when using this engine:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"ALTER queries are not supported"),(0,i.kt)("li",{parentName:"ul"},"SAMPLE operations are not supported"),(0,i.kt)("li",{parentName:"ul"},"There is no notion of indexes, i.e. primary or skip.")))),(0,i.kt)("h2",{id:"managing-credentials"},"Managing Credentials"),(0,i.kt)("p",null,"In the previous examples, we have passed credentials in the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," function or ",(0,i.kt)("inlineCode",{parentName:"p"},"S3")," table definition. While this may be acceptable for occasional usage, users require less explicit authentication mechanisms in production. To address this, ClickHouse has several options:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Specify the connection details in the ",(0,i.kt)("strong",{parentName:"p"},"config.xml")," or an equivalent configuration file under ",(0,i.kt)("strong",{parentName:"p"},"conf.d"),". The contents of an example file are shown below, assuming installation using the debian package."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"ubuntu@single-node-clickhouse:/etc/clickhouse-server/config.d$ cat s3.xml\n<clickhouse>\n    <s3>\n        <endpoint-name>\n            <endpoint>https://dalem-files.s3.amazonaws.com/test/</endpoint>\n            <access_key_id>key</access_key_id>\n            <secret_access_key>secret</secret_access_key>\n            \x3c!-- <use_environment_credentials>false</use_environment_credentials> --\x3e\n            \x3c!-- <header>Authorization: Bearer SOME-TOKEN</header> --\x3e\n        </endpoint-name>\n    </s3>\n</clickhouse>\n")),(0,i.kt)("p",{parentName:"li"},"  These credentials will be used for any requests where the endpoint above is an exact prefix match for the requested URL. Also, note the ability in this example to declare an authorization header as an alternative to access and secret keys. A complete list of supported settings can be found ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/engines/table-engines/integrations/s3/#settings"},"here"),".")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"The example above highlights the availability of the configuration parameter ",(0,i.kt)("inlineCode",{parentName:"p"},"use_environment_credentials"),". This configuration parameter can also be set globally at the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," level:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<clickhouse>\n    <s3>\n    <use_environment_credentials>true</use_environment_credentials>\n    </s3>\n</clickhouse>\n")),(0,i.kt)("p",{parentName:"li"},"  This setting turns on an attempt to retrieve S3 credentials from the environment, thus allowing access through IAM roles. Specifically, the following order of retrieval is performed:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"A lookup for the environment variables ",(0,i.kt)("inlineCode",{parentName:"li"},"AWS_ACCESS_KEY_ID"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"AWS_SECRET_ACCESS_KEY")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"AWS_SESSION_TOKEN")),(0,i.kt)("li",{parentName:"ul"},"Check performed in ",(0,i.kt)("strong",{parentName:"li"},"$HOME/.aws")),(0,i.kt)("li",{parentName:"ul"},"Temporary credentials obtained via the AWS Security Token Service - i.e. via ",(0,i.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"},"AssumeRole")," API"),(0,i.kt)("li",{parentName:"ul"},"Checks for credentials in the ECS environment variables ",(0,i.kt)("inlineCode",{parentName:"li"},"AWS_CONTAINER_CREDENTIALS_RELATIVE_URI")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"AWS_CONTAINER_CREDENTIALS_FULL_URI")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"AWS_ECS_CONTAINER_AUTHORIZATION_TOKEN"),"."),(0,i.kt)("li",{parentName:"ul"},"Obtains the credentials via ",(0,i.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-metadata.html"},"Amazon EC2 instance metadata")," provided ",(0,i.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html#envvars-list-AWS_EC2_METADATA_DISABLED"},"AWS_EC2_METADATA_DISABLED")," is not set to true."),(0,i.kt)("li",{parentName:"ul"},"These same settings can also be set for a specific endpoint, using the same prefix matching rule.")))),(0,i.kt)("h2",{id:"s3-optimizing-performance"},"Optimizing for Performance"),(0,i.kt)(r,{mdxType:"SelfManaged"}),(0,i.kt)("h3",{id:"measuring-performance"},"Measuring Performance"),(0,i.kt)("p",null,"Before making any changes to improve performance, ensure you measure appropriately. As S3 API calls are sensitive to latency and may impact client timings, use the query log for performance metrics, i.e., ",(0,i.kt)("inlineCode",{parentName:"p"},"system.query_log"),"."),(0,i.kt)("p",null,"If measuring the performance of ",(0,i.kt)("inlineCode",{parentName:"p"},"SELECT")," queries, where large volumes of data are returned to the client, either utilize the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/interfaces/formats/#null"},"null format")," for queries or direct results to the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/engines/table-engines/special/null"},"Null engine"),". This should avoid the client being overwhelmed with data and network saturation."),(0,i.kt)("h3",{id:"region-locality"},"Region Locality"),(0,i.kt)("p",null,"Ensure your buckets are located in the same region as your ClickHouse instances. This simple optimization can dramatically improve throughput performance, especially if you deploy your ClickHouse instances on AWS infrastructure."),(0,i.kt)("h3",{id:"using-threads"},"Using Threads"),(0,i.kt)("p",null,"Read performance on S3 will scale linearly with the number of cores, provided you are not limited by network bandwidth or local IO. Increasing the number of threads also has memory overhead permutations that users should be aware of. The following can be modified to improve throughput performance potentially:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Usually, the default value of ",(0,i.kt)("inlineCode",{parentName:"li"},"max_threads")," is sufficient, i.e., the number of cores. If the amount of memory used for a query is high, and this needs to be reduced, or the LIMIT on results is low, this value can be set lower. Users with plenty of memory may wish to experiment with increasing this value for possible higher read throughput from S3. Typically this is only beneficial on machines with lower core counts, i.e., ","<"," 10. The benefit from further parallelization typically diminishes as other resources act as a bottleneck, e.g., network."),(0,i.kt)("li",{parentName:"ul"},"If performing an ",(0,i.kt)("inlineCode",{parentName:"li"},"INSERT INTO x SELECT")," request, note that the number of threads will be set to 1 as dictated by the setting ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/operations/settings/settings/#settings-max_threads"},"max_insert_threads"),". Provided max_threads is greater than 1 (confirm with ",(0,i.kt)("inlineCode",{parentName:"li"},"SELECT * FROM system.settings WHERE name='max_threads'"),"), increasing this will improve insert performance at the expense of memory. Increase with caution due to memory consumption overheads. This value should not be as high as the max_threads as resources are consumed on background merges. Furthermore, not all target engines (MergeTree does) support parallel inserts. Finally, parallel inserts invariably cause more parts, slowing down subsequent reads. Increase with caution."),(0,i.kt)("li",{parentName:"ul"},"For low memory scenarios, consider lowering ",(0,i.kt)("inlineCode",{parentName:"li"},"max_insert_delayed_streams_for_parallel_write")," if inserting into S3."),(0,i.kt)("li",{parentName:"ul"},"Versions of ClickHouse before 22.3.1 only parallelized reads across multiple files when using the ",(0,i.kt)("inlineCode",{parentName:"li"},"s3")," function or ",(0,i.kt)("inlineCode",{parentName:"li"},"S3")," table engine. This required the user to ensure files were split into chunks on S3 and read using a glob pattern to achieve optimal read performance. Later versions now parallelize downloads within a file."),(0,i.kt)("li",{parentName:"ul"},"Assuming sufficient memory (test!), increasing ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/operations/settings/settings/#min-insert-block-size-rows"},"min_insert_block_size_rows")," can improve insert throughput."),(0,i.kt)("li",{parentName:"ul"},"In low thread count scenarios, users may benefit from setting ",(0,i.kt)("inlineCode",{parentName:"li"},"remote_filesystem_read_method"),' to "read" to cause the synchronous reading of files from S3.')),(0,i.kt)("h3",{id:"formats"},"Formats"),(0,i.kt)("p",null,"ClickHouse can read files stored in S3 buckets in the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/interfaces/formats/#data-formatting"},"supported formats")," using the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," function and ",(0,i.kt)("inlineCode",{parentName:"p"},"S3")," engine. If reading raw files, some of these formats have distinct advantages:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Formats with encoded column names such as Native, Parquet, CSVWithNames, and TabSeparatedWithNames will be less verbose to query since the user will not be required to specify the column name is the ",(0,i.kt)("inlineCode",{parentName:"li"},"s3")," function. The column names allow this information to be inferred."),(0,i.kt)("li",{parentName:"ul"},"Formats will differ in performance with respect to read and write throughputs. Native and parquet represent the most optimal formats for read performance since they are already column orientated and more compact. The native format additionally benefits from alignment with how ClickHouse stores data in memory - thus reducing processing overhead as data is streamed into ClickHouse."),(0,i.kt)("li",{parentName:"ul"},"The block size will often impact the latency of reads on large files. This is very apparent if you only sample the data, e.g., returning the top N rows. In the case of formats such as CSV and TSV, files must be parsed to return a set of rows. Formats such as Native and Parquet will allow faster sampling as a result."),(0,i.kt)("li",{parentName:"ul"},"Each compression format brings pros and cons, often balancing the compression level for speed and biasing compression or decompression performance. If compressing raw files such as CSV or TSV, lz4 offers the fastest decompression performance, sacrificing the compression level. Gzip typically compresses better at the expense of slightly slower read speeds. Xz takes this further by usually offering the best compression with the slowest compression and decompression performance. If exporting, Gz and lz4 offer comparable compression speeds. Balance this against your connection speeds. Any gains from faster decompression or compression will be easily negated by a slower connection to your s3 buckets."),(0,i.kt)("li",{parentName:"ul"},"Formats such as native or parquet do not typically justify the overhead of compression. Any savings in data size are likely to be minimal since these formats are inherently compact. The time spent compressing and decompressing will rarely offset network transfer times - especially since s3 is globally available with higher network bandwidth.")),(0,i.kt)("p",null,"Internally the ClickHouse merge tree uses two primary storage formats: ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-data-storage"},"Wide and Compact"),". While the current implementation uses the default behavior of ClickHouse - controlled through the settings ",(0,i.kt)("inlineCode",{parentName:"p"},"min_bytes_for_wide_part")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"min_rows_for_wide_part"),"; we expect behavior to diverge for S3 in the future releases, e.g., a larger default value of min_bytes_for_wide_part encouraging a more Compact format and thus fewer files. Users may now wish to tune these settings when using exclusively s3 storage."),(0,i.kt)("h3",{id:"scaling-with-nodes"},"Scaling with Nodes"),(0,i.kt)("p",null,"Users will often have more than one node of ClickHouse available. While users can scale vertically, improving S3 throughput linearly with the number of cores, horizontal scaling is often necessary due to hardware availability and cost-efficiency."),(0,i.kt)("p",null,"Utilizing a cluster for S3 reads requires using the ",(0,i.kt)("inlineCode",{parentName:"p"},"s3Cluster")," function as described in ",(0,i.kt)("a",{parentName:"p",href:"#utilizing-clusters"},"Utilizing Clusters"),". While this allows reads to be distributed across nodes, thread settings will not currently be sent to all nodes. For example, if the following query was executed against a node, only the receiving initiator node will respect the ",(0,i.kt)("inlineCode",{parentName:"p"},"max_insert_threads")," setting:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO default.trips_all\n   SELECT *\n   FROM s3Cluster(\n       'events',\n       'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz',\n       'TabSeparatedWithNames'\n    )\n    SETTINGS max_insert_threads=8;\n")),(0,i.kt)("p",null,"To ensure this setting is used, the following would need to be added to each nodes ",(0,i.kt)("strong",{parentName:"p"},"config.xml")," file (or under ",(0,i.kt)("strong",{parentName:"p"},"conf.d"),"):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<clickhouse>\n  <profiles>\n    <default>\n      <max_insert_threads>8</max_insert_threads>\n    </default>\n  </profiles>\n</clickhouse>\n")),(0,i.kt)("h2",{id:"s3-backed-mergetree"},"S3 Backed MergeTree"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"s3")," functions and associated table engine allow us to query data in S3 using familiar ClickHouse syntax. However, concerning data management features and performance, they are limited. There is no support for primary indexes, no-cache support, and files inserts need to be managed by the user."),(0,i.kt)("p",null,"ClickHouse recognizes that S3 represents an attractive storage solution, especially where query performance on \u201ccolder\u201d data is less critical, and users seek to separate storage and compute. To help achieve this, support is provided for using S3 as the storage for a MergeTree engine. This will enable users to exploit the scalability and cost benefits of S3, and the insert and query performance of the MergeTree engine."),(0,i.kt)("h3",{id:"storage-tiers"},"Storage Tiers"),(0,i.kt)("p",null,"ClickHouse storage volumes allow physical disks to be abstracted from the MergeTree table engine. Any single volume can be composed of an ordered set of disks. Whilst principally allowing multiple block devices to be potentially used for data storage, this abstraction also allows other storage types, including S3. ClickHouse data parts can be moved between volumes and fill rates according to storage policies, thus creating the concept of storage tiers."),(0,i.kt)("p",null,"Storage tiers unlock hot-cold architectures where the most recent data, which is typically also the most queried, requires only a small amount of space on high-performing storage, e.g., NVMe SSDs. As the data ages, SLAs for query times increase, as does query frequency. This fat tail of data can be stored on slower, less performant storage such as HDD or object storage such as S3."),(0,i.kt)("h3",{id:"creating-a-disk"},"Creating a Disk"),(0,i.kt)("p",null,"To utilize an S3 bucket as a disk, we must first declare it within the ClickHouse configuration file. Either extend config.xml or preferably provide a new file under conf.d. An example of an S3 disk declaration is shown below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<clickhouse>\n    <storage_configuration>\n        ...\n        <disks>\n            <s3>\n                <type>s3</type>\n                <endpoint>https://sample-bucket.s3.us-east-2.amazonaws.com/tables/</endpoint>\n                <access_key_id>your_access_key_id</access_key_id>\n                <secret_access_key>your_secret_access_key</secret_access_key>\n                <region></region>\n                <metadata_path>/var/lib/clickhouse/disks/s3/</metadata_path>\n            </s3>\n            <s3_cache>\n                <type>cache</type>\n                <disk>s3</disk>\n                <path>/var/lib/clickhouse/disks/s3_cache/</path>\n                <max_size>10Gi</max_size>\n            </s3_cache>\n        </disks>\n        ...\n    </storage_configuration>\n</clickhouse>\n\n")),(0,i.kt)("p",null,"A complete list of settings relevant to this disk declaration can be found ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-s3"},"here"),". Note that credentials can be managed here using the same approaches described in ",(0,i.kt)("a",{parentName:"p",href:"#managing-credentials"},"Managing credentials"),", i.e., the use_environment_credentials can be set to true in the above settings block to use IAM roles."),(0,i.kt)("h3",{id:"creating-a-storage-policy"},"Creating a Storage Policy"),(0,i.kt)("p",null,"Once configured, this \u201cdisk\u201d can be used by a storage volume declared within a policy. For the example below, we assume s3 is our only storage. This ignores more complex hot-cold architectures where data can be relocated based on TTLs and fill rates."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<clickhouse>\n    <storage_configuration>\n        <disks>\n            <s3>\n            ...\n            </s3>\n            <s3_cache>\n            ...\n            </s3_cache>\n        </disks>\n        <policies>\n            <s3_main>\n                <volumes>\n                    <main>\n                        <disk>s3</disk>\n                    </main>\n                </volumes>\n            </s3_main>\n        </policies>\n    </storage_configuration>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"creating-a-table"},"Creating a table"),(0,i.kt)("p",null,"Assuming you have configured your disk to use a bucket with write access, you should be able to create a table such as in the example below. For purposes of brevity, we use a subset of the NYC taxi columns and stream data directly to the s3 backed table:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE trips_s3\n(\n   `trip_id` UInt32,\n   `pickup_date` Date,\n   `pickup_datetime` DateTime,\n   `dropoff_datetime` DateTime,\n   `pickup_longitude` Float64,\n   `pickup_latitude` Float64,\n   `dropoff_longitude` Float64,\n   `dropoff_latitude` Float64,\n   `passenger_count` UInt8,\n   `trip_distance` Float64,\n   `tip_amount` Float32,\n   `total_amount` Float32,\n   `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4)\n)\nENGINE = MergeTree\nPARTITION BY toYYYYMM(pickup_date)\nORDER BY pickup_datetime\nSETTINGS index_granularity = 8192, storage_policy='s3_main'\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO trips_s3 SELECT trip_id, pickup_date, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, trip_distance, tip_amount, total_amount, payment_type FROM s3('https://ch-nyc-taxi.s3.eu-west-3.amazonaws.com/tsv/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames') LIMIT 1000000;\n")),(0,i.kt)("p",null,"Depending on the hardware, this latter insert of 1m rows may take a few minutes to execute. You can confirm the progress via the system.processes table. Feel free to adjust the row count up to the limit of 10m and explore some sample queries."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT passenger_count, avg(tip_amount) as avg_tip, avg(total_amount) as avg_amount FROM trips_s3 GROUP BY passenger_count;\n")),(0,i.kt)("h3",{id:"modifying-a-table"},"Modifying a Table"),(0,i.kt)("p",null,"Occasionally users may need to modify the storage policy of a specific table. Whilst this is possible, it comes with limitations. The new target policy must contain all of the disks and volumes of the previous policy, i.e., data will not be migrated to satisfy a policy change. When validating these constraints, volumes and disks will be identified by their name, with attempts to violate resulting in an error. However, assuming you use the previous examples, the following changes are valid."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<policies>\n   <s3_main>\n       <volumes>\n           <main>\n               <disk>s3</disk>\n           </main>\n       </volumes>\n   </s3_main>\n   <s3_tiered>\n       <volumes>\n           <hot>\n               <disk>default</disk>\n           </hot>\n           <main>\n               <disk>s3</disk>\n           </main>\n       </volumes>\n       <move_factor>0.2</move_factor>\n   </s3_tiered>\n</policies>\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"ALTER TABLE trips_s3 MODIFY SETTING storage_policy='s3_tiered'\n")),(0,i.kt)("p",null,"Here we reuse the main volume in our new s3_tiered policy and introduce a new hot volume. This uses the default disk, which consists of only one disk configured via the parameter ",(0,i.kt)("inlineCode",{parentName:"p"},"<path>"),". Note that our volume names and disks do not change.  New inserts to our table will reside on the default disk until this reaches move_factor * disk_size - at which data will be relocated to S3."),(0,i.kt)("h3",{id:"handling-replication"},"Handling Replication"),(0,i.kt)("p",null,"Replication with S3 disks can be accomplished by using the ",(0,i.kt)("inlineCode",{parentName:"p"},"ReplicatedMergeTree")," table engine.  See the ",(0,i.kt)("a",{parentName:"p",href:"#s3-multi-region"},"replicating a single shard across two AWS regions using S3 Object Storage")," guide for details."),(0,i.kt)("h3",{id:"read--writes"},"Read & Writes"),(0,i.kt)("p",null,"The following notes cover the implementation of S3 interactions with ClickHouse. Whilst generally only informative, it may help the readers when ",(0,i.kt)("a",{parentName:"p",href:"#s3-optimizing-performance"},"Optimizing for Performance"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"By default, the maximum number of query processing threads used by any stage of the query processing pipeline is equal to the number of cores. Some stages are more parallelizable than others, so this value provides an upper bound.  Multiple query stages may execute at once since data is streamed from the disk. The exact number of threads used for a query may thus exceed this. Modify through the setting ",(0,i.kt)("a",{parentName:"li",href:"/docs/en/operations/settings/settings/#settings-max_threads"},"max_threads"),"."),(0,i.kt)("li",{parentName:"ul"},"Reads on S3 are asynchronous by default. This behavior is determined by setting ",(0,i.kt)("inlineCode",{parentName:"li"},"remote_filesystem_read_method"),", set to the value ",(0,i.kt)("inlineCode",{parentName:"li"},"threadpool")," by default. When serving a request, ClickHouse reads granules in stripes. Each of these stripes potentially contain many columns. A thread will read the columns for their granules one by one. Rather than doing this synchronously, a prefetch is made for all columns before waiting for the data. This offers significant performance improvements over synchronous waits on each column. Users will not need to change this setting in most cases - see ",(0,i.kt)("a",{parentName:"li",href:"#s3-optimizing-performance"},"Optimizing for Performance"),"."),(0,i.kt)("li",{parentName:"ul"},"For the s3 function and table, parallel downloading is determined by the values ",(0,i.kt)("inlineCode",{parentName:"li"},"max_download_threads")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"max_download_buffer_size"),". Files will only be downloaded in parallel if their size is greater than the total buffer size combined across all threads. This is only available on versions > 22.3.1."),(0,i.kt)("li",{parentName:"ul"},"Writes are performed in parallel, with a maximum of 100 concurrent file writing threads. ",(0,i.kt)("inlineCode",{parentName:"li"},"max_insert_delayed_streams_for_parallel_write"),", which has a default value of 1000,  controls the number of S3 blobs written in parallel. Since a buffer is required for each file being written (~1MB), this effectively limits the memory consumption of an INSERT. It may be appropriate to lower this value in low server memory scenarios.")),(0,i.kt)("h2",{id:"configuring-s3-for-clickhouse-use"},"Use S3 Object Storage as a ClickHouse disk"),(0,i.kt)("p",null,"This article demonstrates the basics of how to configure an AWS IAM user, create an S3 bucket and configure ClickHouse to use the bucket as an S3 disk. You should work with your security team to determine the permissions to be used, and consider these as a starting point."),(0,i.kt)("h3",{id:"create-an-aws-iam-user"},"Create an AWS IAM user"),(0,i.kt)("p",null,"In this procedure, we'll be creating a service account user, not a login user."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Log into the AWS IAM Management Console.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},'In "users", select ',(0,i.kt)("strong",{parentName:"p"},"Add users")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_0",src:n(35251).Z,width:"1493",height:"307"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Enter the user name and set the credential type to ",(0,i.kt)("strong",{parentName:"p"},"Access key - Programmatic access")," and select ",(0,i.kt)("strong",{parentName:"p"},"Next: Permissions")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_1",src:n(57325).Z,width:"984",height:"556"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Do not add the user to any group; select ",(0,i.kt)("strong",{parentName:"p"},"Next: Tags")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_2",src:n(88800).Z,width:"999",height:"557"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Unless you need to add any tags, select ",(0,i.kt)("strong",{parentName:"p"},"Next: Review")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_3",src:n(38989).Z,width:"983",height:"386"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Select ",(0,i.kt)("strong",{parentName:"p"},"Create User")),(0,i.kt)("p",{parentName:"li"},"  :::note\nThe warning message stating that the user has no permissions can be ignored; permissions will be granted on the bucket for the user in the next section\n:::"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_4",src:n(86327).Z,width:"987",height:"581"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"The user is now created; click on ",(0,i.kt)("strong",{parentName:"p"},"show")," and copy the access and secret keys."),(0,i.kt)("admonition",{parentName:"li",type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Save the keys somewhere else; this is the only time that the secret access key will be available.")),(0,i.kt)("p",{parentName:"li"},"  ",(0,i.kt)("img",{alt:"create_iam_user_5",src:n(66954).Z,width:"983",height:"576"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Click close, then find the user in the users screen."),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_6",src:n(90079).Z,width:"837",height:"54"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Copy the ARN (Amazon Resource Name) and save it for use when configuring the access policy for the bucket."),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_iam_user_7",src:n(2569).Z,width:"595",height:"265"})))),(0,i.kt)("h3",{id:"create-an-s3-bucket"},"Create an S3 bucket"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"In the S3 bucket section, select ",(0,i.kt)("strong",{parentName:"p"},"Create bucket")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_0",src:n(88594).Z,width:"1465",height:"326"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Enter a bucket name, leave other options default"),(0,i.kt)("admonition",{parentName:"li",type:"note"},(0,i.kt)("p",{parentName:"admonition"},"The bucket name must be unique across AWS, not just the organization, or it will emit an error."))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Leave ",(0,i.kt)("inlineCode",{parentName:"p"},"Block all Public Access")," enabled; public access is not needed."),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_2",src:n(41561).Z,width:"841",height:"754"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Select ",(0,i.kt)("strong",{parentName:"p"},"Create Bucket")," at the bottom of the page"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_3",src:n(83977).Z,width:"826",height:"132"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Select the link, copy the ARN, and save it for use when configuring the access policy for the bucket.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Once the bucket has been created, find the new S3 bucket in the S3 buckets list and select the link"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_4",src:n(96247).Z,width:"1088",height:"56"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Select ",(0,i.kt)("strong",{parentName:"p"},"Create folder")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_5",src:n(38881).Z,width:"1134",height:"448"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Enter a folder name that will be the target for the ClickHouse S3 disk and select ",(0,i.kt)("strong",{parentName:"p"},"Create folder")),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_6",src:n(92020).Z,width:"853",height:"788"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"The folder should now be visible on the bucket list"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_7",src:n(91699).Z,width:"1207",height:"569"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Select the checkbox for the new folder and click on ",(0,i.kt)("strong",{parentName:"p"},"Copy URL")," Save the URL copied to be used in the ClickHouse storage configuration in the next section."),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_8",src:n(36774).Z,width:"1200",height:"569"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Select the ",(0,i.kt)("strong",{parentName:"p"},"Permissions")," tab and click on the ",(0,i.kt)("strong",{parentName:"p"},"Edit")," button in the ",(0,i.kt)("strong",{parentName:"p"},"Bucket Policy")," section"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"create_s3_bucket_9",src:n(25129).Z,width:"1176",height:"762"}))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Add a bucket policy, example below:"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "Version": "2012-10-17",\n    "Id": "Policy123456",\n    "Statement": [\n        {\n            "Sid": "abc123",\n            "Effect": "Allow",\n            "Principal": {\n                "AWS": "arn:aws:iam::921234567898:user/mars-s3-user"\n            },\n            "Action": "s3:*",\n            "Resource": [\n                "arn:aws:s3:::mars-doc-test",\n                "arn:aws:s3:::mars-doc-test/*"\n            ]\n        }\n    ]\n}\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},'|Parameter | Description | Example Value |\n|----------|-------------|----------------|\n|Version | Version of the policy interpreter, leave as-is | 2012-10-17 |\n|Sid | User-defined policy id | abc123 |\n|Effect | Whether user requests will be allowed or denied | Allow |\n|Principal | The accounts or user that will be allowed | arn:aws:iam::921234567898:user/mars-s3-user |\n|Action | What operations are allowed on the bucket| s3:*|\n|Resource | Which resources in the bucket will operations be allowed in | "arn:aws:s3:::mars-doc-test", "arn:aws:s3:::mars-doc-test/*" |\n')),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"You should work with your security team to determine the permissions to be used, consider these as a starting point.\nFor more information on Policies and settings, refer to AWS documentation:\n",(0,i.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-language-overview.html"},"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-language-overview.html"))),(0,i.kt)("ol",{start:13},(0,i.kt)("li",{parentName:"ol"},"Save the policy configuration.")),(0,i.kt)("h3",{id:"configure-clickhouse-to-use-the-s3-bucket-as-a-disk"},"Configure ClickHouse to use the S3 bucket as a disk"),(0,i.kt)("p",null,"The following example is based on a Linux Deb package installed as a service with default ClickHouse directories."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create a new file in the ClickHouse ",(0,i.kt)("inlineCode",{parentName:"li"},"config.d")," directory to store the storage configuration.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"vim /etc/clickhouse-server/config.d/storage_config.xml\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Add the following for storage configuration; substituting the bucket path, access key and secret keys from earlier steps")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<clickhouse>\n  <storage_configuration>\n    <disks>\n      <s3_disk>\n        <type>s3</type>\n        <endpoint>https://mars-doc-test.s3.amazonaws.com/clickhouse3/</endpoint>\n        <access_key_id>ABC123</access_key_id>\n        <secret_access_key>Abc+123</secret_access_key>\n        <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>\n      </s3_disk>\n      <s3_cache>\n        <type>cache</type>\n        <disk>s3_disk</disk>\n        <path>/var/lib/clickhouse/disks/s3_cache/</path>\n        <max_size>10Gi</max_size>\n      </s3_cache>\n    </disks>\n    <policies>\n      <s3_main>\n        <volumes>\n          <main>\n            <disk>s3_disk</disk>\n          </main>\n        </volumes>\n      </s3_main>\n    </policies>\n  </storage_configuration>\n</clickhouse>\n")),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"The tags ",(0,i.kt)("inlineCode",{parentName:"p"},"s3_disk")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"s3_cache")," within the ",(0,i.kt)("inlineCode",{parentName:"p"},"<disks>")," tag are arbitrary labels. These can be set to something else but the same label must be used in the ",(0,i.kt)("inlineCode",{parentName:"p"},"<disk>")," tab under the ",(0,i.kt)("inlineCode",{parentName:"p"},"<policies>")," tab to reference the disk.\nThe ",(0,i.kt)("inlineCode",{parentName:"p"},"<S3_main>")," tag is also arbitrary and is the name of the policy which will be used as the identifier storage target when creating resources in ClickHouse."),(0,i.kt)("p",{parentName:"admonition"},"The configuration shown above is for ClickHouse version 22.8 or higher, if you are using an older version please see the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/operations/storing-data/#using-local-cache"},"storing data")," docs."),(0,i.kt)("p",{parentName:"admonition"},"For more information about using S3:\nIntegrations Guide: ",(0,i.kt)("a",{parentName:"p",href:"#s3-backed-mergetree"},"S3 Backed MergeTree"))),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Update the owner of the file to the ",(0,i.kt)("inlineCode",{parentName:"li"},"clickhouse")," user and group")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"chown clickhouse:clickhouse /etc/clickhouse-server/config.d/storage_config.xml\n")),(0,i.kt)("ol",{start:4},(0,i.kt)("li",{parentName:"ol"},"Restart the ClickHouse instance to have the changes take effect.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"service clickhouse-server restart\n")),(0,i.kt)("h3",{id:"testing"},"Testing"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Log in with the ClickHouse client, something like the following")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"clickhouse-client --user default --password ClickHouse123!\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Create a table specifying the new S3 storage policy")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE s3_table1\n           (\n               `id` UInt64,\n               `column1` String\n           )\n           ENGINE = MergeTree\n           ORDER BY id\n           SETTINGS storage_policy = 's3_main';\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Show that the table was created with the correct policy")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SHOW CREATE TABLE s3_table1;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502 CREATE TABLE default.s3_table1\n(\n    `id` UInt64,\n    `column1` String\n)\nENGINE = MergeTree\nORDER BY id\nSETTINGS storage_policy = 's3_main', index_granularity = 8192\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n")),(0,i.kt)("ol",{start:4},(0,i.kt)("li",{parentName:"ol"},"Insert test rows into the table")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO s3_table1\n           (id, column1)\n           VALUES\n           (1, 'abc'),\n           (2, 'xyz');\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"INSERT INTO s3_table1 (id, column1) FORMAT Values\n\nQuery id: 0265dd92-3890-4d56-9d12-71d4038b85d5\n\nOk.\n\n2 rows in set. Elapsed: 0.337 sec.\n")),(0,i.kt)("ol",{start:5},(0,i.kt)("li",{parentName:"ol"},"View the rows")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM s3_table1;\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500id\u2500\u252c\u2500column1\u2500\u2510\n\u2502  1 \u2502 abc     \u2502\n\u2502  2 \u2502 xyz     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows in set. Elapsed: 0.284 sec.\n")),(0,i.kt)("ol",{start:6},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"In the AWS console, navigate to the buckets, and select the new one and the folder.\nYou should see something like the following:"),(0,i.kt)("p",{parentName:"li"},"  ",(0,i.kt)("img",{alt:"create_s3_bucket_10",src:n(84375).Z,width:"1208",height:"736"})))),(0,i.kt)("h2",{id:"s3-multi-region"},"Replicating a single shard across two AWS regions using S3 Object Storage"),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"Object storage is used by default in ClickHouse Cloud, you do not need to follow this procedure if you are running in ClickHouse Cloud.")),(0,i.kt)("h3",{id:"plan-the-deployment"},"Plan the deployment"),(0,i.kt)("p",null,"This tutorial is based on deploying two ClickHouse Server nodes and three ClickHouse Keeper nodes in AWS EC2.  The data store for the ClickHouse servers is S3. Two AWS regions, with a ClickHouse Server and an S3 Bucket in each region, are used in order to support disaster recovery."),(0,i.kt)("p",null,"ClickHouse tables are replicated across the two servers, and therefore across the two regions."),(0,i.kt)("h3",{id:"install-software"},"Install software"),(0,i.kt)("h4",{id:"clickhouse-server-nodes"},"ClickHouse server nodes"),(0,i.kt)("p",null,"Refer to the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#available-installation-options"},"installation instructions")," when performing the deployment steps on the ClickHouse server nodes."),(0,i.kt)("h4",{id:"deploy-clickhouse"},"Deploy ClickHouse"),(0,i.kt)("p",null,"Deploy ClickHouse on two hosts, in the sample configurations these are named ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),"."),(0,i.kt)("p",null,"Place ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1")," in one AWS region, and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2")," in a second."),(0,i.kt)("h4",{id:"deploy-clickhouse-keeper"},"Deploy ClickHouse Keeper"),(0,i.kt)("p",null,"Deploy ClickHouse Keeper on three hosts, in the sample configurations these are named ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode2"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode3"),".  ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode1")," can be deployed in the same region as ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode2")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode3")," in either region but a different availability zone from the ClickHouse node in that region."),(0,i.kt)("p",null,"Refer to the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/install/#install-standalone-clickhouse-keeper"},"installation instructions")," when performing the deployment steps on the ClickHouse Keeper nodes."),(0,i.kt)("h3",{id:"create-s3-buckets"},"Create S3 Buckets"),(0,i.kt)("p",null,"Creating S3 buckets is covered in the guide ",(0,i.kt)("a",{parentName:"p",href:"#configuring-s3-for-clickhouse-use"},"use S3 Object Storage as a ClickHouse disk"),". Create two S3 buckets, one in each of the regions that you have placed ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),".  The configuration files will then be placed in ",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  Here is a sample configuration file for one bucket, the other is similar with the three highlighted lines differing:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/storage_config.xml"',title:'"/etc/clickhouse-server/config.d/storage_config.xml"'},"<clickhouse>\n  <storage_configuration>\n     <disks>\n        <s3_disk>\n           <type>s3</type>\n    \x3c!--highlight-start--\x3e\n           <endpoint>https://docs-clickhouse-s3.s3.us-east-2.amazonaws.com/clickhouses3/</endpoint>\n           <access_key_id>ABCDEFGHIJKLMNOPQRST</access_key_id>\n           <secret_access_key>Tjdm4kf5snfkj303nfljnev79wkjn2l3knr81007</secret_access_key>\n    \x3c!--highlight-end--\x3e\n           <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>\n        </s3_disk>\n\n        <s3_cache>\n           <type>cache</type>\n           <disk>s3</disk>\n           <path>/var/lib/clickhouse/disks/s3_cache/</path>\n           <max_size>10Gi</max_size>\n        </s3_cache>\n     </disks>\n        <policies>\n            <s3_main>\n                <volumes>\n                    <main>\n                        <disk>s3_disk</disk>\n                    </main>\n                </volumes>\n            </s3_main>\n    </policies>\n   </storage_configuration>\n</clickhouse>\n")),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Many of the steps in this guide will ask you to place a configuration file in ",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  This is the default location on Linux systems for configuration override files.  When you put these files into that directory ClickHouse will use the content to override the default configuration.  By placing these files in the override directory you will avoid losing your configuration during an upgrade.")),(0,i.kt)("h3",{id:"configure-clickhouse-keeper"},"Configure ClickHouse Keeper"),(0,i.kt)("p",null,"When running ClickHouse Keeper standalone (separate from ClickHouse server) the configuration is a single XML file.  In this tutorial, the file is ",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-keeper/keeper_config.xml"),".  All three Keeper servers use the same configuration with one setting different; ",(0,i.kt)("inlineCode",{parentName:"p"},"<server_id>"),"."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," indicates the ID to be assigned to the host where the configuration files is used.  In the example below, the ",(0,i.kt)("inlineCode",{parentName:"p"},"server_id")," is ",(0,i.kt)("inlineCode",{parentName:"p"},"3"),", and if you look further down in the file in the ",(0,i.kt)("inlineCode",{parentName:"p"},"<raft_configuration>")," section, you will see that server 3 has the hostname ",(0,i.kt)("inlineCode",{parentName:"p"},"keepernode3"),".  This is how the ClickHouse Keeper process knows which other servers to connect to when choosing a leader and all other activities."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-keeper/keeper_config.xml"',title:'"/etc/clickhouse-keeper/keeper_config.xml"'},"<clickhouse>\n    <logger>\n        <level>trace</level>\n        <log>/var/log/clickhouse-keeper/clickhouse-keeper.log</log>\n        <errorlog>/var/log/clickhouse-keeper/clickhouse-keeper.err.log</errorlog>\n        <size>1000M</size>\n        <count>3</count>\n    </logger>\n    <listen_host>0.0.0.0</listen_host>\n    <keeper_server>\n        <tcp_port>9181</tcp_port>\n\x3c!--highlight-next-line--\x3e\n        <server_id>3</server_id>\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n\n        <coordination_settings>\n            <operation_timeout_ms>10000</operation_timeout_ms>\n            <session_timeout_ms>30000</session_timeout_ms>\n            <raft_logs_level>warning</raft_logs_level>\n        </coordination_settings>\n\n        <raft_configuration>\n            <server>\n                <id>1</id>\n                <hostname>keepernode1</hostname>\n                <port>9234</port>\n            </server>\n            <server>\n                <id>2</id>\n                <hostname>keepernode2</hostname>\n                <port>9234</port>\n            </server>\n\x3c!--highlight-start--\x3e\n            <server>\n                <id>3</id>\n                <hostname>keepernode3</hostname>\n                <port>9234</port>\n            </server>\n\x3c!--highlight-end--\x3e\n        </raft_configuration>\n    </keeper_server>\n</clickhouse>\n")),(0,i.kt)("p",null,"Copy the configuration file for ClickHouse Keeper in place (remembering to set the ",(0,i.kt)("inlineCode",{parentName:"p"},"<server_id>"),"):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo -u clickhouse \\\n  cp keeper.xml /etc/clickhouse-keeper/keeper.xml\n")),(0,i.kt)("h3",{id:"configure-clickhouse-server"},"Configure ClickHouse Server"),(0,i.kt)("h4",{id:"define-a-cluster"},"Define a cluster"),(0,i.kt)("p",null,"ClickHouse cluster(s) are defined in the ",(0,i.kt)("inlineCode",{parentName:"p"},"<remote_servers>")," section of the configuration.  In this sample one cluster, ",(0,i.kt)("inlineCode",{parentName:"p"},"cluster_1S_2R"),", is defined and it consists of a single shard with two replicas.  The replicas are located on the hosts ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/remote-servers.xml"',title:'"/etc/clickhouse-server/config.d/remote-servers.xml"'},'<clickhouse>\n    <remote_servers replace="true">\n        <cluster_1S_2R>\n            <shard>\n                <replica>\n                    <host>chnode1</host>\n                    <port>9000</port>\n                </replica>\n                <replica>\n                    <host>chnode2</host>\n                    <port>9000</port>\n                </replica>\n            </shard>\n        </cluster_1S_2R>\n    </remote_servers>\n</clickhouse>\n')),(0,i.kt)("p",null,"When working with clusters it is handy to define macros that populate DDL queries with the cluster, shard, and replica settings.  This sample allows you to specify the use of a replicated table engine without providing ",(0,i.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," details.  When you create a table you can see how the ",(0,i.kt)("inlineCode",{parentName:"p"},"shard")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," macros are used by querying ",(0,i.kt)("inlineCode",{parentName:"p"},"system.tables"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/macros.xml"',title:'"/etc/clickhouse-server/config.d/macros.xml"'},"<clickhouse>\n    <distributed_ddl>\n            <path>/clickhouse/task_queue/ddl</path>\n    </distributed_ddl>\n    <macros>\n        <cluster>cluster_1S_2R</cluster>\n        <shard>1</shard>\n        <replica>replica_1</replica>\n    </macros>\n</clickhouse>\n")),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"The above macros are for ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode1"),", on ",(0,i.kt)("inlineCode",{parentName:"p"},"chnode2")," set ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," to ",(0,i.kt)("inlineCode",{parentName:"p"},"replica_2"),".")),(0,i.kt)("h4",{id:"disable-zero-copy-replication"},"Disable zero-copy replication"),(0,i.kt)("p",null,"In ClickHouse versions 22.7 and lower the setting ",(0,i.kt)("inlineCode",{parentName:"p"},"allow_remote_fs_zero_copy_replication")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"true")," by default for S3 and HDFS disks. This setting should be set to ",(0,i.kt)("inlineCode",{parentName:"p"},"false")," for this disaster recovery scenario, and in version 22.8 and higher it is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"false")," by default."),(0,i.kt)("p",null,"This setting should be false for two reasons: 1) this feature is not production ready; 2) in a disaster recovery scenario both the data and metadata need to be stored in multiple regions. Set ",(0,i.kt)("inlineCode",{parentName:"p"},"allow_remote_fs_zero_copy_replication")," to ",(0,i.kt)("inlineCode",{parentName:"p"},"false"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/remote-servers.xml"',title:'"/etc/clickhouse-server/config.d/remote-servers.xml"'},"<clickhouse>\n   <merge_tree>\n        <allow_remote_fs_zero_copy_replication>false</allow_remote_fs_zero_copy_replication>\n   </merge_tree>\n</clickhouse>\n")),(0,i.kt)("p",null,"ClickHouse Keeper is responsible for coordinating the replication of data across the ClickHouse nodes.  To inform ClickHouse about the ClickHouse Keeper nodes add a configuration file to each of the ClickHouse nodes."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/use_keeper.xml"',title:'"/etc/clickhouse-server/config.d/use_keeper.xml"'},'<clickhouse>\n    <zookeeper>\n        <node index="1">\n            <host>keepernode1</host>\n            <port>9181</port>\n        </node>\n        <node index="2">\n            <host>keepernode2</host>\n            <port>9181</port>\n        </node>\n        <node index="3">\n            <host>keepernode3</host>\n            <port>9181</port>\n        </node>\n    </zookeeper>\n</clickhouse>\n')),(0,i.kt)("h3",{id:"configure-networking"},"Configure networking"),(0,i.kt)("p",null,"See the ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/guides/sre/network-ports"},"network ports")," list when you configure the security settings in AWS so that your servers can communicate with each other, and you can communicate with them."),(0,i.kt)("p",null,"All three servers must listen for network connections so that they can communicate between the servers and with S3.  By default, ClickHouse listens ony on the loopback address, so this must be changed.  This is configured in ",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse-server/config.d/"),".  Here is a sample that configures ClickHouse and ClickHouse Keeper to listen on all IP v4 interfaces.  see the documentation or the default configuration file ",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/clickhouse/config.xml")," for more information."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml",metastring:'title="/etc/clickhouse-server/config.d/networking.xml"',title:'"/etc/clickhouse-server/config.d/networking.xml"'},"<clickhouse>\n    <listen_host>0.0.0.0</listen_host>\n</clickhouse>\n")),(0,i.kt)("h3",{id:"start-the-servers"},"Start the servers"),(0,i.kt)("h4",{id:"run-clickhouse-keeper"},"Run ClickHouse Keeper"),(0,i.kt)("p",null,"On each Keeper server run the commands for your operating system, for example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo systemctl enable clickhouse-keeper\nsudo systemctl start clickhouse-keeper\nsudo systemctl status clickhouse-keeper\n")),(0,i.kt)("h4",{id:"check-clickhouse-keeper-status"},"Check ClickHouse Keeper status"),(0,i.kt)("p",null,"Send commands to the ClickHouse Keeper with ",(0,i.kt)("inlineCode",{parentName:"p"},"netcat"),".  For example, ",(0,i.kt)("inlineCode",{parentName:"p"},"mntr")," returns the state of the ClickHouse Keeper cluster.  If you run the command on each of the Keeper nodes you will see that one is a leader, and the other two are followers:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"echo mntr | nc localhost 9181\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-response"},"zk_version  v22.7.2.15-stable-f843089624e8dd3ff7927b8a125cf3a7a769c069\nzk_avg_latency  0\nzk_max_latency  11\nzk_min_latency  0\nzk_packets_received 1783\nzk_packets_sent 1783\n# highlight-start\nzk_num_alive_connections    2\nzk_outstanding_requests 0\nzk_server_state leader\n# highlight-end\nzk_znode_count  135\nzk_watch_count  8\nzk_ephemerals_count 3\nzk_approximate_data_size    42533\nzk_key_arena_size   28672\nzk_latest_snapshot_size 0\nzk_open_file_descriptor_count   182\nzk_max_file_descriptor_count    18446744073709551615\n# highlight-start\nzk_followers    2\nzk_synced_followers 2\n# highlight-end\n")),(0,i.kt)("h4",{id:"run-clickhouse-server"},"Run ClickHouse Server"),(0,i.kt)("p",null,"On each ClickHouse server run"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"sudo service clickhouse-server start\n")),(0,i.kt)("h4",{id:"verify-clickhouse-server"},"Verify ClickHouse Server"),(0,i.kt)("p",null,"When you added the ",(0,i.kt)("a",{parentName:"p",href:"#define-a-cluster"},"cluster configuration")," a single shard replicated across the two ClickHouse nodes was defined.  In this verification step you will check that the cluster was built when ClickHouse was started, and you will create a replicated table using that cluster."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Verify that the cluster exists:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"show clusters\n")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 cluster_1S_2R \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.009 sec. `\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Create a table in the cluster using the ",(0,i.kt)("inlineCode",{parentName:"p"},"ReplicatedMergeTree")," table engine:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"create table trips on cluster 'cluster_1S_2R' (\n `trip_id` UInt32,\n `pickup_date` Date,\n `pickup_datetime` DateTime,\n `dropoff_datetime` DateTime,\n `pickup_longitude` Float64,\n `pickup_latitude` Float64,\n `dropoff_longitude` Float64,\n `dropoff_latitude` Float64,\n `passenger_count` UInt8,\n `trip_distance` Float64,\n `tip_amount` Float32,\n `total_amount` Float32,\n `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4))\nENGINE = ReplicatedMergeTree\nPARTITION BY toYYYYMM(pickup_date)\nORDER BY pickup_datetime\nSETTINGS index_granularity = 8192, storage_policy='s3_main'\n")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-response"},"\u250c\u2500host\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\n\u2502 chnode1 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\n\u2502 chnode2 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Understand the use of the macros defined earlier"),(0,i.kt)("p",{parentName:"li"},"The macros ",(0,i.kt)("inlineCode",{parentName:"p"},"shard"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"replica")," were ",(0,i.kt)("a",{parentName:"p",href:"#define-a-cluster"},"defined earlier"),", and in the highlighted line below you can see where the values are substituted on each ClickHouse node.  Additionally, the value ",(0,i.kt)("inlineCode",{parentName:"p"},"uuid")," is used; ",(0,i.kt)("inlineCode",{parentName:"p"},"uuid")," is not defined in the macros as it is generated by the system."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT create_table_query\nFROM system.tables\nWHERE name = 'trips'\nFORMAT Vertical\n")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-response"},"Query id: 4d326b66-0402-4c14-9c2f-212bedd282c0\n\nRow 1:\n\u2500\u2500\u2500\u2500\u2500\u2500\ncreate_table_query: CREATE TABLE default.trips (`trip_id` UInt32, `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_datetime` DateTime, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `tip_amount` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4))\n# highlight-next-line\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{uuid}/{shard}', '{replica}')\nPARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192, storage_policy = 's3_main'\n\n1 row in set. Elapsed: 0.012 sec.\n")),(0,i.kt)("admonition",{parentName:"li",type:"note"},(0,i.kt)("p",{parentName:"admonition"},"You can customize the zookeeper path ",(0,i.kt)("inlineCode",{parentName:"p"},"'clickhouse/tables/{uuid}/{shard}")," shown above by setting ",(0,i.kt)("inlineCode",{parentName:"p"},"default_replica_path")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"default_replica_name"),".  The docs are ",(0,i.kt)("a",{parentName:"p",href:"/docs/en/operations/server-configuration-parameters/settings/#default_replica_path"},"here"),".")))),(0,i.kt)("h3",{id:"testing-1"},"Testing"),(0,i.kt)("p",null,"These tests will verify that data is being replicated across the two servers, and that it is stored in the S3 Buckets and not on local disk."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Add data from the New York City taxi dataset:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO trips\nSELECT trip_id,\n       pickup_date,\n       pickup_datetime,\n       dropoff_datetime,\n       pickup_longitude,\n       pickup_latitude,\n       dropoff_longitude,\n       dropoff_latitude,\n       passenger_count,\n       trip_distance,\n       tip_amount,\n       total_amount,\n       payment_type\n   FROM s3('https://ch-nyc-taxi.s3.eu-west-3.amazonaws.com/tsv/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames') LIMIT 1000000;\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Verify that data is stored in S3."),(0,i.kt)("p",{parentName:"li"},"This query shows the size of the data on disk, and the policy used to determine which disk is used."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT\n    engine,\n    data_paths,\n    metadata_path,\n    storage_policy,\n    formatReadableSize(total_bytes)\nFROM system.tables\nWHERE name = 'trips'\nFORMAT Vertical\n")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-response"},"Query id: af7a3d1b-7730-49e0-9314-cc51c4cf053c\n\nRow 1:\n\u2500\u2500\u2500\u2500\u2500\u2500\nengine:                          ReplicatedMergeTree\ndata_paths:                      ['/var/lib/clickhouse/disks/s3_disk/store/551/551a859d-ec2d-4512-9554-3a4e60782853/']\nmetadata_path:                   /var/lib/clickhouse/store/e18/e18d3538-4c43-43d9-b083-4d8e0f390cf7/trips.sql\nstorage_policy:                  s3_main\nformatReadableSize(total_bytes): 36.42 MiB\n\n1 row in set. Elapsed: 0.009 sec.\n")),(0,i.kt)("p",{parentName:"li"},"Check the size of data on the local disk.  From above, the size on disk for the millions of rows stored is 36.42 MiB.  This should be on S3, and not the local disk.  The query above also tells us where on local disk data and metadata is stored.  Check the local data:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-response"},"root@chnode1:~# du -sh /var/lib/clickhouse/disks/s3_disk/store/551\n536K    /var/lib/clickhouse/disks/s3_disk/store/551\n")),(0,i.kt)("p",{parentName:"li"},"Check the S3 data in each S3 Bucket (the totals are not shown, but both buckets have approximately 36 MiB stored after the inserts):"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"size in first S3 bucket",src:n(49620).Z,width:"1315",height:"935"})),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("img",{alt:"size in second S3 bucket",src:n(76254).Z,width:"1315",height:"935"})))))}k.isMDXComponent=!0},49620:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/bucket1-68884eb470ad9b974ecbe9ea36c9cdfa.png"},76254:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/bucket2-4a39a7d29034721f8bb52dbb6faad814.png"},35251:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-1-03bea40f47bb9ae0f8a38b921fb5fc65.png"},57325:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-2-d1769946c5baf28dec5266388f46ca6d.png"},88800:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-3-e687a417d48603c3a1a4c456639691e6.png"},38989:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-4-8bc97d7d71749ecca4b0ca006ec3e659.png"},86327:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-5-d9258893409b46a1cceeb1fb26938d3a.png"},66954:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-6-0f6f4bbe29c6b238875fe1b0e28f6764.png"},90079:(e,t,n)=>{n.d(t,{Z:()=>a});const a="data:image/png;base64,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"},2569:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-8-629425bf3160cfe75e44876e5c71eb8b.png"},88594:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-9-4aa76542e482dff3894ec423231dd1b0.png"},41561:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-a-0e6dc95b7af088e0bd74a31fbef15076.png"},83977:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-b-63b4e2c1dae1494a8abbb0298c70d606.png"},96247:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-c-2d809bd0ea2cd6138cd8ec16a07b29f1.png"},38881:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-d-8b4bb75da064d5d60ccb01b78026b4b2.png"},92020:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-e-3d5076dfc4d904eb6e1df91fd5a7f540.png"},91699:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-f-d6312cc21827dfaaf0177e642286bec0.png"},36774:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-g-fc09549f49847b6c59c04123a76baae6.png"},25129:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-h-bfc7b2b7bfd8dac081ed4a10b34a9e03.png"},84375:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/s3-j-5dcc986c9002270f2d67ba97e8d7474e.png"}}]);